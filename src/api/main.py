import os
import json
import base64
import logging
import shutil
from flask import Flask, request, jsonify
from datetime import datetime
import traceback
from typing import Dict, Any, Optional
import google.generativeai as genai
import subprocess
import sys
from pathlib import Path

# UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç¢ºå®Ÿã«ã™ã‚‹
os.environ.setdefault('PYTHONIOENCODING', 'utf-8')
os.environ.setdefault('LANG', 'ja_JP.UTF-8')
os.environ.setdefault('LC_ALL', 'ja_JP.UTF-8')
from src.api.model_downloader import ensure_models_available

app = Flask(__name__)

# UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç¢ºå®Ÿã«ã™ã‚‹
app.config['JSON_AS_ASCII'] = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Cloud Runèµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
def initialize_models():
    """èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆ512MBæ§‹æˆç”¨ã«è»½é‡åŒ–ï¼‰"""
    logger.info("ğŸš€ Initializing models (lightweight mode)...")
    
    # ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ãŒã‚ã‚‹å ´åˆã¯ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—
    memory_limit = os.environ.get('CLOUD_RUN_MEMORY', '512Mi')
    if '512' in memory_limit:
        logger.info("ğŸ’¡ 512MBæ§‹æˆæ¤œå‡º - ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        logger.info("ğŸ”§ ãƒ¢ãƒ‡ãƒ«ã¯å‡¦ç†æ™‚ã«å‹•çš„ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™")
        return
    
    try:
        success = ensure_models_available()
        if success:
            logger.info("âœ… Models initialized successfully")
        else:
            logger.warning("âš ï¸ Some models may be missing")
    except Exception as e:
        logger.error(f"âŒ Failed to initialize models: {e}")
        logger.warning("âš ï¸ Continuing without models - OCR features may be limited")

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
initialize_models()

def get_project_root():
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—"""
    # Cloud Runã§ã¯/appé…ä¸‹ã«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒé…ç½®ã•ã‚Œã‚‹
    if Path("/app").exists():
        return Path("/app")
    else:
        # ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒ
        return Path(__file__).parent.parent.parent

def run_main_pipeline(pdf_path: str) -> Dict[str, Any]:
    """
    main_pipeline.pyã‚’å®Ÿè¡Œã—ã¦OCRå‡¦ç†ã‚’è¡Œã†
    
    Args:
        pdf_path: å‡¦ç†å¯¾è±¡ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        
    Returns:
        Dict: å‡¦ç†çµæœ
    """
    try:
        project_root = get_project_root()
        main_pipeline_path = project_root / "src" / "main_pipeline.py"
        
        if not main_pipeline_path.exists():
            raise FileNotFoundError(f"main_pipeline.py not found: {main_pipeline_path}")
        
        # main_pipeline.pyã‚’å®Ÿè¡Œ
        cmd = [
            sys.executable,
            str(main_pipeline_path),
            "--input", pdf_path
        ]
        
        logger.info(f"ğŸš€ Running main_pipeline.py with command: {' '.join(cmd)}")
        
        result = subprocess.run(
            cmd,
            cwd=str(project_root),
            capture_output=True,
            text=True,
            encoding='utf-8',
            timeout=300  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        )
        
        if result.returncode == 0:
            logger.info("âœ… main_pipeline.py executed successfully")
            return {
                "success": True,
                "stdout": result.stdout,
                "stderr": result.stderr
            }
        else:
            logger.error(f"âŒ main_pipeline.py failed with return code: {result.returncode}")
            logger.error(f"âŒ STDOUT: {result.stdout}")
            logger.error(f"âŒ STDERR: {result.stderr}")
            return {
                "success": False,
                "error": f"Pipeline execution failed: {result.stderr}",
                "stdout": result.stdout,
                "stderr": result.stderr
            }
            
    except subprocess.TimeoutExpired:
        logger.error("âŒ main_pipeline.py execution timed out")
        return {
            "success": False,
            "error": "Pipeline execution timed out"
        }
    except Exception as e:
        logger.error(f"âŒ Error running main_pipeline.py: {e}")
        return {
            "success": False,
            "error": str(e)
        }

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({'status': 'healthy', 'timestamp': datetime.utcnow().isoformat()}), 200

@app.route('/debug-blobs', methods=['GET'])
def debug_blobs():
    """GCSå†…ã®blobã‚’ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒªã‚¹ãƒˆã™ã‚‹"""
    try:
        from google.cloud import storage
        
        prefix = request.args.get('prefix', '')
        bucket_name = 'app_contracts_staging'
        
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        
        blobs = list(bucket.list_blobs(prefix=prefix))
        
        blob_info = []
        for blob in blobs:
            blob_info.append({
                'name': blob.name,
                'size': blob.size,
                'created': blob.time_created.isoformat() if blob.time_created else None,
                'content_type': blob.content_type
            })
        
        return jsonify({
            'bucket': bucket_name,
            'prefix': prefix,
            'total_blobs': len(blob_info),
            'blobs': blob_info
        }), 200
        
    except Exception as e:
        logger.error(f"Debug blobs error: {str(e)}")
        return jsonify({
            'error': str(e)
        }), 500

@app.route('/', methods=['GET'])
def root():
    """
    ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ - APIæƒ…å ±ã‚’è¿”ã™
    """
    return jsonify({
        'service': 'DD-OPS OCR API',
        'version': '1.0.0',
        'endpoints': {
            '/health': 'Health check',
            '/ocr [GET]': 'Test OCR with local PDF files',
            '/ocr [POST]': 'OCR with GCS PDF URL',
            '/pubsub/push [POST]': 'PubSub webhook for automatic processing'
        },
        'usage': {
            'GET /ocr': 'Process PDF from /pdf/ directory',
            'GET /ocr?file=test.pdf': 'Process specific PDF file',
            'POST /ocr': 'Send {"pdf_url": "gs://bucket/file.pdf", "workspace_id": "ws", "project_id": "proj"}'
        },
        'timestamp': datetime.utcnow().isoformat()
    }), 200

@app.route('/ocr', methods=['GET'])
def ocr_test():
    """
    GETãƒ¡ã‚½ãƒƒãƒ‰ã§OCRå‡¦ç†ã‚’ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    
    ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
        file: PDFãƒ•ã‚¡ã‚¤ãƒ«å (ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯è‡ªå‹•æ¤œå‡º)
        
    ä¾‹: /ocr?file=test.pdf
    """
    try:
        logger.info("ğŸš€ GET /ocr endpoint called")
        
        # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—
        pdf_filename = request.args.get('file')
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®OCRå‡¦ç†ã‚’å®Ÿè¡Œ
        result = process_test_pdf(pdf_filename)
        
        response = {
            "status": "completed",
            "method": "GET",
            "timestamp": datetime.now().isoformat(),
            "result": result
        }
        
        return jsonify(response), 200
        
    except Exception as e:
        logger.error(f"OCR test endpoint error: {str(e)}")
        return jsonify({
            "status": "error",
            "method": "GET",
            "timestamp": datetime.now().isoformat(),
            "error": str(e)
        }), 500

@app.route('/ocr', methods=['POST'])
def ocr_upload():
    """
    POSTãƒ¡ã‚½ãƒƒãƒ‰ã§OCRå‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    
    JSON body:
        {
            "pdf_url": "gs://bucket/path/to/file.pdf",
            "workspace_id": "workspace123",
            "project_id": "project456"
        }
    """
    try:
        logger.info("ğŸš€ POST /ocr endpoint called")
        
        data = request.get_json()
        if not data:
            return jsonify({
                "status": "error",
                "error": "JSON body required"
            }), 400
        
        pdf_url = data.get('pdf_url')
        workspace_id = data.get('workspace_id', 'test_workspace')
        project_id = data.get('project_id', 'test_project')
        
        if not pdf_url:
            return jsonify({
                "status": "error",
                "error": "pdf_url is required"
            }), 400
        
        # GCS URIã®å ´åˆ
        if pdf_url.startswith('gs://'):
            bucket_name = pdf_url.replace('gs://', '').split('/')[0]
            object_name = '/'.join(pdf_url.replace('gs://', '').split('/')[1:])
            
            result = process_single_pdf(bucket_name, object_name, workspace_id, project_id)
        else:
            return jsonify({
                "status": "error",
                "error": "Only gs:// URLs are supported"
            }), 400
        
        response = {
            "status": "completed",
            "method": "POST",
            "timestamp": datetime.now().isoformat(),
            "workspace_id": workspace_id,
            "project_id": project_id,
            "result": result
        }
        
        return jsonify(response), 200
        
    except Exception as e:
        logger.error(f"OCR upload endpoint error: {str(e)}")
        return jsonify({
            "status": "error",
            "method": "POST",
            "timestamp": datetime.now().isoformat(),
            "error": str(e)
        }), 500

@app.route('/pubsub/push', methods=['POST'])
def pubsub_push():
    """
    Cloud PubSub Pushé€šçŸ¥ã‚’å—ã‘å–ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    GCS Storage Object notificationã‚’å‡¦ç†
    """
    try:
        logger.info("="*80)
        logger.info("ğŸ”µ PUBSUB PUSH REQUEST RECEIVED")
        logger.info("="*80)
        
        logger.info(f"ğŸ“Œ Request Method: {request.method}")
        logger.info(f"ğŸ“Œ Request URL: {request.url}")
        logger.info(f"ğŸ“Œ Request Path: {request.path}")
        
        logger.info("ğŸ“‹ REQUEST HEADERS:")
        for key, value in request.headers:
            logger.info(f"  - {key}: {value}")
        
        logger.info(f"ğŸ“ Content-Type: {request.content_type}")
        logger.info(f"ğŸ“ Content-Length: {request.content_length}")
        
        logger.info(f"ğŸ“¦ Raw Request Data: {request.data}")
        logger.info(f"ğŸ“¦ Request Data Type: {type(request.data)}")
        logger.info(f"ğŸ“¦ Request Data Length: {len(request.data) if request.data else 0}")
        
        logger.info("ğŸ” Attempting to parse JSON...")
        envelope = None
        try:
            envelope = request.get_json()
            logger.info(f"âœ… JSON parsed successfully")
            logger.info(f"ğŸ“Š Envelope type: {type(envelope)}")
            logger.info(f"ğŸ“Š Envelope keys: {list(envelope.keys()) if envelope else 'None'}")
            logger.info(f"ğŸ“Š Full envelope content: {json.dumps(envelope, indent=2) if envelope else 'None'}")
        except Exception as json_error:
            logger.error(f"âŒ JSON parsing failed: {str(json_error)}")
            logger.error(f"âŒ Error type: {type(json_error).__name__}")
        
        if not envelope:
            logger.error("âŒ No PubSub message received (envelope is None or empty)")
            return jsonify({"error": "Bad Request: no PubSub message received"}), 400
            
        if not isinstance(envelope, dict) or "message" not in envelope:
            logger.error(f"âŒ Invalid PubSub message format - envelope type: {type(envelope)}, has 'message' key: {'message' in envelope if isinstance(envelope, dict) else 'N/A'}")
            return jsonify({"error": "Bad Request: invalid PubSub message format"}), 400
            
        pubsub_message = envelope["message"]
        logger.info("ğŸ“¨ PUBSUB MESSAGE:")
        logger.info(f"  - Message type: {type(pubsub_message)}")
        logger.info(f"  - Message keys: {list(pubsub_message.keys()) if isinstance(pubsub_message, dict) else 'Not a dict'}")
        logger.info(f"  - Full message: {json.dumps(pubsub_message, indent=2)}")
        
        if isinstance(pubsub_message.get("data"), str):
            try:
                logger.info("ğŸ”“ Attempting Base64 decode...")
                logger.info(f"  - Data length (encoded): {len(pubsub_message['data'])}")
                logger.info(f"  - First 100 chars: {pubsub_message['data'][:100]}...")
                
                message_data = base64.b64decode(pubsub_message["data"]).decode("utf-8")
                logger.info(f"âœ… Base64 decode successful")
                logger.info(f"  - Decoded length: {len(message_data)}")
                logger.info(f"  - First 200 chars: {message_data[:200] if message_data else 'Empty'}...")
                
                if not message_data or not message_data.strip():
                    logger.warning(f"âš ï¸ Decoded message is empty or whitespace only")
                    return jsonify({"status": "ignored", "reason": "Empty message"}), 200
                
                storage_object = json.loads(message_data)
                logger.info(f"âœ… JSON parse of decoded data successful")
                logger.info(f"ğŸ“„ Storage Object keys: {list(storage_object.keys())}")
                logger.info(f"ğŸ“„ Full Storage Object: {json.dumps(storage_object, indent=2)}")
            except Exception as e:
                logger.error(f"âŒ Failed to decode PubSub message: {str(e)}")
                logger.error(f"âŒ Error type: {type(e).__name__}")
                logger.error(f"âŒ Stack trace: {traceback.format_exc()}")
                return jsonify({"error": "Bad Request: invalid message data"}), 400
        else:
            logger.error(f"âŒ PubSub message data is not a string, type: {type(pubsub_message.get('data'))}")
            return jsonify({"error": "Bad Request: message data must be base64 encoded"}), 400
            
        if not isinstance(storage_object, dict) or "id" not in storage_object:
            logger.error(f"âŒ Invalid Storage Object format - type: {type(storage_object)}, has 'id': {'id' in storage_object if isinstance(storage_object, dict) else 'N/A'}")
            return jsonify({"error": "Bad Request: invalid Storage Object"}), 400
            
        object_id = storage_object.get("id", "")
        object_name = storage_object.get("name", "")
        object_bucket = storage_object.get("bucket", "")
        
        logger.info("ğŸ—‚ï¸ STORAGE OBJECT INFO:")
        logger.info(f"  - Object ID: {object_id}")
        logger.info(f"  - Object Name: {object_name}")
        logger.info(f"  - Bucket: {object_bucket}")
        logger.info(f"  - Content Type: {storage_object.get('contentType', 'N/A')}")
        logger.info(f"  - Size: {storage_object.get('size', 'N/A')} bytes")
        
        name_parts = object_name.split("/")
        logger.info(f"ğŸ“‚ Name parts: {name_parts}")
        logger.info(f"ğŸ“‚ Number of parts: {len(name_parts)}")
        
        if len(name_parts) < 3:
            logger.error(f"âŒ Invalid object name format: {object_name} - expected at least 3 parts")
            return jsonify({"error": "Invalid object name format"}), 400
            
        workspace_id = name_parts[0]
        project_id = name_parts[1]
        filename = "/".join(name_parts[2:])
        
        logger.info("âœ¨ EXTRACTED INFO:")
        logger.info(f"  - Workspace ID: {workspace_id}")
        logger.info(f"  - Project ID: {project_id}")
        logger.info(f"  - Filename: {filename}")
        
        if not filename.lower().endswith('.pdf'):
            logger.info(f"âš ï¸ Ignoring non-PDF file: {filename}")
            return jsonify({"message": "File ignored (not a PDF)"}), 200
            
        logger.info(f"ğŸš€ Starting PDF processing - workspace: {workspace_id}, project: {project_id}, file: {filename}")
        
        result = process_single_pdf(object_bucket, object_name, workspace_id, project_id)
        
        response = {
            "workspace_id": workspace_id,
            "project_id": project_id,
            "file": filename,
            "success": result["success"],
            "timestamp": datetime.now().isoformat()
        }
        
        if result["success"]:
            response["contract_json"] = result.get("contract_json", "")
            response["output_files"] = result.get("output_files", [])
            logger.info(f"Successfully processed PDF: {filename}")
        else:
            response["error"] = result.get("error", "Processing failed")
            logger.error(f"Failed to process PDF: {filename} - {result.get('error')}")
            
        return jsonify(response), 200
        
    except Exception as e:
        logger.error(f"Unexpected error in PubSub handler: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            "error": "Internal server error",
            "message": str(e)
        }), 200

def process_test_pdf(pdf_filename: Optional[str] = None) -> Dict[str, Any]:
    """
    ãƒ†ã‚¹ãƒˆç”¨ã®PDFå‡¦ç†ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã®pdf/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ï¼‰
    
    Args:
        pdf_filename: å‡¦ç†ã™ã‚‹PDFãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
    Returns:
        å‡¦ç†çµæœã‚’å«ã‚€è¾æ›¸
    """
    try:
        project_root = get_project_root()
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ã®pdf/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰PDFã‚’æ¤œç´¢
        local_pdf_dir = project_root / "pdf"
        
        if not local_pdf_dir.exists():
            return {
                'success': False,
                'error': 'PDF directory not found. Please place PDF files in /pdf/ directory.'
            }
        
        # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®æ±ºå®š
        if pdf_filename:
            pdf_path = local_pdf_dir / pdf_filename
            if not pdf_path.exists():
                return {
                    'success': False,
                    'error': f'PDF file not found: {pdf_filename}'
                }
        else:
            # è‡ªå‹•æ¤œå‡ºï¼šæœ€åˆã«è¦‹ã¤ã‹ã£ãŸPDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
            pdf_files = list(local_pdf_dir.glob("*.pdf"))
            if not pdf_files:
                return {
                    'success': False,
                    'error': 'No PDF files found in /pdf/ directory'
                }
            pdf_path = pdf_files[0]
            pdf_filename = pdf_path.name
        
        logger.info(f"Processing local PDF: {pdf_path}")
        
        # main_pipeline.pyã‚’å®Ÿè¡Œ
        pipeline_result = run_main_pipeline(str(pdf_path))
        
        if not pipeline_result["success"]:
            logger.error(f"Pipeline execution failed: {pipeline_result.get('error')}")
            return {
                'success': False,
                'error': pipeline_result.get('error', 'Pipeline execution failed'),
                'pipeline_output': pipeline_result
            }
        
        # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†
        basename = os.path.splitext(pdf_filename)[0]
        result_files = []
        contract_data = None
        
        # resultãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒ£ãƒ³
        result_dir = project_root / "result"
        if not result_dir.exists():
            result_dir = Path("/tmp/result")
        
        if result_dir.exists():
            for result_file in result_dir.glob("*"):
                if result_file.is_file():
                    file_info = {
                        "filename": result_file.name,
                        "size": result_file.stat().st_size,
                        "path": str(result_file)
                    }
                    
                    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯å†…å®¹ã‚‚èª­ã¿å–ã‚Š
                    if result_file.suffix == '.json':
                        try:
                            with open(result_file, 'r', encoding='utf-8') as f:
                                json_content = json.load(f)
                            file_info["content"] = json_content
                            
                            # integration_metadataã®å ´åˆã¯å¥‘ç´„ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æ‰±ã†
                            if 'integration_metadata' in result_file.name:
                                contract_data = json_content
                        except Exception as e:
                            logger.warning(f"Failed to read JSON file {result_file.name}: {e}")
                    
                    result_files.append(file_info)
        
        return {
            'success': True,
            'pdf_file': pdf_filename,
            'pdf_path': str(pdf_path),
            'result_files': result_files,
            'contract_data': contract_data,
            'pipeline_result': pipeline_result,
            'processing_time': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error processing test PDF: {str(e)}")
        return {
            'success': False,
            'error': str(e)
        }

def process_single_pdf(bucket_name: str, object_name: str, workspace_id: str, project_id: str) -> Dict[str, Any]:
    """
    å˜ä¸€ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    
    Args:
        bucket_name: GCSãƒã‚±ãƒƒãƒˆå
        object_name: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ (workspace_id/project_id/filename.pdf)
        workspace_id: ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ID
        project_id: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
    
    Returns:
        å‡¦ç†çµæœã‚’å«ã‚€è¾æ›¸
    """
    try:
        gcs_uri = f"gs://{bucket_name}/{object_name}"
        logger.info(f"Processing PDF from: {gcs_uri}")
        
        # Cloud Runå¯¾å¿œï¼šæ›¸ãè¾¼ã¿å¯èƒ½ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨
        pdf_dir = Path("/tmp/pdf")
        pdf_dir.mkdir(exist_ok=True)
        
        # GCSã‹ã‚‰PDFã‚’ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        filename = os.path.basename(object_name)
        local_file_path = pdf_dir / filename
        download_from_gcs(gcs_uri, str(local_file_path))
        
        logger.info(f"Starting OCR pipeline for: {local_file_path}")
        
        # main_pipeline.pyã‚’å®Ÿè¡Œ
        pipeline_result = run_main_pipeline(str(local_file_path))
        
        if not pipeline_result["success"]:
            logger.error(f"Pipeline execution failed: {pipeline_result.get('error')}")
            return {
                'success': False,
                'error': pipeline_result.get('error', 'Pipeline execution failed')
            }
        
        # å‡¦ç†å¾Œã«PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        if local_file_path.exists():
            local_file_path.unlink()
            logger.info(f"Cleaned up local PDF file: {local_file_path}")
        
        output_bucket = os.environ.get('GCS_BUCKET_NAME', bucket_name)
        
        # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        basename = os.path.splitext(filename)[0]
        
        # resultãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªãƒ»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        # Cloud Runã§ã¯çµæœã‚‚tmpã«å‡ºåŠ›ã•ã‚Œã‚‹å¯èƒ½æ€§ã‚’è€ƒæ…®
        project_root = get_project_root()
        result_dir = project_root / "result"
        if not result_dir.exists():
            result_dir = Path("/tmp/result")
        output_files = []
        
        if result_dir.exists():
            # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«ï¼‰
            for result_file in result_dir.glob("*"):
                if result_file.is_file():
                    # å¥‘ç´„æ›¸ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSONã¯ä¿å­˜ã—ãªã„
                    if result_file.suffix == '.json' and 'integration_metadata' in result_file.name:
                        logger.info(f"ğŸš« Skipping contract metadata JSON: {result_file.name}")
                        continue
                    
                    # ãã®ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ocr_resultsã«ä¿å­˜
                    output_prefix = f"{workspace_id}/{project_id}/ocr_results/"
                    gcs_path = upload_file_to_gcs(
                        str(result_file),
                        output_bucket,
                        output_prefix + result_file.name
                    )
                    output_files.append(gcs_path)
                    logger.info(f"âœ… Result file uploaded to: {gcs_path}")
        
        # çµ±åˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã—ã¦Geminiã§æ§‹é€ åŒ–
        structured_json_path = None
        logger.info(f"ğŸ” Looking for integrated files to structure. Found {len(output_files)} files: {output_files}")
        for output_file in output_files:
            logger.info(f"ğŸ” Checking file: {output_file}")
            # çµ±åˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆé€šå¸¸ã¯æœ€å¤§ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ã‚’ç‰¹å®š
            if 'integrated' in output_file or output_file.endswith('.txt'):
                logger.info(f"ğŸ¯ Found integrated file for structuring: {output_file}")
                try:
                    logger.info(f"ğŸ§  Starting Gemini structured output for: {output_file}")
                    # Geminiã®æ§‹é€ åŒ–å‡ºåŠ›ã‚’ä½¿ç”¨
                    structured_result = convert_to_contract_schema(output_file, basename)
                    if structured_result:
                        # æ§‹é€ åŒ–ã•ã‚ŒãŸJSONã‚’after_ocrã«ä¿å­˜
                        json_output_path = f"{workspace_id}/{project_id}/after_ocr/{basename}.json"
                        structured_json_path = upload_json_to_gcs(
                            structured_result,
                            output_bucket,
                            json_output_path
                        )
                        logger.info(f"âœ… Structured contract JSON saved to: {structured_json_path}")
                        break
                    else:
                        logger.warning(f"âš ï¸ Gemini structured output returned None for: {output_file}")
                except Exception as e:
                    logger.error(f"âŒ Failed to structure contract data for {output_file}: {str(e)}")
                    logger.error(f"âŒ Stack trace: {traceback.format_exc()}")
                    continue
            else:
                logger.info(f"â­ï¸ Skipping non-integrated file: {output_file}")
        
        return {
            'success': True,
            'output_files': output_files,
            'structured_json': structured_json_path,
            'pipeline_result': pipeline_result
        }
        
    except Exception as e:
        logger.error(f"Error processing PDF: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            'success': False,
            'error': str(e)
        }

def convert_to_contract_schema(gcs_file_path: str, basename: str) -> Optional[Dict[str, Any]]:
    """
    GCSã«ä¿å­˜ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’Geminiã®æ§‹é€ åŒ–å‡ºåŠ›ã‚’ä½¿ã£ã¦å¥‘ç´„æ›¸ã‚¹ã‚­ãƒ¼ãƒã«å¤‰æ›
    
    Args:
        gcs_file_path: GCSã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        basename: ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹å
    
    Returns:
        æ§‹é€ åŒ–ã•ã‚ŒãŸå¥‘ç´„æ›¸ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯None
    """
    try:
        # Gemini APIã®è¨­å®š
        genai.configure(api_key=os.environ.get('GEMINI_API_KEY'))
        
        # å¥‘ç´„æ›¸ã‚¹ã‚­ãƒ¼ãƒã®å®šç¾©
        contract_schema = {
            "type": "object",
            "properties": {
                "success": {"type": "boolean"},
                "info": {
                    "type": "object",
                    "properties": {
                        "title": {"type": "string"},
                        "party": {"type": "string"},  # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®å½“äº‹è€…å
                        "start_date": {"type": "string"},  # ç©ºæ–‡å­—åˆ—ã§å¯¾å¿œ
                        "end_date": {"type": "string"},  # ç©ºæ–‡å­—åˆ—ã§å¯¾å¿œ
                        "conclusion_date": {"type": "string"}  # ç©ºæ–‡å­—åˆ—ã§å¯¾å¿œ
                    },
                    "required": ["title", "party"]
                },
                "result": {
                    "type": "object",
                    "properties": {
                        "articles": {
                            "type": "array",
                            "items": {
                                "type": "object",
                                "properties": {
                                    "article_number": {"type": "string"},  # "ç¬¬1æ¡" ã¾ãŸã¯ "ç½²åæ¬„"
                                    "title": {"type": "string"},
                                    "content": {"type": "string"},
                                    "table_number": {"type": "string"}  # è¡¨ã®å ´åˆã®ã¿
                                },
                                "required": ["content", "title"]  # titleã‚‚å¿…é ˆã«ã™ã‚‹
                            }
                        }
                    },
                    "required": ["articles"]
                }
            },
            "required": ["success", "info", "result"]
        }
        
        # GCSã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿å–ã‚Š
        file_content = download_text_from_gcs(gcs_file_path)
        if not file_content:
            logger.warning(f"Could not read content from: {gcs_file_path}")
            return None
        
        # Geminiãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ï¼ˆæ§‹é€ åŒ–å‡ºåŠ›å¯¾å¿œï¼‰
        model = genai.GenerativeModel(
            'gemini-1.5-pro',
            generation_config={
                "response_mime_type": "application/json",
                "response_schema": contract_schema
            }
        )
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆ
        prompt = f"""
ä»¥ä¸‹ã®OCRå‡¦ç†æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æã—ã€å¥‘ç´„æ›¸ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ãƒ•ã‚¡ã‚¤ãƒ«å: {basename}

ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹:
{file_content}

æŠ½å‡ºæŒ‡ç¤º:
1. success: å¸¸ã«true
2. infoéƒ¨åˆ†:
   - title: å¥‘ç´„æ›¸ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½¿ç”¨ï¼‰
   - party: å¥‘ç´„å½“äº‹è€…ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¨˜è¼‰ï¼ˆä¾‹: "æ ªå¼ä¼šç¤¾A,æ ªå¼ä¼šç¤¾B"ï¼‰
   - start_date: å¥‘ç´„é–‹å§‹æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰
   - end_date: å¥‘ç´„çµ‚äº†æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰
   - conclusion_date: å¥‘ç´„ç· çµæ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰

3. resultéƒ¨åˆ†:
   - articles: å¥‘ç´„æ¡é …ã®é…åˆ—
     - article_number: æ¡é …ç•ªå·ï¼ˆä¾‹: "ç¬¬1æ¡"ã€"ç½²åæ¬„"ç­‰ï¼‰
     - title: æ¡é …ã®ã‚¿ã‚¤ãƒˆãƒ«
     - content: æ¡é …ã®å†…å®¹
     - table_number: è¡¨ãŒã‚ã‚‹å ´åˆã®ã¿è¡¨ç•ªå·

æ³¨æ„äº‹é …:
- æ—¥ä»˜ã¯å¯èƒ½ãªé™ã‚ŠYYYY-MM-DDå½¢å¼ã«å¤‰æ›ã—ã¦ãã ã•ã„
- è¡¨ã‚„å›³ãŒã‚ã‚‹å ´åˆã¯é©åˆ‡ã«èª¬æ˜ã‚’å«ã‚ã¦ãã ã•ã„
- ç½²åæ¬„ã‚‚1ã¤ã®æ¡é …ã¨ã—ã¦æ‰±ã£ã¦ãã ã•ã„
"""
        
        # Geminiã«é€ä¿¡ã—ã¦æ§‹é€ åŒ–å‡ºåŠ›ã‚’å–å¾—
        response = model.generate_content(prompt)
        
        # JSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹
        structured_data = json.loads(response.text)
        
        logger.info(f"Successfully structured contract data with {len(structured_data.get('result', {}).get('articles', []))} articles")
        
        return structured_data
        
    except Exception as e:
        logger.error(f"Error in Gemini structured output: {str(e)}")
        return None


def download_text_from_gcs(gcs_path: str) -> Optional[str]:
    """
    GCSã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª­ã¿å–ã‚Š
    
    Args:
        gcs_path: GCSã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (gs://bucket/path/to/file.txt)
    
    Returns:
        ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã¾ãŸã¯None
    """
    try:
        from google.cloud import storage
        
        # GCS URIã‚’ãƒ‘ãƒ¼ã‚¹
        if not gcs_path.startswith('gs://'):
            return None
            
        path_parts = gcs_path.replace('gs://', '').split('/', 1)
        bucket_name = path_parts[0]
        blob_name = path_parts[1]
        
        # GCSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿å–ã‚Š
        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(blob_name)
        
        # ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦èª­ã¿å–ã‚Š
        content = blob.download_as_text(encoding='utf-8')
        
        return content
        
    except Exception as e:
        logger.error(f"Error downloading text from GCS: {str(e)}")
        return None

def download_from_gcs(gcs_uri: str, local_path: str) -> str:
    """
    GCSã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    """
    from google.cloud import storage
    import urllib.parse
    
    if not gcs_uri.startswith('gs://'):
        raise ValueError(f"Invalid GCS URI: {gcs_uri}")
    
    path_parts = gcs_uri[5:].split('/', 1)
    if len(path_parts) != 2:
        raise ValueError(f"Invalid GCS URI format: {gcs_uri}")
    
    bucket_name, blob_path = path_parts
    
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    
    # Try to find the blob directly first
    try:
        blob = bucket.blob(blob_path)
        logger.info(f"Downloading {gcs_uri} to {local_path}")
        blob.download_to_filename(local_path)
        return local_path
    except Exception as e:
        logger.warning(f"Direct download failed: {e}")
        
        # If direct access fails, try to list and find matching blobs
        logger.info(f"Searching for blobs matching: {blob_path}")
        
        # Get the directory path and filename
        path_parts = blob_path.split('/')
        if len(path_parts) > 1:
            prefix = '/'.join(path_parts[:-1]) + '/'
            target_filename = path_parts[-1]
        else:
            prefix = ''
            target_filename = blob_path
            
        logger.info(f"Listing blobs with prefix: {prefix}")
        blobs = list(bucket.list_blobs(prefix=prefix))
        logger.info(f"Found {len(blobs)} blobs with prefix")
        
        # Look for exact or partial matches
        for blob in blobs:
            blob_filename = blob.name.split('/')[-1]
            logger.info(f"Checking blob: {blob.name} (filename: {blob_filename})")
            
            # Normalize Unicode for comparison
            import unicodedata
            normalized_blob_name = unicodedata.normalize('NFC', blob.name)
            normalized_blob_filename = unicodedata.normalize('NFC', blob_filename)
            normalized_target_filename = unicodedata.normalize('NFC', target_filename)
            normalized_blob_path = unicodedata.normalize('NFC', blob_path)
            
            # Try multiple matching strategies with normalized strings
            match_found = (
                normalized_blob_filename == normalized_target_filename or  # Exact match
                normalized_target_filename in normalized_blob_filename or  # Target is substring of blob
                normalized_blob_filename in normalized_target_filename or  # Blob is substring of target
                normalized_blob_name == normalized_blob_path or           # Full path exact match
                normalized_blob_name.endswith(normalized_target_filename) or  # Ends with target filename
                # Also try without normalization for backward compatibility
                blob_filename == target_filename or
                target_filename in blob_filename or
                blob_filename in target_filename or
                blob.name == blob_path or
                blob.name.endswith(target_filename)
            )
            
            if match_found:
                logger.info(f"Found matching blob: {blob.name}")
                try:
                    blob.download_to_filename(local_path)
                    return local_path
                except Exception as download_error:
                    logger.warning(f"Failed to download {blob.name}: {download_error}")
                    continue
                    
        raise FileNotFoundError(f"Could not find or download blob matching: {blob_path}")
    
    return local_path

def upload_file_to_gcs(local_path: str, bucket_name: str, blob_name: str) -> str:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    """
    from google.cloud import storage
    
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(blob_name)
    
    logger.info(f"Uploading {local_path} to gs://{bucket_name}/{blob_name}")
    blob.upload_from_filename(local_path)
    
    return f"gs://{bucket_name}/{blob_name}"

def upload_json_to_gcs(json_data: Dict[str, Any], bucket_name: str, blob_path: str) -> str:
    """
    JSONãƒ‡ãƒ¼ã‚¿ã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    """
    from google.cloud import storage
    
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(blob_path)
    
    json_str = json.dumps(json_data, ensure_ascii=False, indent=2)
    
    logger.info(f"Uploading JSON to gs://{bucket_name}/{blob_path}")
    blob.upload_from_string(json_str, content_type='application/json')
    
    return f"gs://{bucket_name}/{blob_path}"

def upload_results_to_gcs(result: Dict[str, Any], bucket_name: str, prefix: str) -> str:
    """
    å‡¦ç†çµæœã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    """
    from google.cloud import storage
    
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    
    result_json = json.dumps(result, ensure_ascii=False, indent=2)
    blob_name = f"{prefix}result.json"
    blob = bucket.blob(blob_name)
    
    logger.info(f"Uploading results to gs://{bucket_name}/{blob_name}")
    blob.upload_from_string(result_json, content_type='application/json')
    
    return f"gs://{bucket_name}/{blob_name}"


if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8080))
    app.run(host='0.0.0.0', port=port, debug=False)