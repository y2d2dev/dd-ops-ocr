import os
import json
import base64
import logging
import shutil
from flask import Flask, request, jsonify
from datetime import datetime
import traceback
from typing import Dict, Any, Optional
from google.cloud import storage
import subprocess
import sys
from pathlib import Path

# UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç¢ºå®Ÿã«ã™ã‚‹
os.environ.setdefault('PYTHONIOENCODING', 'utf-8')
os.environ.setdefault('LANG', 'ja_JP.UTF-8')
os.environ.setdefault('LC_ALL', 'ja_JP.UTF-8')
from src.api.model_downloader import ensure_models_available

app = Flask(__name__)

# UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç¢ºå®Ÿã«ã™ã‚‹
app.config['JSON_AS_ASCII'] = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Cloud Runèµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
def initialize_models():
    """èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆ512MBæ§‹æˆç”¨ã«è»½é‡åŒ–ï¼‰"""
    logger.info("ğŸš€ Initializing models (lightweight mode)...")
    
    # ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ãŒã‚ã‚‹å ´åˆã¯ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—
    memory_limit = os.environ.get('CLOUD_RUN_MEMORY', '512Mi')
    if '512' in memory_limit:
        logger.info("ğŸ’¡ 512MBæ§‹æˆæ¤œå‡º - ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        logger.info("ğŸ”§ ãƒ¢ãƒ‡ãƒ«ã¯å‡¦ç†æ™‚ã«å‹•çš„ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™")
        return
    
    try:
        success = ensure_models_available()
        if success:
            logger.info("âœ… Models initialized successfully")
        else:
            logger.warning("âš ï¸ Some models may be missing")
    except Exception as e:
        logger.error(f"âŒ Failed to initialize models: {e}")
        logger.warning("âš ï¸ Continuing without models - OCR features may be limited")

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
initialize_models()

def get_project_root():
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—"""
    # Cloud Runã§ã¯/appé…ä¸‹ã«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒé…ç½®ã•ã‚Œã‚‹
    if Path("/app").exists():
        return Path("/app")
    else:
        # ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒ
        return Path(__file__).parent.parent.parent

def run_main_pipeline(pdf_path: str) -> Dict[str, Any]:
    """
    main_pipeline.pyã‚’å®Ÿè¡Œã—ã¦OCRå‡¦ç†ã‚’è¡Œã†
    
    Args:
        pdf_path: å‡¦ç†å¯¾è±¡ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        
    Returns:
        Dict: å‡¦ç†çµæœ
    """
    try:
        project_root = get_project_root()
        main_pipeline_path = project_root / "src" / "main_pipeline.py"
        
        if not main_pipeline_path.exists():
            raise FileNotFoundError(f"main_pipeline.py not found: {main_pipeline_path}")
        
        # main_pipeline.pyã‚’å®Ÿè¡Œ
        cmd = [
            sys.executable,
            str(main_pipeline_path),
            "--input", pdf_path
        ]
        
        logger.info(f"ğŸš€ Running main_pipeline.py with command: {' '.join(cmd)}")
        logger.info(f"ğŸ” Working directory for subprocess: {project_root}")

        result = subprocess.run(
            cmd,
            cwd=str(project_root),
            capture_output=True,
            text=True,
            encoding='utf-8'
        )
        
        if result.returncode == 0:
            logger.info("âœ… main_pipeline.py executed successfully")
            return {
                "success": True,
                "stdout": result.stdout,
                "stderr": result.stderr
            }
        else:
            logger.error(f"âŒ main_pipeline.py failed with return code: {result.returncode}")
            logger.error(f"âŒ STDOUT: {result.stdout}")
            logger.error(f"âŒ STDERR: {result.stderr}")
            return {
                "success": False,
                "error": f"Pipeline execution failed: {result.stderr}",
                "stdout": result.stdout,
                "stderr": result.stderr
            }

    except Exception as e:
        logger.error(f"âŒ Error running main_pipeline.py: {e}")
        return {
            "success": False,
            "error": str(e)
        }

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({'status': 'healthy', 'timestamp': datetime.utcnow().isoformat()}), 200

@app.route('/debug-blobs', methods=['GET'])
def debug_blobs():
    """GCSå†…ã®blobã‚’ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒªã‚¹ãƒˆã™ã‚‹"""
    try:
        
        prefix = request.args.get('prefix', '')
        bucket_name = 'app_contracts_staging'
        
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        
        blobs = list(bucket.list_blobs(prefix=prefix))
        
        blob_info = []
        for blob in blobs:
            blob_info.append({
                'name': blob.name,
                'size': blob.size,
                'created': blob.time_created.isoformat() if blob.time_created else None,
                'content_type': blob.content_type
            })
        
        return jsonify({
            'bucket': bucket_name,
            'prefix': prefix,
            'total_blobs': len(blob_info),
            'blobs': blob_info
        }), 200
        
    except Exception as e:
        logger.error(f"Debug blobs error: {str(e)}")
        return jsonify({
            'error': str(e)
        }), 500

@app.route('/', methods=['GET'])
def root():
    """
    ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ - APIæƒ…å ±ã‚’è¿”ã™
    """
    return jsonify({
        'service': 'DD-OPS OCR API',
        'version': '1.0.0',
        'endpoints': {
            '/health': 'Health check',
            '/ocr [GET]': 'Test OCR with local PDF files',
            '/ocr [POST]': 'OCR with GCS PDF URL',
            '/pubsub/push [POST]': 'PubSub webhook for automatic processing'
        },
        'usage': {
            'GET /ocr': 'Process PDF from /pdf/ directory',
            'GET /ocr?file=test.pdf': 'Process specific PDF file',
            'POST /ocr': 'Send {"pdf_url": "gs://bucket/file.pdf", "workspace_id": "ws", "project_id": "proj"}'
        },
        'timestamp': datetime.utcnow().isoformat()
    }), 200

@app.route('/ocr', methods=['GET'])
def ocr_test():
    """
    GETãƒ¡ã‚½ãƒƒãƒ‰ã§OCRå‡¦ç†ã‚’ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    
    ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
        file: PDFãƒ•ã‚¡ã‚¤ãƒ«å (ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯è‡ªå‹•æ¤œå‡º)
        
    ä¾‹: /ocr?file=test.pdf
    """
    try:
        logger.info("ğŸš€ GET /ocr endpoint called")
        
        # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—
        pdf_filename = request.args.get('file')
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®OCRå‡¦ç†ã‚’å®Ÿè¡Œ
        result = process_test_pdf(pdf_filename)
        
        response = {
            "status": "completed",
            "method": "GET",
            "timestamp": datetime.now().isoformat(),
            "result": result
        }
        
        return jsonify(response), 200
        
    except Exception as e:
        logger.error(f"OCR test endpoint error: {str(e)}")
        return jsonify({
            "status": "error",
            "method": "GET",
            "timestamp": datetime.now().isoformat(),
            "error": str(e)
        }), 500

@app.route('/ocr', methods=['POST'])
def ocr_upload():
    """
    POSTãƒ¡ã‚½ãƒƒãƒ‰ã§OCRå‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    
    JSON body:
        {
            "pdf_url": "gs://bucket/path/to/file.pdf",
            "workspace_id": "workspace123",
            "project_id": "project456"
        }
    """
    try:
        logger.info("ğŸš€ POST /ocr endpoint called")
        
        data = request.get_json()
        if not data:
            return jsonify({
                "status": "error",
                "error": "JSON body required"
            }), 400
        
        pdf_url = data.get('pdf_url')
        workspace_id = data.get('workspace_id', 'test_workspace')
        project_id = data.get('project_id', 'test_project')
        
        if not pdf_url:
            return jsonify({
                "status": "error",
                "error": "pdf_url is required"
            }), 400
        
        # GCS URIã®å ´åˆ
        if pdf_url.startswith('gs://'):
            bucket_name = pdf_url.replace('gs://', '').split('/')[0]
            object_name = '/'.join(pdf_url.replace('gs://', '').split('/')[1:])
            
            result = process_single_pdf(bucket_name, object_name, workspace_id, project_id)
        else:
            return jsonify({
                "status": "error",
                "error": "Only gs:// URLs are supported"
            }), 400
        
        response = {
            "status": "completed",
            "method": "POST",
            "timestamp": datetime.now().isoformat(),
            "workspace_id": workspace_id,
            "project_id": project_id,
            "result": result
        }
        
        return jsonify(response), 200
        
    except Exception as e:
        logger.error(f"OCR upload endpoint error: {str(e)}")
        return jsonify({
            "status": "error",
            "method": "POST",
            "timestamp": datetime.now().isoformat(),
            "error": str(e)
        }), 500

@app.route('/pubsub/push', methods=['POST'])
def pubsub_push():
    """
    Cloud PubSub Pushé€šçŸ¥ã‚’å—ã‘å–ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    GCS Storage Object notificationã‚’å‡¦ç†
    """
    try:
        logger.info("="*80)
        logger.info("ğŸ”µ PUBSUB PUSH REQUEST RECEIVED")
        logger.info("="*80)

        logger.info(f"ğŸ“Œ Request Method: {request.method}")
        logger.info(f"ğŸ“Œ Request URL: {request.url}")
        logger.info(f"ğŸ“Œ Request Path: {request.path}")

        logger.info("ğŸ“‹ REQUEST HEADERS:")
        for key, value in request.headers:
            logger.info(f"  - {key}: {value}")

        logger.info(f"ğŸ“ Content-Type: {request.content_type}")
        logger.info(f"ğŸ“ Content-Length: {request.content_length}")

        logger.info(f"ğŸ“¦ Raw Request Data: {request.data}")
        logger.info(f"ğŸ“¦ Request Data Type: {type(request.data)}")
        logger.info(f"ğŸ“¦ Request Data Length: {len(request.data) if request.data else 0}")

        logger.info("ğŸ” Attempting to parse JSON...")
        envelope = None
        try:
            envelope = request.get_json()
            logger.info(f"âœ… JSON parsed successfully")
            logger.info(f"ğŸ“Š Envelope type: {type(envelope)}")
            logger.info(f"ğŸ“Š Envelope keys: {list(envelope.keys()) if envelope else 'None'}")
            logger.info(f"ğŸ“Š Full envelope content: {json.dumps(envelope, indent=2) if envelope else 'None'}")
        except Exception as json_error:
            logger.error(f"âŒ JSON parsing failed: {str(json_error)}")
            logger.error(f"âŒ Error type: {type(json_error).__name__}")

        if not envelope:
            logger.error("âŒ No PubSub message received (envelope is None or empty)")
            return jsonify({"error": "Bad Request: no PubSub message received"}), 400

        # Check delivery attempt and skip if too many retries
        delivery_attempt = envelope.get('deliveryAttempt', 0)
        logger.info(f"ğŸ“¬ Delivery attempt: {delivery_attempt}")

        if delivery_attempt > 2:
            logger.warning(f"âš ï¸ Skipping message after {delivery_attempt} delivery attempts")
            return jsonify({"status": "skipped", "reason": f"Too many retries ({delivery_attempt})"}), 200
            
        if not isinstance(envelope, dict) or "message" not in envelope:
            logger.error(f"âŒ Invalid PubSub message format - envelope type: {type(envelope)}, has 'message' key: {'message' in envelope if isinstance(envelope, dict) else 'N/A'}")
            return jsonify({"error": "Bad Request: invalid PubSub message format"}), 400
            
        pubsub_message = envelope["message"]
        logger.info("ğŸ“¨ PUBSUB MESSAGE:")
        logger.info(f"  - Message type: {type(pubsub_message)}")
        logger.info(f"  - Message keys: {list(pubsub_message.keys()) if isinstance(pubsub_message, dict) else 'Not a dict'}")
        logger.info(f"  - Full message: {json.dumps(pubsub_message, indent=2)}")
        
        if isinstance(pubsub_message.get("data"), str):
            try:
                logger.info("ğŸ”“ Attempting Base64 decode...")
                logger.info(f"  - Data length (encoded): {len(pubsub_message['data'])}")
                logger.info(f"  - First 100 chars: {pubsub_message['data'][:100]}...")
                
                message_data = base64.b64decode(pubsub_message["data"]).decode("utf-8")
                logger.info(f"âœ… Base64 decode successful")
                logger.info(f"  - Decoded length: {len(message_data)}")
                logger.info(f"  - First 200 chars: {message_data[:200] if message_data else 'Empty'}...")
                
                if not message_data or not message_data.strip():
                    logger.warning(f"âš ï¸ Decoded message is empty or whitespace only")
                    return jsonify({"status": "ignored", "reason": "Empty message"}), 200
                
                storage_object = json.loads(message_data)
                logger.info(f"âœ… JSON parse of decoded data successful")
                logger.info(f"ğŸ“„ Storage Object keys: {list(storage_object.keys())}")
                logger.info(f"ğŸ“„ Full Storage Object: {json.dumps(storage_object, indent=2)}")
            except Exception as e:
                logger.error(f"âŒ Failed to decode PubSub message: {str(e)}")
                logger.error(f"âŒ Error type: {type(e).__name__}")
                logger.error(f"âŒ Stack trace: {traceback.format_exc()}")
                return jsonify({"error": "Bad Request: invalid message data"}), 400
        else:
            logger.error(f"âŒ PubSub message data is not a string, type: {type(pubsub_message.get('data'))}")
            return jsonify({"error": "Bad Request: message data must be base64 encoded"}), 400
            
        if not isinstance(storage_object, dict):
            logger.error(f"âŒ Invalid Storage Object format - type: {type(storage_object)}")
            return jsonify({"error": "Bad Request: invalid Storage Object"}), 400

        # Testç”¨ã§idãŒç„¡ã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆ
        if "id" not in storage_object:
            logger.warning("âš ï¸ Storage Object has no 'id' field, generating one for testing...")
            storage_object["id"] = f"test-{storage_object.get('name', 'unknown')}-{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
        object_id = storage_object.get("id", "")
        object_name = storage_object.get("name", "")
        object_bucket = storage_object.get("bucket", "")
        
        logger.info("ğŸ—‚ï¸ STORAGE OBJECT INFO:")
        logger.info(f"  - Object ID: {object_id}")
        logger.info(f"  - Object Name: {object_name}")
        logger.info(f"  - Bucket: {object_bucket}")
        logger.info(f"  - Content Type: {storage_object.get('contentType', 'N/A')}")
        logger.info(f"  - Size: {storage_object.get('size', 'N/A')} bytes")
        
        name_parts = object_name.split("/")
        logger.info(f"ğŸ“‚ Name parts: {name_parts}")
        logger.info(f"ğŸ“‚ Number of parts: {len(name_parts)}")
        
        if len(name_parts) < 3:
            logger.error(f"âŒ Invalid object name format: {object_name} - expected at least 3 parts")
            return jsonify({"error": "Invalid object name format"}), 400
            
        workspace_id = name_parts[0]
        project_id = name_parts[1]
        filename = "/".join(name_parts[2:])
        
        logger.info("âœ¨ EXTRACTED INFO:")
        logger.info(f"  - Workspace ID: {workspace_id}")
        logger.info(f"  - Project ID: {project_id}")
        logger.info(f"  - Filename: {filename}")
        
        if not filename.lower().endswith('.pdf'):
            logger.info(f"âš ï¸ Ignoring non-PDF file: {filename}")
            return jsonify({"message": "File ignored (not a PDF)"}), 200
            
        logger.info(f"ğŸš€ Starting PDF processing - workspace: {workspace_id}, project: {project_id}, file: {filename}")
        
        result = process_single_pdf(object_bucket, object_name, workspace_id, project_id)
        
        response = {
            "workspace_id": workspace_id,
            "project_id": project_id,
            "file": filename,
            "success": result["success"],
            "timestamp": datetime.now().isoformat()
        }
        
        if result["success"]:
            response["contract_json"] = result.get("contract_json", "")
            response["output_files"] = result.get("output_files", [])
            logger.info(f"Successfully processed PDF: {filename}")
        else:
            response["error"] = result.get("error", "Processing failed")
            logger.error(f"Failed to process PDF: {filename} - {result.get('error')}")
            
        return jsonify(response), 200
        
    except Exception as e:
        logger.error(f"Unexpected error in PubSub handler: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            "error": "Internal server error",
            "message": str(e)
        }), 200

def process_test_pdf(pdf_filename: Optional[str] = None) -> Dict[str, Any]:
    """
    ãƒ†ã‚¹ãƒˆç”¨ã®PDFå‡¦ç†ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã®pdf/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ï¼‰
    
    Args:
        pdf_filename: å‡¦ç†ã™ã‚‹PDFãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
    Returns:
        å‡¦ç†çµæœã‚’å«ã‚€è¾æ›¸
    """
    try:
        project_root = get_project_root()
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ã®pdf/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰PDFã‚’æ¤œç´¢
        local_pdf_dir = project_root / "pdf"
        
        if not local_pdf_dir.exists():
            return {
                'success': False,
                'error': 'PDF directory not found. Please place PDF files in /pdf/ directory.'
            }
        
        # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®æ±ºå®š
        if pdf_filename:
            pdf_path = local_pdf_dir / pdf_filename
            if not pdf_path.exists():
                return {
                    'success': False,
                    'error': f'PDF file not found: {pdf_filename}'
                }
        else:
            # è‡ªå‹•æ¤œå‡ºï¼šæœ€åˆã«è¦‹ã¤ã‹ã£ãŸPDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
            pdf_files = list(local_pdf_dir.glob("*.pdf"))
            if not pdf_files:
                return {
                    'success': False,
                    'error': 'No PDF files found in /pdf/ directory'
                }
            pdf_path = pdf_files[0]
            pdf_filename = pdf_path.name
        
        logger.info(f"Processing local PDF: {pdf_path}")
        
        # main_pipeline.pyã‚’å®Ÿè¡Œ
        pipeline_result = run_main_pipeline(str(pdf_path))
        
        if not pipeline_result["success"]:
            logger.error(f"Pipeline execution failed: {pipeline_result.get('error')}")
            return {
                'success': False,
                'error': pipeline_result.get('error', 'Pipeline execution failed'),
                'pipeline_output': pipeline_result
            }
        
        # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†
        basename = os.path.splitext(pdf_filename)[0]
        result_files = []
        contract_data = None
        
        # resultãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒ£ãƒ³
        result_dir = project_root / "result"
        if not result_dir.exists():
            result_dir = Path("/tmp/result")
        
        if result_dir.exists():
            for result_file in result_dir.glob("*"):
                if result_file.is_file():
                    file_info = {
                        "filename": result_file.name,
                        "size": result_file.stat().st_size,
                        "path": str(result_file)
                    }
                    
                    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯å†…å®¹ã‚‚èª­ã¿å–ã‚Š
                    if result_file.suffix == '.json':
                        try:
                            with open(result_file, 'r', encoding='utf-8') as f:
                                json_content = json.load(f)
                            file_info["content"] = json_content
                            
                            # integration_metadataã®å ´åˆã¯å¥‘ç´„ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æ‰±ã†
                            if 'integration_metadata' in result_file.name:
                                contract_data = json_content
                        except Exception as e:
                            logger.warning(f"Failed to read JSON file {result_file.name}: {e}")
                    
                    result_files.append(file_info)
        
        return {
            'success': True,
            'pdf_file': pdf_filename,
            'pdf_path': str(pdf_path),
            'result_files': result_files,
            'contract_data': contract_data,
            'pipeline_result': pipeline_result,
            'processing_time': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error processing test PDF: {str(e)}")
        return {
            'success': False,
            'error': str(e)
        }

def process_single_pdf(bucket_name: str, object_name: str, workspace_id: str, project_id: str) -> Dict[str, Any]:
    """
    å˜ä¸€ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    
    Args:
        bucket_name: GCSãƒã‚±ãƒƒãƒˆå
        object_name: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ (workspace_id/project_id/filename.pdf)
        workspace_id: ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ID
        project_id: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
    
    Returns:
        å‡¦ç†çµæœã‚’å«ã‚€è¾æ›¸
    """
    try:
        gcs_uri = f"gs://{bucket_name}/{object_name}"
        logger.info(f"Processing PDF from: {gcs_uri}")
        
        # Cloud Runå¯¾å¿œï¼šæ›¸ãè¾¼ã¿å¯èƒ½ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨
        pdf_dir = Path("/tmp/pdf")
        pdf_dir.mkdir(exist_ok=True)
        
        # GCSã‹ã‚‰PDFã‚’ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        filename = os.path.basename(object_name)
        local_file_path = pdf_dir / filename
        download_from_gcs(gcs_uri, str(local_file_path))

        # å‡¦ç†é–‹å§‹å‰ã«resultãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        project_root = get_project_root()
        result_dir = project_root / "result"
        if not result_dir.exists():
            result_dir = Path("/tmp/result")

        # è©³ç´°ãƒ­ã‚°è¿½åŠ ã§ãƒ‡ãƒãƒƒã‚°
        logger.info(f"ğŸ” project_root: {project_root}")
        logger.info(f"ğŸ” result_dir: {result_dir}")
        logger.info(f"ğŸ” result_dir.exists(): {result_dir.exists()}")

        # é‡è¦: Cloud Runã§ã¯æ¯å›æ–°ã—ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒèµ·å‹•ã•ã‚Œã‚‹ã¯ãšã ãŒã€
        # Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã«å¤ã„resultãƒ•ã‚¡ã‚¤ãƒ«ãŒå«ã¾ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€
        # å‡¦ç†é–‹å§‹å‰ã«å¿…ãšresultãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if result_dir.exists():
            logger.info(f"ğŸ§¹ Cleaning up result directory before processing: {result_dir}")
            for old_file in result_dir.glob("*"):
                if old_file.is_file():
                    try:
                        old_file.unlink()
                        logger.info(f"ğŸ—‘ï¸ Deleted old file: {old_file.name}")
                    except Exception as e:
                        logger.warning(f"Failed to delete old file {old_file.name}: {e}")

        logger.info(f"Starting OCR pipeline for: {local_file_path}")

        # main_pipeline.pyã‚’å®Ÿè¡Œ
        pipeline_result = run_main_pipeline(str(local_file_path))

        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œå¾Œã®è©³ç´°ãƒ­ã‚°
        logger.info(f"ğŸ” After pipeline execution:")
        logger.info(f"ğŸ” result_dir.exists(): {result_dir.exists()}")
        if result_dir.exists():
            all_files = list(result_dir.glob("*"))
            logger.info(f"ğŸ” Files in result_dir: {[f.name for f in all_files]}")
        else:
            logger.warning(f"âš ï¸ result_dir does not exist: {result_dir}")

        # è¿½åŠ ã®å€™è£œãƒ‘ã‚¹ã‚‚ãƒã‚§ãƒƒã‚¯
        alternative_paths = [
            Path("/app/result"),
            project_root / "src" / "result",
            Path.cwd() / "result",
            Path("/tmp/result"),
            Path("/app/src/result")
        ]

        for alt_path in alternative_paths:
            logger.info(f"ğŸ” Checking alternative path: {alt_path}")
            logger.info(f"ğŸ” Path exists: {alt_path.exists()}")
            if alt_path.exists():
                files = list(alt_path.glob("*"))
                logger.info(f"ğŸ” Files in {alt_path}: {[f.name for f in files]}")
                if files:
                    logger.info(f"ğŸ“ Found {len(files)} files in {alt_path} - using this as result directory")
                    result_dir = alt_path  # å®Ÿéš›ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´æ‰€ã‚’ä½¿ç”¨
                    break

        if not pipeline_result["success"]:
            logger.error(f"Pipeline execution failed: {pipeline_result.get('error')}")
            return {
                'success': False,
                'error': pipeline_result.get('error', 'Pipeline execution failed')
            }

        # å‡¦ç†å¾Œã«PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        if local_file_path.exists():
            local_file_path.unlink()
            logger.info(f"Cleaned up local PDF file: {local_file_path}")

        output_bucket = os.environ.get('GCS_BUCKET_NAME', bucket_name)

        # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ - ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œå¾Œã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
        basename = os.path.splitext(filename)[0]
        output_files = []
        
        txt_files_to_delete = []  # å‰Šé™¤äºˆå®šã®txtãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½è·¡

        if result_dir.exists():
            # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«ï¼‰
            # é‡è¦: ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å‡¦ç†ã™ã‚‹
            all_files = list(result_dir.glob("*"))
            logger.info(f"ğŸ“‚ Found {len(all_files)} files in result directory for basename '{basename}'")
            for result_file in all_files:
                if result_file.is_file():
                    logger.info(f"ğŸ” Processing file: {result_file.name}")
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯ã®ã¿å®Ÿè¡Œ
                    # å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«åˆ¤å®šã¯å‰Šé™¤ã—ã€ç¾åœ¨ã®PDFã«é–¢é€£ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç†

                    # ãƒ•ã‚¡ã‚¤ãƒ«åã«ç¾åœ¨ã®PDFã®basenameãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    if basename not in result_file.name:
                        logger.info(f"â­ï¸ Skipping file from different PDF: {result_file.name}")
                        continue
                    # å¥‘ç´„æ›¸ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSONã¯ä¿å­˜ã—ãªã„
                    if result_file.suffix == '.json' and 'integration_metadata' in result_file.name:
                        logger.info(f"ğŸš« Skipping contract metadata JSON: {result_file.name}")
                        continue

                    # txtãƒ•ã‚¡ã‚¤ãƒ«ã¯æ§‹é€ åŒ–å‡¦ç†ã§ä½¿ç”¨ã—ã€ãã®å¾Œå‰Šé™¤
                    if result_file.suffix == '.txt':
                        logger.info(f"ğŸ“ Found txt file for local processing: {result_file.name}")
                        txt_files_to_delete.append(result_file)
                        # GCSã«ã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã›ãšã€ãƒ­ãƒ¼ã‚«ãƒ«ã§å‡¦ç†
                        continue

                    # ãã®ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ocr_resultsã«ä¿å­˜
                    output_prefix = f"{workspace_id}/{project_id}/ocr_results/"
                    gcs_path = upload_file_to_gcs(
                        str(result_file),
                        output_bucket,
                        output_prefix + result_file.name
                    )
                    output_files.append(gcs_path)
                    logger.info(f"âœ… Result file uploaded to: {gcs_path}")

        # ãƒ­ãƒ¼ã‚«ãƒ«ã®txtãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦Geminiã§æ§‹é€ åŒ–
        structured_json_path = None
        logger.info(f"ğŸ” Looking for local txt files to structure. Found {len(txt_files_to_delete)} txt files")
        for txt_file in txt_files_to_delete:
            # çµ±åˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆintegratedã‚’å«ã‚€ï¼‰ã‚’ç‰¹å®š
            if 'integrated' in txt_file.name:
                logger.info(f"ğŸ¯ Found integrated file for structuring: {txt_file.name}")
                try:
                    logger.info(f"ğŸ§  Starting Gemini structured output for local file: {txt_file.name}")
                    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç›´æ¥ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿
                    with open(txt_file, 'r', encoding='utf-8') as f:
                        file_content = f.read()

                    # Geminiã®æ§‹é€ åŒ–å‡ºåŠ›ã‚’ä½¿ç”¨ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç‰ˆï¼‰
                    structured_result = convert_local_text_to_contract_schema(file_content, basename, workspace_id, project_id, output_bucket)
                    if structured_result:
                        # æ§‹é€ åŒ–ã•ã‚ŒãŸJSONã‚’after_ocrã«ä¿å­˜
                        json_output_path = f"{workspace_id}/{project_id}/after_ocr/{basename}.json"
                        structured_json_path = upload_json_to_gcs(
                            structured_result,
                            output_bucket,
                            json_output_path
                        )
                        logger.info(f"âœ… Structured contract JSON saved to: {structured_json_path}")
                        break
                    else:
                        logger.warning(f"âš ï¸ Gemini structured output returned None for: {txt_file.name}")
                except Exception as e:
                    logger.error(f"âŒ Failed to structure contract data for {txt_file.name}: {str(e)}")
                    logger.error(f"âŒ Stack trace: {traceback.format_exc()}")
                    continue

        # æ§‹é€ åŒ–å‡¦ç†ãŒå®Œäº†ã—ãŸã‚‰ã€ãƒ­ãƒ¼ã‚«ãƒ«ã®txtãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        for txt_file in txt_files_to_delete:
            try:
                txt_file.unlink()
                logger.info(f"ğŸ—‘ï¸ Deleted local txt file: {txt_file.name}")
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to delete txt file {txt_file.name}: {e}")

        # GCSä¸Šã«txtãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ãªã„ãŸã‚ã€å‰Šé™¤å‡¦ç†ã¯ä¸è¦
        
        return {
            'success': True,
            'output_files': output_files,
            'structured_json': structured_json_path,
            'pipeline_result': pipeline_result
        }
        
    except Exception as e:
        logger.error(f"Error processing PDF: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            'success': False,
            'error': str(e)
        }

def convert_local_text_to_contract_schema(file_content: str, basename: str, workspace_id: str, project_id: str, bucket_name: str) -> Optional[Dict[str, Any]]:
    """
    ãƒ­ãƒ¼ã‚«ãƒ«ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’Vertex AIã®æ§‹é€ åŒ–å‡ºåŠ›ã‚’ä½¿ã£ã¦å¥‘ç´„æ›¸ã‚¹ã‚­ãƒ¼ãƒã«å¤‰æ›

    Args:
        file_content: ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹
        basename: ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹å
        workspace_id: ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ID
        project_id: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
        bucket_name: GCSãƒã‚±ãƒƒãƒˆå

    Returns:
        æ§‹é€ åŒ–ã•ã‚ŒãŸå¥‘ç´„æ›¸ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯None
    """
    try:
        # Vertex AIè¨­å®š
        import vertexai
        from vertexai.generative_models import GenerativeModel, GenerationConfig

        project_id = os.getenv('GCP_PROJECT_ID')
        location = os.getenv('GCP_LOCATION', 'us-central1')
        if not project_id:
            logger.error("GCP_PROJECT_IDç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None

        vertexai.init(project=project_id, location=location)

        # å¥‘ç´„æ›¸ã‚¹ã‚­ãƒ¼ãƒã®å®šç¾©
        contract_schema = {
            "type": "object",
            "properties": {
                "success": {"type": "boolean"},
                "info": {
                    "type": "object",
                    "properties": {
                        "title": {"type": "string"},
                        "party": {"type": "string"},  # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®å½“äº‹è€…å
                        "start_date": {"type": "string"},  # ç©ºæ–‡å­—åˆ—ã§å¯¾å¿œ
                        "end_date": {"type": "string"},  # ç©ºæ–‡å­—åˆ—ã§å¯¾å¿œ
                        "conclusion_date": {"type": "string"}  # ç©ºæ–‡å­—åˆ—ã§å¯¾å¿œ
                    },
                    "required": ["title", "party"]
                },
                "result": {
                    "type": "object",
                    "properties": {
                        "articles": {
                            "type": "array",
                            "items": {
                                "anyOf": [
                                    {
                                        "type": "object",
                                        "properties": {
                                            "article_number": {"type": "string"},
                                            "title": {"type": "string"},
                                            "content": {"type": "string"},
                                            "table_number": {"type": "string"}
                                        },
                                        "required": ["content", "title"]
                                    },
                                    {
                                        "type": "object",
                                        "properties": {
                                            "title": {"type": "string"},
                                            "party": {"type": "string"},
                                            "start_date": {"type": "string"},
                                            "end_date": {"type": "string"},
                                            "conclusion_date": {"type": "string"}
                                        },
                                        "required": ["title", "party"]
                                    }
                                ]
                            }
                        }
                    },
                    "required": ["articles"]
                }
            },
            "required": ["success", "info", "result"]
        }

        if not file_content:
            logger.warning(f"Empty content provided")
            return None

        # Vertex AIãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ï¼ˆæ§‹é€ åŒ–å‡ºåŠ›å¯¾å¿œï¼‰
        model = GenerativeModel('gemini-2.5-flash')
        generation_config = GenerationConfig(
            response_mime_type="application/json",
            response_schema=contract_schema,
            max_output_tokens=65535
        )

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆ
        prompt = f"""
ä»¥ä¸‹ã®OCRå‡¦ç†æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æã—ã€å¥‘ç´„æ›¸ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ãƒ•ã‚¡ã‚¤ãƒ«å: {basename}

ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹:
{file_content}

æŠ½å‡ºæŒ‡ç¤º:
1. success: å¸¸ã«true
2. infoéƒ¨åˆ†ï¼ˆ1ã¤ç›®ã®å¥‘ç´„æ›¸ã®æƒ…å ±ã®ã¿ï¼‰:
   - title: å¥‘ç´„æ›¸ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½¿ç”¨ï¼‰
   - party: å¥‘ç´„å½“äº‹è€…ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¨˜è¼‰ï¼ˆä¾‹: "æ ªå¼ä¼šç¤¾A,æ ªå¼ä¼šç¤¾B"ï¼‰
   - start_date: å¥‘ç´„é–‹å§‹æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰
   - end_date: å¥‘ç´„çµ‚äº†æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰
   - conclusion_date: å¥‘ç´„ç· çµæ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰

3. resultéƒ¨åˆ†:
   - articles: å¥‘ç´„æ¡é …ã®é…åˆ—ï¼ˆå…¨ã¦ã®æ¡é …ã‚’æ¼ã‚ŒãªãæŠ½å‡ºï¼‰
     - article_number: æ¡é …ç•ªå·ï¼ˆä¾‹: "ç¬¬1æ¡"ã€"ç¬¬2æ¡"ã€ç•ªå·ãŒãªã„å ´åˆã¯"ç½²åæ¬„"ç­‰ï¼‰
     - title: æ¡é …ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè¦‹å‡ºã—ãŒãªã„å ´åˆã¯å†…å®¹ã‹ã‚‰è¦ç´„ï¼‰
     - content: æ¡é …ã®å®Œå…¨ãªå†…å®¹ï¼ˆçœç•¥ç¦æ­¢ï¼‰
     - table_number: è¡¨ãŒã‚ã‚‹å ´åˆã®ã¿è¡¨ç•ªå·

é‡è¦ãªæ³¨æ„äº‹é …:
- ãƒ†ã‚­ã‚¹ãƒˆå†…ã®å…¨ã¦ã®æ¡é …ã‚’å¿…ãšæŠ½å‡ºã—ã¦ãã ã•ã„ï¼ˆç¬¬1æ¡ã‹ã‚‰æœ€å¾Œã¾ã§ï¼‰
- å„æ¡é …ã®contentã¯å®Œå…¨ã«ã‚³ãƒ”ãƒ¼ã—ã€çœç•¥ã‚„è¦ç´„ã¯è¡Œã‚ãªã„ã§ãã ã•ã„
- æ¡é …ç•ªå·ãŒæ˜è¨˜ã•ã‚Œã¦ã„ãªã„éƒ¨åˆ†ï¼ˆå‰æ–‡ã€ç½²åæ¬„ã€ä»˜è¨˜ç­‰ï¼‰ã‚‚ç‹¬ç«‹ã—ãŸæ¡é …ã¨ã—ã¦æ‰±ã£ã¦ãã ã•ã„
- æ—¥ä»˜ã¯å¯èƒ½ãªé™ã‚ŠYYYY-MM-DDå½¢å¼ã«å¤‰æ›ã—ã¦ãã ã•ã„
- è¡¨ã‚„å›³ãŒã‚ã‚‹å ´åˆã¯HTMLå½¢å¼ã§contentã«å«ã‚ã¦ãã ã•ã„
- ç½²åæ¬„ã‚‚å¿…ãš1ã¤ã®æ¡é …ã¨ã—ã¦æ‰±ã£ã¦ãã ã•ã„
- å‡ºåŠ›ã¯å¿…ãšå®Œå…¨ãªJSONå½¢å¼ã§ã€é€”ä¸­ã§åˆ‡ã‚Œã‚‹ã“ã¨ãªãæœ€å¾Œã¾ã§å‡ºåŠ›ã—ã¦ãã ã•ã„

ã€è¤‡æ•°å¥‘ç´„æ›¸ãŒã‚ã‚‹å ´åˆã®ç‰¹åˆ¥ãƒ«ãƒ¼ãƒ«ã€‘:
1. å¥‘ç´„æ›¸å†…éƒ¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨å¥‘ç´„æ›¸ã®åŒºåˆ‡ã‚Šã‚’æ­£ç¢ºã«åˆ¤åˆ¥:
   - ã€Œé ­æ›¸ã€ã€Œè¦é …ã€ã€Œå¥‘ç´„æ›¸æœ¬æ–‡ã€ã€Œç”¨ç´™ã€ã€Œæ¡ä»¶è¡¨ã€ã€Œæ¦‚è¦ã€ã€Œç‰¹ç´„ã€ã€Œç´°å‰‡ã€ã€Œåˆ¥ç´™ã€ã€Œä»•æ§˜æ›¸ã€ã€Œåˆ¥æ·»ã€ã€Œå›³é¢ã€ã€Œç´„æ¬¾ã€ã€Œæ´¾é£å€‹åˆ¥å¥‘ç´„ç¥¨å¥‘ç´„åŸºæœ¬æƒ…å ±ã€ã€Œå®šç¾©ä¸€è¦§è¡¨ã€ãªã©ã¯å¥‘ç´„æ›¸ã®å†…éƒ¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã‚ã‚Šã€åŒºåˆ‡ã‚Šã§ã¯ã‚ã‚Šã¾ã›ã‚“
   - ã“ã‚Œã‚‰ã¯1ã¤ã®å¥‘ç´„æ›¸ã‚’æ§‹æˆã™ã‚‹è¦ç´ ã¨ã—ã¦ã€åŒã˜articlesé…åˆ—å†…ã«å«ã‚ã¦ãã ã•ã„
   - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã®ã‚¿ã‚¤ãƒˆãƒ«ã€å¥‘ç´„å½“äº‹è€…ã€å¥‘ç´„æ¡é …ã€ç½²åãªã©ã®é‡è¦ãªæƒ…å ±ãŒåˆ†å‰²å¾Œã‚‚å®Œå…¨ã«ä¿æŒã•ã‚Œã‚‹ã‚ˆã†ã«èª¿æŸ»ã—ã¦ãã ã•ã„
   - åˆ†å‰²ã«ã‚ˆã‚Šæƒ…å ±ã®æ¬ å¦‚ã‚„æ¼ã‚ŒãŒç™ºç”Ÿã—ãªã„ã‚ˆã†ã€æ…é‡ã«åˆ†æã—ã¦ãã ã•ã„

2. å¥‘ç´„æ›¸ã®çµ‚äº†ã‚’ç¤ºã™ç®‡æ‰€ï¼ˆã€Œä»¥ä¸Šã€ç­‰ï¼‰ã¯ã€ä»¥ä¸‹ã®å½¢å¼ã§çµ±ä¸€:
   {{
     "article_number": "",
     "title": "å¥‘ç´„æ›¸çµ‚äº†",
     "content": "----------",
     "table_number": ""
   }}

3. 2ã¤ç›®ä»¥é™ã®å¥‘ç´„æ›¸ãŒå§‹ã¾ã‚‹å ´åˆã€å¥‘ç´„æ›¸çµ‚äº†ã®ç›´å¾Œã«å¥‘ç´„æ›¸åŸºæœ¬æƒ…å ±ã‚’ãã®ã¾ã¾æŒ¿å…¥:
   {{
     "title": "[å¥‘ç´„æ›¸ã‚¿ã‚¤ãƒˆãƒ«]",
     "party": "[å½“äº‹è€…ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š]",
     "start_date": "[YYYY-MM-DD ã¾ãŸã¯ç©ºæ–‡å­—åˆ—]",
     "end_date": "[YYYY-MM-DD ã¾ãŸã¯ç©ºæ–‡å­—åˆ—]",
     "conclusion_date": "[YYYY-MM-DD ã¾ãŸã¯ç©ºæ–‡å­—åˆ—]"
   }}

4. ãã®å¾Œã€2ã¤ç›®ã®å¥‘ç´„æ›¸ã®æ¡é …ã‚’ç¶šã‘ã¦è¨˜è¼‰
"""

        # Vertex AIã«é€ä¿¡ã—ã¦æ§‹é€ åŒ–å‡ºåŠ›ã‚’å–å¾—
        response = model.generate_content(
            prompt,
            generation_config=generation_config
        )

        # JSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹
        try:
            structured_data = json.loads(response.text)
            logger.info(f"Successfully structured contract data with {len(structured_data.get('result', {}).get('articles', []))} articles")
            return structured_data
        except json.JSONDecodeError as json_error:
            logger.error(f"Error in Vertex AI structured output: {str(json_error)}")

            # ã‚¨ãƒ©ãƒ¼æ™‚ã«ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’GCSã«ä¿å­˜
            from datetime import datetime
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            error_path = f"{workspace_id}/{project_id}/err/{basename}_error_{timestamp}.txt"

            try:
                upload_json_to_gcs(
                    {"error": str(json_error), "response": response.text},
                    bucket_name,
                    error_path.replace('.txt', '.json')
                )
                logger.info(f"ğŸ“ Error response saved to: gs://{bucket_name}/{error_path.replace('.txt', '.json')}")
            except Exception as upload_error:
                logger.error(f"Failed to save error response: {upload_error}")

            return None

    except Exception as e:
        logger.error(f"Error in Vertex AI structured output: {str(e)}")
        return None


def convert_to_contract_schema(gcs_file_path: str, basename: str) -> Optional[Dict[str, Any]]:
    """
    GCSã«ä¿å­˜ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’Vertex AIã®æ§‹é€ åŒ–å‡ºåŠ›ã‚’ä½¿ã£ã¦å¥‘ç´„æ›¸ã‚¹ã‚­ãƒ¼ãƒã«å¤‰æ›

    Args:
        gcs_file_path: GCSã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        basename: ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹å

    Returns:
        æ§‹é€ åŒ–ã•ã‚ŒãŸå¥‘ç´„æ›¸ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯None
    """
    try:
        # Vertex AIè¨­å®š
        import vertexai
        from vertexai.generative_models import GenerativeModel, GenerationConfig

        project_id = os.getenv('GCP_PROJECT_ID')
        location = os.getenv('GCP_LOCATION', 'us-central1')
        if not project_id:
            logger.error("GCP_PROJECT_IDç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None

        vertexai.init(project=project_id, location=location)
        
        # å¥‘ç´„æ›¸ã‚¹ã‚­ãƒ¼ãƒã®å®šç¾©
        contract_schema = {
            "type": "object",
            "properties": {
                "success": {"type": "boolean"},
                "info": {
                    "type": "object",
                    "properties": {
                        "title": {"type": "string"},
                        "party": {"type": "string"},  # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®å½“äº‹è€…å
                        "start_date": {"type": "string"},  # ç©ºæ–‡å­—åˆ—ã§å¯¾å¿œ
                        "end_date": {"type": "string"},  # ç©ºæ–‡å­—åˆ—ã§å¯¾å¿œ
                        "conclusion_date": {"type": "string"}  # ç©ºæ–‡å­—åˆ—ã§å¯¾å¿œ
                    },
                    "required": ["title", "party"]
                },
                "result": {
                    "type": "object",
                    "properties": {
                        "articles": {
                            "type": "array",
                            "items": {
                                "anyOf": [
                                    {
                                        "type": "object",
                                        "properties": {
                                            "article_number": {"type": "string"},
                                            "title": {"type": "string"},
                                            "content": {"type": "string"},
                                            "table_number": {"type": "string"}
                                        },
                                        "required": ["content", "title"]
                                    },
                                    {
                                        "type": "object",
                                        "properties": {
                                            "title": {"type": "string"},
                                            "party": {"type": "string"},
                                            "start_date": {"type": "string"},
                                            "end_date": {"type": "string"},
                                            "conclusion_date": {"type": "string"}
                                        },
                                        "required": ["title", "party"]
                                    }
                                ]
                            }
                        }
                    },
                    "required": ["articles"]
                }
            },
            "required": ["success", "info", "result"]
        }
        
        # GCSã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿å–ã‚Š
        file_content = download_text_from_gcs(gcs_file_path)
        if not file_content:
            logger.warning(f"Could not read content from: {gcs_file_path}")
            return None
        
        # Vertex AIãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ï¼ˆæ§‹é€ åŒ–å‡ºåŠ›å¯¾å¿œï¼‰
        model = GenerativeModel('gemini-2.5-flash')
        generation_config = GenerationConfig(
            response_mime_type="application/json",
            response_schema=contract_schema,
            max_output_tokens=65535
        )

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆ
        prompt = f"""
ä»¥ä¸‹ã®OCRå‡¦ç†æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æã—ã€å¥‘ç´„æ›¸ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ãƒ•ã‚¡ã‚¤ãƒ«å: {basename}

ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹:
{file_content}

æŠ½å‡ºæŒ‡ç¤º:
1. success: å¸¸ã«true
2. infoéƒ¨åˆ†ï¼ˆ1ã¤ç›®ã®å¥‘ç´„æ›¸ã®æƒ…å ±ã®ã¿ï¼‰:
   - title: å¥‘ç´„æ›¸ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½¿ç”¨ï¼‰
   - party: å¥‘ç´„å½“äº‹è€…ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¨˜è¼‰ï¼ˆä¾‹: "æ ªå¼ä¼šç¤¾A,æ ªå¼ä¼šç¤¾B"ï¼‰
   - start_date: å¥‘ç´„é–‹å§‹æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰
   - end_date: å¥‘ç´„çµ‚äº†æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰
   - conclusion_date: å¥‘ç´„ç· çµæ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰

3. resultéƒ¨åˆ†:
   - articles: å¥‘ç´„æ¡é …ã®é…åˆ—ï¼ˆå…¨ã¦ã®æ¡é …ã‚’æ¼ã‚ŒãªãæŠ½å‡ºï¼‰
     - article_number: æ¡é …ç•ªå·ï¼ˆä¾‹: "ç¬¬1æ¡"ã€"ç¬¬2æ¡"ã€ç•ªå·ãŒãªã„å ´åˆã¯"ç½²åæ¬„"ç­‰ï¼‰
     - title: æ¡é …ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè¦‹å‡ºã—ãŒãªã„å ´åˆã¯å†…å®¹ã‹ã‚‰è¦ç´„ï¼‰
     - content: æ¡é …ã®å®Œå…¨ãªå†…å®¹ï¼ˆçœç•¥ç¦æ­¢ï¼‰
     - table_number: è¡¨ãŒã‚ã‚‹å ´åˆã®ã¿è¡¨ç•ªå·

é‡è¦ãªæ³¨æ„äº‹é …:
- ãƒ†ã‚­ã‚¹ãƒˆå†…ã®å…¨ã¦ã®æ¡é …ã‚’å¿…ãšæŠ½å‡ºã—ã¦ãã ã•ã„ï¼ˆç¬¬1æ¡ã‹ã‚‰æœ€å¾Œã¾ã§ï¼‰
- å„æ¡é …ã®contentã¯å®Œå…¨ã«ã‚³ãƒ”ãƒ¼ã—ã€çœç•¥ã‚„è¦ç´„ã¯è¡Œã‚ãªã„ã§ãã ã•ã„
- æ¡é …ç•ªå·ãŒæ˜è¨˜ã•ã‚Œã¦ã„ãªã„éƒ¨åˆ†ï¼ˆå‰æ–‡ã€ç½²åæ¬„ã€ä»˜è¨˜ç­‰ï¼‰ã‚‚ç‹¬ç«‹ã—ãŸæ¡é …ã¨ã—ã¦æ‰±ã£ã¦ãã ã•ã„
- æ—¥ä»˜ã¯å¯èƒ½ãªé™ã‚ŠYYYY-MM-DDå½¢å¼ã«å¤‰æ›ã—ã¦ãã ã•ã„
- è¡¨ã‚„å›³ãŒã‚ã‚‹å ´åˆã¯HTMLå½¢å¼ã§contentã«å«ã‚ã¦ãã ã•ã„
- ç½²åæ¬„ã‚‚å¿…ãš1ã¤ã®æ¡é …ã¨ã—ã¦æ‰±ã£ã¦ãã ã•ã„
- å‡ºåŠ›ã¯å¿…ãšå®Œå…¨ãªJSONå½¢å¼ã§ã€é€”ä¸­ã§åˆ‡ã‚Œã‚‹ã“ã¨ãªãæœ€å¾Œã¾ã§å‡ºåŠ›ã—ã¦ãã ã•ã„

ã€è¤‡æ•°å¥‘ç´„æ›¸ãŒã‚ã‚‹å ´åˆã®ç‰¹åˆ¥ãƒ«ãƒ¼ãƒ«ã€‘:
1. å¥‘ç´„æ›¸å†…éƒ¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨å¥‘ç´„æ›¸ã®åŒºåˆ‡ã‚Šã‚’æ­£ç¢ºã«åˆ¤åˆ¥:
   - ã€Œé ­æ›¸ã€ã€Œè¦é …ã€ã€Œå¥‘ç´„æ›¸æœ¬æ–‡ã€ã€Œç”¨ç´™ã€ã€Œæ¡ä»¶è¡¨ã€ã€Œæ¦‚è¦ã€ã€Œç‰¹ç´„ã€ã€Œç´°å‰‡ã€ã€Œåˆ¥ç´™ã€ã€Œä»•æ§˜æ›¸ã€ã€Œåˆ¥æ·»ã€ã€Œå›³é¢ã€ã€Œç´„æ¬¾ã€ã€Œæ´¾é£å€‹åˆ¥å¥‘ç´„ç¥¨å¥‘ç´„åŸºæœ¬æƒ…å ±ã€ã€Œå®šç¾©ä¸€è¦§è¡¨ã€ãªã©ã¯å¥‘ç´„æ›¸ã®å†…éƒ¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã‚ã‚Šã€åŒºåˆ‡ã‚Šã§ã¯ã‚ã‚Šã¾ã›ã‚“
   - ã“ã‚Œã‚‰ã¯1ã¤ã®å¥‘ç´„æ›¸ã‚’æ§‹æˆã™ã‚‹è¦ç´ ã¨ã—ã¦ã€åŒã˜articlesé…åˆ—å†…ã«å«ã‚ã¦ãã ã•ã„
   - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã®ã‚¿ã‚¤ãƒˆãƒ«ã€å¥‘ç´„å½“äº‹è€…ã€å¥‘ç´„æ¡é …ã€ç½²åãªã©ã®é‡è¦ãªæƒ…å ±ãŒåˆ†å‰²å¾Œã‚‚å®Œå…¨ã«ä¿æŒã•ã‚Œã‚‹ã‚ˆã†ã«èª¿æŸ»ã—ã¦ãã ã•ã„
   - åˆ†å‰²ã«ã‚ˆã‚Šæƒ…å ±ã®æ¬ å¦‚ã‚„æ¼ã‚ŒãŒç™ºç”Ÿã—ãªã„ã‚ˆã†ã€æ…é‡ã«åˆ†æã—ã¦ãã ã•ã„

2. å¥‘ç´„æ›¸ã®çµ‚äº†ã‚’ç¤ºã™ç®‡æ‰€ï¼ˆã€Œä»¥ä¸Šã€ç­‰ï¼‰ã¯ã€ä»¥ä¸‹ã®å½¢å¼ã§çµ±ä¸€:
   {{
     "article_number": "",
     "title": "å¥‘ç´„æ›¸çµ‚äº†",
     "content": "----------",
     "table_number": ""
   }}

3. 2ã¤ç›®ä»¥é™ã®å¥‘ç´„æ›¸ãŒå§‹ã¾ã‚‹å ´åˆã€å¥‘ç´„æ›¸çµ‚äº†ã®ç›´å¾Œã«å¥‘ç´„æ›¸åŸºæœ¬æƒ…å ±ã‚’ãã®ã¾ã¾æŒ¿å…¥:
   {{
     "title": "[å¥‘ç´„æ›¸ã‚¿ã‚¤ãƒˆãƒ«]",
     "party": "[å½“äº‹è€…ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š]",
     "start_date": "[YYYY-MM-DD ã¾ãŸã¯ç©ºæ–‡å­—åˆ—]",
     "end_date": "[YYYY-MM-DD ã¾ãŸã¯ç©ºæ–‡å­—åˆ—]",
     "conclusion_date": "[YYYY-MM-DD ã¾ãŸã¯ç©ºæ–‡å­—åˆ—]"
   }}

4. ãã®å¾Œã€2ã¤ç›®ã®å¥‘ç´„æ›¸ã®æ¡é …ã‚’ç¶šã‘ã¦è¨˜è¼‰
"""

        # Vertex AIã«é€ä¿¡ã—ã¦æ§‹é€ åŒ–å‡ºåŠ›ã‚’å–å¾—
        response = model.generate_content(
            prompt,
            generation_config=generation_config
        )

        # JSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹
        structured_data = json.loads(response.text)

        logger.info(f"Successfully structured contract data with {len(structured_data.get('result', {}).get('articles', []))} articles")

        return structured_data

    except Exception as e:
        logger.error(f"Error in Vertex AI structured output: {str(e)}")
        return None


def download_text_from_gcs(gcs_path: str) -> Optional[str]:
    """
    GCSã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª­ã¿å–ã‚Š

    Args:
        gcs_path: GCSã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (gs://bucket/path/to/file.txt)

    Returns:
        ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã¾ãŸã¯None
    """
    try:
        
        # GCS URIã‚’ãƒ‘ãƒ¼ã‚¹
        if not gcs_path.startswith('gs://'):
            return None
            
        path_parts = gcs_path.replace('gs://', '').split('/', 1)
        bucket_name = path_parts[0]
        blob_name = path_parts[1]
        
        # GCSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿å–ã‚Š
        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(blob_name)
        
        # ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦èª­ã¿å–ã‚Š
        content = blob.download_as_text(encoding='utf-8')
        
        return content
        
    except Exception as e:
        logger.error(f"Error downloading text from GCS: {str(e)}")
        return None

def download_from_gcs(gcs_uri: str, local_path: str) -> str:
    """
    GCSã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    """
    import urllib.parse
    
    if not gcs_uri.startswith('gs://'):
        raise ValueError(f"Invalid GCS URI: {gcs_uri}")
    
    path_parts = gcs_uri[5:].split('/', 1)
    if len(path_parts) != 2:
        raise ValueError(f"Invalid GCS URI format: {gcs_uri}")
    
    bucket_name, blob_path = path_parts
    
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    
    # Try to find the blob directly first
    try:
        blob = bucket.blob(blob_path)
        logger.info(f"Downloading {gcs_uri} to {local_path}")
        blob.download_to_filename(local_path)
        return local_path
    except Exception as e:
        logger.warning(f"Direct download failed: {e}")
        
        # If direct access fails, try to list and find matching blobs
        logger.info(f"Searching for blobs matching: {blob_path}")
        
        # Get the directory path and filename
        path_parts = blob_path.split('/')
        if len(path_parts) > 1:
            prefix = '/'.join(path_parts[:-1]) + '/'
            target_filename = path_parts[-1]
        else:
            prefix = ''
            target_filename = blob_path
            
        logger.info(f"Listing blobs with prefix: {prefix}")
        blobs = list(bucket.list_blobs(prefix=prefix))
        logger.info(f"Found {len(blobs)} blobs with prefix")
        
        # Look for exact or partial matches
        for blob in blobs:
            blob_filename = blob.name.split('/')[-1]
            logger.info(f"Checking blob: {blob.name} (filename: {blob_filename})")
            
            # Normalize Unicode for comparison
            import unicodedata
            normalized_blob_name = unicodedata.normalize('NFC', blob.name)
            normalized_blob_filename = unicodedata.normalize('NFC', blob_filename)
            normalized_target_filename = unicodedata.normalize('NFC', target_filename)
            normalized_blob_path = unicodedata.normalize('NFC', blob_path)
            
            # Try multiple matching strategies with normalized strings
            match_found = (
                normalized_blob_filename == normalized_target_filename or  # Exact match
                normalized_target_filename in normalized_blob_filename or  # Target is substring of blob
                normalized_blob_filename in normalized_target_filename or  # Blob is substring of target
                normalized_blob_name == normalized_blob_path or           # Full path exact match
                normalized_blob_name.endswith(normalized_target_filename) or  # Ends with target filename
                # Also try without normalization for backward compatibility
                blob_filename == target_filename or
                target_filename in blob_filename or
                blob_filename in target_filename or
                blob.name == blob_path or
                blob.name.endswith(target_filename)
            )
            
            if match_found:
                logger.info(f"Found matching blob: {blob.name}")
                try:
                    blob.download_to_filename(local_path)
                    return local_path
                except Exception as download_error:
                    logger.warning(f"Failed to download {blob.name}: {download_error}")
                    continue
                    
        raise FileNotFoundError(f"Could not find or download blob matching: {blob_path}")
    
    return local_path

def upload_file_to_gcs(local_path: str, bucket_name: str, blob_name: str) -> str:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    """
    
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(blob_name)
    
    logger.info(f"Uploading {local_path} to gs://{bucket_name}/{blob_name}")
    blob.upload_from_filename(local_path)
    
    return f"gs://{bucket_name}/{blob_name}"

def upload_json_to_gcs(json_data: Dict[str, Any], bucket_name: str, blob_path: str) -> str:
    """
    JSONãƒ‡ãƒ¼ã‚¿ã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    """
    
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(blob_path)
    
    json_str = json.dumps(json_data, ensure_ascii=False, indent=2)
    
    logger.info(f"Uploading JSON to gs://{bucket_name}/{blob_path}")
    blob.upload_from_string(json_str, content_type='application/json')
    
    return f"gs://{bucket_name}/{blob_path}"

def upload_results_to_gcs(result: Dict[str, Any], bucket_name: str, prefix: str) -> str:
    """
    å‡¦ç†çµæœã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    """
    
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    
    result_json = json.dumps(result, ensure_ascii=False, indent=2)
    blob_name = f"{prefix}result.json"
    blob = bucket.blob(blob_name)
    
    logger.info(f"Uploading results to gs://{bucket_name}/{blob_name}")
    blob.upload_from_string(result_json, content_type='application/json')
    
    return f"gs://{bucket_name}/{blob_name}"


if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8080))
    app.run(host='0.0.0.0', port=port, debug=False)