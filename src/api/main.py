import os
import json
import base64
import logging
import shutil
from flask import Flask, request, jsonify
from datetime import datetime
import traceback
from typing import Dict, Any, Optional, List
from google.cloud import storage
import subprocess
import sys
from pathlib import Path
import psycopg2
from psycopg2.extras import RealDictCursor

# UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç¢ºå®Ÿã«ã™ã‚‹
os.environ.setdefault('PYTHONIOENCODING', 'utf-8')
os.environ.setdefault('LANG', 'ja_JP.UTF-8')
os.environ.setdefault('LC_ALL', 'ja_JP.UTF-8')
from src.api.model_downloader import ensure_models_available

app = Flask(__name__)

# UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç¢ºå®Ÿã«ã™ã‚‹
app.config['JSON_AS_ASCII'] = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Cloud Runèµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
def initialize_models():
    """èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆ512MBæ§‹æˆç”¨ã«è»½é‡åŒ–ï¼‰"""
    logger.info("ğŸš€ Initializing models (lightweight mode)...")
    
    # ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ãŒã‚ã‚‹å ´åˆã¯ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—
    memory_limit = os.environ.get('CLOUD_RUN_MEMORY', '512Mi')
    if '512' in memory_limit:
        logger.info("ğŸ’¡ 512MBæ§‹æˆæ¤œå‡º - ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        logger.info("ğŸ”§ ãƒ¢ãƒ‡ãƒ«ã¯å‡¦ç†æ™‚ã«å‹•çš„ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™")
        return
    
    try:
        success = ensure_models_available()
        if success:
            logger.info("âœ… Models initialized successfully")
        else:
            logger.warning("âš ï¸ Some models may be missing")
    except Exception as e:
        logger.error(f"âŒ Failed to initialize models: {e}")
        logger.warning("âš ï¸ Continuing without models - OCR features may be limited")

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
initialize_models()

# ================================
# Database Connection
# ================================

def get_db_connection(bucket_name: Optional[str] = None):
    """
    PostgreSQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¸ã®æ¥ç¶šã‚’å–å¾—

    Args:
        bucket_name: GCSãƒã‚±ãƒƒãƒˆåï¼ˆapp_contracts=æœ¬ç•ª, app_contracts_staging=STGï¼‰

    Returns:
        psycopg2æ¥ç¶šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    """
    # ãƒã‚±ãƒƒãƒˆåã§DBæ¥ç¶šå…ˆã‚’åˆ¤å®š
    if bucket_name == 'app_contracts_staging':
        # STGç’°å¢ƒã®DB
        database_url = os.getenv('DATABASE_URL_STAGING') or 'postgresql://postgres:Avop9ghE5uTR3mm@10.1.1.3:5432/dd_ops'
        logger.info(f"ğŸ”§ Using STAGING database for bucket: {bucket_name}")
    elif bucket_name == 'app_contracts':
        # æœ¬ç•ªç’°å¢ƒã®DB
        database_url = os.getenv('DATABASE_URL') or 'postgresql://postgres:qjFJ8foxA2Qy722mqeweQ@10.1.0.3:5432/dd_ops'
        logger.info(f"ğŸ”§ Using PRODUCTION database for bucket: {bucket_name}")
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
        database_url = os.getenv('DATABASE_URL')
        if not database_url:
            raise ValueError("DATABASE_URL environment variable is not set")
        logger.info(f"ğŸ”§ Using default database (bucket: {bucket_name})")

    try:
        conn = psycopg2.connect(database_url)
        return conn
    except Exception as e:
        logger.error(f"âŒ Failed to connect to database: {e}")
        raise

def get_risks_from_db(workspace_id: Optional[int] = None, selected_risk_ids: Optional[List[int]] = None, bucket_name: Optional[str] = None) -> List[Dict[str, Any]]:
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒªã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã‚’å–å¾—ã™ã‚‹

    Args:
        workspace_id: ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹IDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        selected_risk_ids: é¸æŠã•ã‚ŒãŸãƒªã‚¹ã‚¯IDã®ãƒªã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        bucket_name: GCSãƒã‚±ãƒƒãƒˆåï¼ˆSTG/æœ¬ç•ªåˆ¤å®šç”¨ï¼‰

    Returns:
        List[Dict]: ãƒªã‚¹ã‚¯æƒ…å ±ã®ãƒªã‚¹ãƒˆ

    Logic:
        - selected_risk_idsãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆ: ãã®IDã®ãƒªã‚¹ã‚¯ã®ã¿ã‚’å–å¾—ï¼ˆã‚«ã‚¹ã‚¿ãƒ å®Ÿè¡Œï¼‰
        - selected_risk_idsãŒãªã„å ´åˆ: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒªã‚¹ã‚¯ï¼ˆworkspace_id=nullï¼‰ã®ã¿ã‚’å–å¾—
    """
    conn = None
    try:
        conn = get_db_connection(bucket_name)
        cursor = conn.cursor(cursor_factory=RealDictCursor)

        if selected_risk_ids and len(selected_risk_ids) > 0:
            # ã‚«ã‚¹ã‚¿ãƒ å®Ÿè¡Œ: æŒ‡å®šã•ã‚ŒãŸIDã®ãƒªã‚¹ã‚¯ã®ã¿å–å¾—
            placeholders = ','.join(['%s'] * len(selected_risk_ids))
            query = f"""
                SELECT id, title, prompt, description, "workspaceId"
                FROM "Risk"
                WHERE id IN ({placeholders})
                ORDER BY id ASC
            """
            cursor.execute(query, selected_risk_ids)
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå®Ÿè¡Œ: workspace_id=nullã®ãƒªã‚¹ã‚¯ã®ã¿å–å¾—
            query = """
                SELECT id, title, prompt, description, "workspaceId"
                FROM "Risk"
                WHERE "workspaceId" IS NULL
                ORDER BY id ASC
            """
            cursor.execute(query)

        risks = cursor.fetchall()
        cursor.close()

        # RealDictRowã‚’dictã«å¤‰æ›
        return [dict(risk) for risk in risks]

    except Exception as e:
        logger.error(f"âŒ Failed to fetch risks from database: {e}")
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
        return []
    finally:
        if conn:
            conn.close()

def calculate_total_page_count(pipeline_result: Dict[str, Any]) -> int:
    """
    ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµæœã‹ã‚‰åˆ†å‰²å¾Œã®ç·ãƒšãƒ¼ã‚¸æ•°ã‚’è¨ˆç®—

    Args:
        pipeline_result: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œçµæœ

    Returns:
        int: åˆ†å‰²å¾Œã®ç·ãƒšãƒ¼ã‚¸æ•°ï¼ˆå„ãƒšãƒ¼ã‚¸ã®page_countã®åˆè¨ˆï¼‰
    """
    try:
        # Step4ã®çµæœã‹ã‚‰page_count_distributionã‚’å–å¾—
        step4_result = pipeline_result.get("steps", {}).get("step4_processing", {})
        summary = step4_result.get("summary", {})
        page_count_distribution = summary.get("page_count_distribution", {})

        if not page_count_distribution:
            logger.warning("âš ï¸ page_count_distribution not found in pipeline result")
            return 0

        # page_count_distribution: {1: 5, 2: 3, 3: 1} ã®ã‚ˆã†ãªå½¢å¼
        # ã“ã‚Œã¯ã€Œpage_count=1ã®ãƒšãƒ¼ã‚¸ãŒ5ã¤ã€page_count=2ã®ãƒšãƒ¼ã‚¸ãŒ3ã¤ã€page_count=3ã®ãƒšãƒ¼ã‚¸ãŒ1ã¤ã€ã¨ã„ã†æ„å‘³
        # åˆ†å‰²å¾Œã®ç·ãƒšãƒ¼ã‚¸æ•° = 1*5 + 2*3 + 3*1 = 5 + 6 + 3 = 14
        total_page_count = 0
        for page_count, count in page_count_distribution.items():
            # page_countã¯æ–‡å­—åˆ—ã®å ´åˆãŒã‚ã‚‹ã®ã§æ•´æ•°ã«å¤‰æ›
            try:
                pc = int(page_count)
                cnt = int(count)
                total_page_count += pc * cnt
                logger.debug(f"  page_count={pc}: {cnt}ãƒšãƒ¼ã‚¸ â†’ {pc * cnt}ãƒšãƒ¼ã‚¸åˆ†")
            except (ValueError, TypeError):
                logger.warning(f"âš ï¸ Invalid page_count_distribution entry: {page_count}={count}")
                continue

        return total_page_count

    except Exception as e:
        logger.error(f"âŒ Error calculating total page count: {e}")
        logger.error(traceback.format_exc())
        return 0

def save_page_count_to_db(project_id: str, total_page_count: int, bucket_name: str) -> bool:
    """
    åˆ†å‰²å¾Œã®ãƒšãƒ¼ã‚¸æ•°ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã™ã‚‹

    Args:
        project_id: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDï¼ˆæ–‡å­—åˆ—ï¼‰
        total_page_count: åˆ†å‰²å¾Œã®ç·ãƒšãƒ¼ã‚¸æ•°
        bucket_name: GCSãƒã‚±ãƒƒãƒˆåï¼ˆSTG/æœ¬ç•ªåˆ¤å®šç”¨ï¼‰

    Returns:
        bool: ä¿å­˜ãŒæˆåŠŸã—ãŸã‹ã©ã†ã‹
    """
    conn = None
    try:
        # project_idã‚’æ•´æ•°ã«å¤‰æ›
        try:
            project_id_int = int(project_id)
        except (ValueError, TypeError):
            logger.error(f"âŒ Invalid project_id format: {project_id}")
            return False

        conn = get_db_connection(bucket_name)
        cursor = conn.cursor()

        # OcrPageCountãƒ†ãƒ¼ãƒ–ãƒ«ã«æŒ¿å…¥
        query = """
            INSERT INTO "OcrPageCount" ("projectId", "pageCount", "createdAt")
            VALUES (%s, %s, NOW())
        """
        cursor.execute(query, (project_id_int, total_page_count))
        conn.commit()
        cursor.close()

        logger.info(f"âœ… Page count saved to database: projectId={project_id_int}, pageCount={total_page_count}")
        return True

    except Exception as e:
        logger.error(f"âŒ Failed to save page count to database: {e}")
        logger.error(traceback.format_exc())
        if conn:
            conn.rollback()
        return False
    finally:
        if conn:
            conn.close()

def get_project_root():
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—"""
    # Cloud Runã§ã¯/appé…ä¸‹ã«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒé…ç½®ã•ã‚Œã‚‹
    if Path("/app").exists():
        return Path("/app")
    else:
        # ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒ
        return Path(__file__).parent.parent.parent

def run_main_pipeline(pdf_path: str) -> Dict[str, Any]:
    """
    main_pipeline.pyã‚’å®Ÿè¡Œã—ã¦OCRå‡¦ç†ã‚’è¡Œã†

    Args:
        pdf_path: å‡¦ç†å¯¾è±¡ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

    Returns:
        Dict: å‡¦ç†çµæœï¼ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµæœã‚’å«ã‚€ï¼‰
    """
    try:
        # main_pipeline.pyã‚’Pythonãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦ç›´æ¥å®Ÿè¡Œ
        from src.main_pipeline import DocumentOCRPipeline
        import asyncio

        project_root = get_project_root()
        config_path = project_root / "config.yml"

        logger.info(f"ğŸš€ Running main pipeline for: {pdf_path}")

        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
        pipeline = DocumentOCRPipeline(str(config_path))

        # éåŒæœŸå‡¦ç†ã‚’å®Ÿè¡Œ
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            pipeline_result = loop.run_until_complete(pipeline.process_pdf(pdf_path))
        finally:
            loop.close()

        if pipeline_result.get("success"):
            logger.info("âœ… main_pipeline executed successfully")
            return pipeline_result
        else:
            logger.error(f"âŒ main_pipeline failed: {pipeline_result.get('error')}")
            return pipeline_result

    except Exception as e:
        logger.error(f"âŒ Error running main_pipeline: {e}")
        logger.error(traceback.format_exc())
        return {
            "success": False,
            "error": str(e)
        }

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({'status': 'healthy', 'timestamp': datetime.utcnow().isoformat()}), 200

@app.route('/debug-blobs', methods=['GET'])
def debug_blobs():
    """GCSå†…ã®blobã‚’ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒªã‚¹ãƒˆã™ã‚‹"""
    try:
        
        prefix = request.args.get('prefix', '')
        bucket_name = 'app_contracts_staging'
        
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        
        blobs = list(bucket.list_blobs(prefix=prefix))
        
        blob_info = []
        for blob in blobs:
            blob_info.append({
                'name': blob.name,
                'size': blob.size,
                'created': blob.time_created.isoformat() if blob.time_created else None,
                'content_type': blob.content_type
            })
        
        return jsonify({
            'bucket': bucket_name,
            'prefix': prefix,
            'total_blobs': len(blob_info),
            'blobs': blob_info
        }), 200
        
    except Exception as e:
        logger.error(f"Debug blobs error: {str(e)}")
        return jsonify({
            'error': str(e)
        }), 500

@app.route('/', methods=['GET'])
def root():
    """
    ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ - APIæƒ…å ±ã‚’è¿”ã™
    """
    return jsonify({
        'service': 'DD-OPS OCR API',
        'version': '1.0.0',
        'endpoints': {
            '/health': 'Health check',
            '/ocr [GET]': 'Test OCR with local PDF files',
            '/ocr [POST]': 'OCR with GCS PDF URL',
            '/pubsub/push [POST]': 'PubSub webhook for automatic processing'
        },
        'usage': {
            'GET /ocr': 'Process PDF from /pdf/ directory',
            'GET /ocr?file=test.pdf': 'Process specific PDF file',
            'POST /ocr': 'Send {"pdf_url": "gs://bucket/file.pdf", "workspace_id": "ws", "project_id": "proj"}'
        },
        'timestamp': datetime.utcnow().isoformat()
    }), 200

@app.route('/ocr', methods=['GET'])
def ocr_test():
    """
    GETãƒ¡ã‚½ãƒƒãƒ‰ã§OCRå‡¦ç†ã‚’ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    
    ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
        file: PDFãƒ•ã‚¡ã‚¤ãƒ«å (ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯è‡ªå‹•æ¤œå‡º)
        
    ä¾‹: /ocr?file=test.pdf
    """
    try:
        logger.info("ğŸš€ GET /ocr endpoint called")
        
        # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—
        pdf_filename = request.args.get('file')
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®OCRå‡¦ç†ã‚’å®Ÿè¡Œ
        result = process_test_pdf(pdf_filename)
        
        response = {
            "status": "completed",
            "method": "GET",
            "timestamp": datetime.now().isoformat(),
            "result": result
        }
        
        return jsonify(response), 200
        
    except Exception as e:
        logger.error(f"OCR test endpoint error: {str(e)}")
        return jsonify({
            "status": "error",
            "method": "GET",
            "timestamp": datetime.now().isoformat(),
            "error": str(e)
        }), 500

@app.route('/ocr', methods=['POST'])
def ocr_upload():
    """
    POSTãƒ¡ã‚½ãƒƒãƒ‰ã§OCRå‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    
    JSON body:
        {
            "pdf_url": "gs://bucket/path/to/file.pdf",
            "workspace_id": "workspace123",
            "project_id": "project456"
        }
    """
    try:
        logger.info("ğŸš€ POST /ocr endpoint called")
        
        data = request.get_json()
        if not data:
            return jsonify({
                "status": "error",
                "error": "JSON body required"
            }), 400
        
        pdf_url = data.get('pdf_url')
        workspace_id = data.get('workspace_id', 'test_workspace')
        project_id = data.get('project_id', 'test_project')
        
        if not pdf_url:
            return jsonify({
                "status": "error",
                "error": "pdf_url is required"
            }), 400
        
        # GCS URIã®å ´åˆ
        if pdf_url.startswith('gs://'):
            bucket_name = pdf_url.replace('gs://', '').split('/')[0]
            object_name = '/'.join(pdf_url.replace('gs://', '').split('/')[1:])
            
            result = process_single_pdf(bucket_name, object_name, workspace_id, project_id)
        else:
            return jsonify({
                "status": "error",
                "error": "Only gs:// URLs are supported"
            }), 400
        
        response = {
            "status": "completed",
            "method": "POST",
            "timestamp": datetime.now().isoformat(),
            "workspace_id": workspace_id,
            "project_id": project_id,
            "result": result
        }
        
        return jsonify(response), 200
        
    except Exception as e:
        logger.error(f"OCR upload endpoint error: {str(e)}")
        return jsonify({
            "status": "error",
            "method": "POST",
            "timestamp": datetime.now().isoformat(),
            "error": str(e)
        }), 500

@app.route('/pubsub/push', methods=['POST'])
def pubsub_push():
    """
    Cloud PubSub Pushé€šçŸ¥ã‚’å—ã‘å–ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    GCS Storage Object notificationã‚’å‡¦ç†
    """
    try:
        logger.info("="*80)
        logger.info("ğŸ”µ PUBSUB PUSH REQUEST RECEIVED")
        logger.info("="*80)

        logger.info(f"ğŸ“Œ Request Method: {request.method}")
        logger.info(f"ğŸ“Œ Request URL: {request.url}")
        logger.info(f"ğŸ“Œ Request Path: {request.path}")

        logger.info("ğŸ“‹ REQUEST HEADERS:")
        for key, value in request.headers:
            logger.info(f"  - {key}: {value}")

        logger.info(f"ğŸ“ Content-Type: {request.content_type}")
        logger.info(f"ğŸ“ Content-Length: {request.content_length}")

        logger.info(f"ğŸ“¦ Raw Request Data: {request.data}")
        logger.info(f"ğŸ“¦ Request Data Type: {type(request.data)}")
        logger.info(f"ğŸ“¦ Request Data Length: {len(request.data) if request.data else 0}")

        logger.info("ğŸ” Attempting to parse JSON...")
        envelope = None
        try:
            envelope = request.get_json()
            logger.info(f"âœ… JSON parsed successfully")
            logger.info(f"ğŸ“Š Envelope type: {type(envelope)}")
            logger.info(f"ğŸ“Š Envelope keys: {list(envelope.keys()) if envelope else 'None'}")
            logger.info(f"ğŸ“Š Full envelope content: {json.dumps(envelope, indent=2) if envelope else 'None'}")
        except Exception as json_error:
            logger.error(f"âŒ JSON parsing failed: {str(json_error)}")
            logger.error(f"âŒ Error type: {type(json_error).__name__}")

        if not envelope:
            logger.error("âŒ No PubSub message received (envelope is None or empty)")
            return jsonify({"error": "Bad Request: no PubSub message received"}), 400

        # Check delivery attempt and skip if too many retries
        delivery_attempt = envelope.get('deliveryAttempt', 0)
        logger.info(f"ğŸ“¬ Delivery attempt: {delivery_attempt}")

        if delivery_attempt > 2:
            logger.warning(f"âš ï¸ Skipping message after {delivery_attempt} delivery attempts")
            return jsonify({"status": "skipped", "reason": f"Too many retries ({delivery_attempt})"}), 200
            
        if not isinstance(envelope, dict) or "message" not in envelope:
            logger.error(f"âŒ Invalid PubSub message format - envelope type: {type(envelope)}, has 'message' key: {'message' in envelope if isinstance(envelope, dict) else 'N/A'}")
            return jsonify({"error": "Bad Request: invalid PubSub message format"}), 400
            
        pubsub_message = envelope["message"]
        logger.info("ğŸ“¨ PUBSUB MESSAGE:")
        logger.info(f"  - Message type: {type(pubsub_message)}")
        logger.info(f"  - Message keys: {list(pubsub_message.keys()) if isinstance(pubsub_message, dict) else 'Not a dict'}")
        logger.info(f"  - Full message: {json.dumps(pubsub_message, indent=2)}")

        # attributesã‹ã‚‰bucketId, workspaceId, selectedRiskIdsã‚’å–å¾—
        attributes = pubsub_message.get("attributes", {})
        bucket_id = attributes.get("bucketId", "")
        workspace_id_from_attr = attributes.get("workspaceId")
        selected_risk_ids_str = attributes.get("selectedRiskIds")

        logger.info(f"ğŸ“¦ Bucket ID from attributes: {bucket_id}")
        logger.info(f"ğŸ“¦ Workspace ID from attributes: {workspace_id_from_attr}")
        logger.info(f"ğŸ“¦ Selected Risk IDs from attributes: {selected_risk_ids_str}")

        # selectedRiskIdsã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®æ–‡å­—åˆ—ã‚’æ•´æ•°é…åˆ—ã«å¤‰æ›ï¼‰
        selected_risk_ids = None
        if selected_risk_ids_str:
            try:
                selected_risk_ids = [int(id.strip()) for id in selected_risk_ids_str.split(",") if id.strip()]
                logger.info(f"ğŸ“Š Parsed selected risk IDs: {selected_risk_ids}")
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to parse selectedRiskIds: {e}")

        if isinstance(pubsub_message.get("data"), str):
            try:
                logger.info("ğŸ”“ Attempting Base64 decode...")
                logger.info(f"  - Data length (encoded): {len(pubsub_message['data'])}")
                logger.info(f"  - First 100 chars: {pubsub_message['data'][:100]}...")
                
                message_data = base64.b64decode(pubsub_message["data"]).decode("utf-8")
                logger.info(f"âœ… Base64 decode successful")
                logger.info(f"  - Decoded length: {len(message_data)}")
                logger.info(f"  - First 200 chars: {message_data[:200] if message_data else 'Empty'}...")
                
                if not message_data or not message_data.strip():
                    logger.warning(f"âš ï¸ Decoded message is empty or whitespace only")
                    return jsonify({"status": "ignored", "reason": "Empty message"}), 200
                
                storage_object = json.loads(message_data)
                logger.info(f"âœ… JSON parse of decoded data successful")
                logger.info(f"ğŸ“„ Storage Object keys: {list(storage_object.keys())}")
                logger.info(f"ğŸ“„ Full Storage Object: {json.dumps(storage_object, indent=2)}")
            except Exception as e:
                logger.error(f"âŒ Failed to decode PubSub message: {str(e)}")
                logger.error(f"âŒ Error type: {type(e).__name__}")
                logger.error(f"âŒ Stack trace: {traceback.format_exc()}")
                return jsonify({"error": "Bad Request: invalid message data"}), 400
        else:
            logger.error(f"âŒ PubSub message data is not a string, type: {type(pubsub_message.get('data'))}")
            return jsonify({"error": "Bad Request: message data must be base64 encoded"}), 400
            
        if not isinstance(storage_object, dict):
            logger.error(f"âŒ Invalid Storage Object format - type: {type(storage_object)}")
            return jsonify({"error": "Bad Request: invalid Storage Object"}), 400

        # Testç”¨ã§idãŒç„¡ã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆ
        if "id" not in storage_object:
            logger.warning("âš ï¸ Storage Object has no 'id' field, generating one for testing...")
            storage_object["id"] = f"test-{storage_object.get('name', 'unknown')}-{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
        object_id = storage_object.get("id", "")
        object_name = storage_object.get("name", "")
        object_bucket = storage_object.get("bucket", "")
        
        logger.info("ğŸ—‚ï¸ STORAGE OBJECT INFO:")
        logger.info(f"  - Object ID: {object_id}")
        logger.info(f"  - Object Name: {object_name}")
        logger.info(f"  - Bucket: {object_bucket}")
        logger.info(f"  - Content Type: {storage_object.get('contentType', 'N/A')}")
        logger.info(f"  - Size: {storage_object.get('size', 'N/A')} bytes")
        
        name_parts = object_name.split("/")
        logger.info(f"ğŸ“‚ Name parts: {name_parts}")
        logger.info(f"ğŸ“‚ Number of parts: {len(name_parts)}")
        
        if len(name_parts) < 3:
            logger.error(f"âŒ Invalid object name format: {object_name} - expected at least 3 parts")
            return jsonify({"error": "Invalid object name format"}), 400
            
        workspace_id = name_parts[0]
        project_id = name_parts[1]
        filename = "/".join(name_parts[2:])

        logger.info("âœ¨ EXTRACTED INFO:")
        logger.info(f"  - Workspace ID: {workspace_id}")
        logger.info(f"  - Project ID: {project_id}")
        logger.info(f"  - Filename: {filename}")

        if not filename.lower().endswith('.pdf'):
            logger.info(f"âš ï¸ Ignoring non-PDF file: {filename}")
            return jsonify({"message": "File ignored (not a PDF)"}), 200

        logger.info(f"ğŸš€ Starting PDF processing - workspace: {workspace_id}, project: {project_id}, file: {filename}")

        # bucketIdãŒãªã„å ´åˆã¯object_bucketã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        target_bucket = bucket_id if bucket_id else object_bucket
        logger.info(f"ğŸª£ Using bucket: {target_bucket}")

        # workspace_idã‚’æ•´æ•°ã«å¤‰æ›ï¼ˆattributesã‹ã‚‰å–å¾—ã—ãŸå€¤ã€ãªã‘ã‚Œã°ãƒ‘ã‚¹ã‹ã‚‰ï¼‰
        workspace_id_int = None
        if workspace_id_from_attr:
            try:
                workspace_id_int = int(workspace_id_from_attr)
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to parse workspaceId from attributes: {e}")

        if workspace_id_int is None:
            try:
                workspace_id_int = int(workspace_id)
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to parse workspaceId from path: {e}")

        logger.info(f"ğŸ“Š Final workspace_id (int): {workspace_id_int}")
        logger.info(f"ğŸ“Š Final selected_risk_ids: {selected_risk_ids}")

        result = process_single_pdf(target_bucket, object_name, workspace_id, project_id, workspace_id_int, selected_risk_ids)
        
        response = {
            "workspace_id": workspace_id,
            "project_id": project_id,
            "file": filename,
            "success": result["success"],
            "timestamp": datetime.now().isoformat()
        }
        
        if result["success"]:
            response["contract_json"] = result.get("contract_json", "")
            response["output_files"] = result.get("output_files", [])
            logger.info(f"Successfully processed PDF: {filename}")
        else:
            response["error"] = result.get("error", "Processing failed")
            logger.error(f"Failed to process PDF: {filename} - {result.get('error')}")
            
        return jsonify(response), 200
        
    except Exception as e:
        logger.error(f"Unexpected error in PubSub handler: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            "error": "Internal server error",
            "message": str(e)
        }), 200

def process_test_pdf(pdf_filename: Optional[str] = None) -> Dict[str, Any]:
    """
    ãƒ†ã‚¹ãƒˆç”¨ã®PDFå‡¦ç†ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã®pdf/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ï¼‰
    
    Args:
        pdf_filename: å‡¦ç†ã™ã‚‹PDFãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
    Returns:
        å‡¦ç†çµæœã‚’å«ã‚€è¾æ›¸
    """
    try:
        project_root = get_project_root()
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ã®pdf/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰PDFã‚’æ¤œç´¢
        local_pdf_dir = project_root / "pdf"
        
        if not local_pdf_dir.exists():
            return {
                'success': False,
                'error': 'PDF directory not found. Please place PDF files in /pdf/ directory.'
            }
        
        # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®æ±ºå®š
        if pdf_filename:
            pdf_path = local_pdf_dir / pdf_filename
            if not pdf_path.exists():
                return {
                    'success': False,
                    'error': f'PDF file not found: {pdf_filename}'
                }
        else:
            # è‡ªå‹•æ¤œå‡ºï¼šæœ€åˆã«è¦‹ã¤ã‹ã£ãŸPDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
            pdf_files = list(local_pdf_dir.glob("*.pdf"))
            if not pdf_files:
                return {
                    'success': False,
                    'error': 'No PDF files found in /pdf/ directory'
                }
            pdf_path = pdf_files[0]
            pdf_filename = pdf_path.name
        
        logger.info(f"Processing local PDF: {pdf_path}")
        
        # main_pipeline.pyã‚’å®Ÿè¡Œ
        pipeline_result = run_main_pipeline(str(pdf_path))
        
        if not pipeline_result["success"]:
            logger.error(f"Pipeline execution failed: {pipeline_result.get('error')}")
            return {
                'success': False,
                'error': pipeline_result.get('error', 'Pipeline execution failed'),
                'pipeline_output': pipeline_result
            }
        
        # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†
        basename = os.path.splitext(pdf_filename)[0]
        result_files = []
        contract_data = None
        
        # resultãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒ£ãƒ³
        result_dir = project_root / "result"
        if not result_dir.exists():
            result_dir = Path("/tmp/result")
        
        if result_dir.exists():
            for result_file in result_dir.glob("*"):
                if result_file.is_file():
                    file_info = {
                        "filename": result_file.name,
                        "size": result_file.stat().st_size,
                        "path": str(result_file)
                    }
                    
                    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯å†…å®¹ã‚‚èª­ã¿å–ã‚Š
                    if result_file.suffix == '.json':
                        try:
                            with open(result_file, 'r', encoding='utf-8') as f:
                                json_content = json.load(f)
                            file_info["content"] = json_content
                            
                            # integration_metadataã®å ´åˆã¯å¥‘ç´„ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æ‰±ã†
                            if 'integration_metadata' in result_file.name:
                                contract_data = json_content
                        except Exception as e:
                            logger.warning(f"Failed to read JSON file {result_file.name}: {e}")
                    
                    result_files.append(file_info)
        
        return {
            'success': True,
            'pdf_file': pdf_filename,
            'pdf_path': str(pdf_path),
            'result_files': result_files,
            'contract_data': contract_data,
            'pipeline_result': pipeline_result,
            'processing_time': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error processing test PDF: {str(e)}")
        return {
            'success': False,
            'error': str(e)
        }

def process_single_pdf(bucket_name: str, object_name: str, workspace_id: str, project_id: str, workspace_id_int: Optional[int] = None, selected_risk_ids: Optional[List[int]] = None) -> Dict[str, Any]:
    """
    å˜ä¸€ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†

    Args:
        bucket_name: GCSãƒã‚±ãƒƒãƒˆå
        object_name: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ (workspace_id/project_id/filename.pdf)
        workspace_id: ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹IDï¼ˆæ–‡å­—åˆ—ï¼‰
        project_id: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
        workspace_id_int: ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹IDï¼ˆæ•´æ•°ã€ãƒªã‚¹ã‚¯å–å¾—ç”¨ï¼‰
        selected_risk_ids: é¸æŠã•ã‚ŒãŸãƒªã‚¹ã‚¯IDã®ãƒªã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

    Returns:
        å‡¦ç†çµæœã‚’å«ã‚€è¾æ›¸
    """
    try:
        gcs_uri = f"gs://{bucket_name}/{object_name}"
        logger.info(f"Processing PDF from: {gcs_uri}")
        
        # Cloud Runå¯¾å¿œï¼šæ›¸ãè¾¼ã¿å¯èƒ½ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨
        pdf_dir = Path("/tmp/pdf")
        pdf_dir.mkdir(exist_ok=True)
        
        # GCSã‹ã‚‰PDFã‚’ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        filename = os.path.basename(object_name)
        local_file_path = pdf_dir / filename
        download_from_gcs(gcs_uri, str(local_file_path))

        # å‡¦ç†é–‹å§‹å‰ã«resultãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        project_root = get_project_root()
        result_dir = project_root / "result"
        if not result_dir.exists():
            result_dir = Path("/tmp/result")

        # è©³ç´°ãƒ­ã‚°è¿½åŠ ã§ãƒ‡ãƒãƒƒã‚°
        logger.info(f"ğŸ” project_root: {project_root}")
        logger.info(f"ğŸ” result_dir: {result_dir}")
        logger.info(f"ğŸ” result_dir.exists(): {result_dir.exists()}")

        # é‡è¦: Cloud Runã§ã¯æ¯å›æ–°ã—ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒèµ·å‹•ã•ã‚Œã‚‹ã¯ãšã ãŒã€
        # Dockerã‚¤ãƒ¡ãƒ¼ã‚¸ã«å¤ã„resultãƒ•ã‚¡ã‚¤ãƒ«ãŒå«ã¾ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€
        # å‡¦ç†é–‹å§‹å‰ã«å¿…ãšresultãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if result_dir.exists():
            logger.info(f"ğŸ§¹ Cleaning up result directory before processing: {result_dir}")
            for old_file in result_dir.glob("*"):
                if old_file.is_file():
                    try:
                        old_file.unlink()
                        logger.info(f"ğŸ—‘ï¸ Deleted old file: {old_file.name}")
                    except Exception as e:
                        logger.warning(f"Failed to delete old file {old_file.name}: {e}")

        logger.info(f"Starting OCR pipeline for: {local_file_path}")

        # main_pipeline.pyã‚’å®Ÿè¡Œ
        pipeline_result = run_main_pipeline(str(local_file_path))

        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œå¾Œã®è©³ç´°ãƒ­ã‚°
        logger.info(f"ğŸ” After pipeline execution:")
        logger.info(f"ğŸ” result_dir.exists(): {result_dir.exists()}")
        if result_dir.exists():
            all_files = list(result_dir.glob("*"))
            logger.info(f"ğŸ” Files in result_dir: {[f.name for f in all_files]}")
        else:
            logger.warning(f"âš ï¸ result_dir does not exist: {result_dir}")

        # åˆ†å‰²å¾Œã®ãƒšãƒ¼ã‚¸æ•°ã‚’è¨ˆç®—ã—ã¦DBã«ä¿å­˜
        try:
            total_page_count = calculate_total_page_count(pipeline_result)
            if total_page_count > 0:
                save_page_count_to_db(project_id, total_page_count, bucket_name)
                logger.info(f"ğŸ“Š Total page count after split: {total_page_count}")
            else:
                logger.warning(f"âš ï¸ Could not calculate page count from pipeline result")
        except Exception as e:
            logger.error(f"âŒ Failed to save page count: {e}")
            # ãƒšãƒ¼ã‚¸æ•°ä¿å­˜å¤±æ•—ã—ã¦ã‚‚OCRå‡¦ç†ã¯ç¶šè¡Œ

        # è¿½åŠ ã®å€™è£œãƒ‘ã‚¹ã‚‚ãƒã‚§ãƒƒã‚¯
        alternative_paths = [
            Path("/app/result"),
            project_root / "src" / "result",
            Path.cwd() / "result",
            Path("/tmp/result"),
            Path("/app/src/result")
        ]

        for alt_path in alternative_paths:
            logger.info(f"ğŸ” Checking alternative path: {alt_path}")
            logger.info(f"ğŸ” Path exists: {alt_path.exists()}")
            if alt_path.exists():
                files = list(alt_path.glob("*"))
                logger.info(f"ğŸ” Files in {alt_path}: {[f.name for f in files]}")
                if files:
                    logger.info(f"ğŸ“ Found {len(files)} files in {alt_path} - using this as result directory")
                    result_dir = alt_path  # å®Ÿéš›ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´æ‰€ã‚’ä½¿ç”¨
                    break

        if not pipeline_result["success"]:
            logger.error(f"Pipeline execution failed: {pipeline_result.get('error')}")
            return {
                'success': False,
                'error': pipeline_result.get('error', 'Pipeline execution failed')
            }

        # å‡¦ç†å¾Œã«PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        if local_file_path.exists():
            local_file_path.unlink()
            logger.info(f"Cleaned up local PDF file: {local_file_path}")

        # å‡ºåŠ›å…ˆãƒã‚±ãƒƒãƒˆã¯å…¥åŠ›ã¨åŒã˜ãƒã‚±ãƒƒãƒˆã‚’ä½¿ç”¨
        output_bucket = bucket_name

        # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ - ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œå¾Œã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
        basename = os.path.splitext(filename)[0]
        output_files = []
        
        txt_files_to_delete = []  # å‰Šé™¤äºˆå®šã®txtãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½è·¡

        if result_dir.exists():
            # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«ï¼‰
            # é‡è¦: ç¾åœ¨ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ã‚’å‡¦ç†ã™ã‚‹
            all_files = list(result_dir.glob("*"))
            logger.info(f"ğŸ“‚ Found {len(all_files)} files in result directory for basename '{basename}'")
            for result_file in all_files:
                if result_file.is_file():
                    logger.info(f"ğŸ” Processing file: {result_file.name}")
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯ã®ã¿å®Ÿè¡Œ
                    # å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«åˆ¤å®šã¯å‰Šé™¤ã—ã€ç¾åœ¨ã®PDFã«é–¢é€£ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç†

                    # ãƒ•ã‚¡ã‚¤ãƒ«åã«ç¾åœ¨ã®PDFã®basenameãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    if basename not in result_file.name:
                        logger.info(f"â­ï¸ Skipping file from different PDF: {result_file.name}")
                        continue
                    # å¥‘ç´„æ›¸ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSONã¯ä¿å­˜ã—ãªã„
                    if result_file.suffix == '.json' and 'integration_metadata' in result_file.name:
                        logger.info(f"ğŸš« Skipping contract metadata JSON: {result_file.name}")
                        continue

                    # txtãƒ•ã‚¡ã‚¤ãƒ«ã¯æ§‹é€ åŒ–å‡¦ç†ã§ä½¿ç”¨ã—ã€ãã®å¾Œå‰Šé™¤
                    if result_file.suffix == '.txt':
                        logger.info(f"ğŸ“ Found txt file for local processing: {result_file.name}")
                        txt_files_to_delete.append(result_file)
                        # GCSã«ã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã›ãšã€ãƒ­ãƒ¼ã‚«ãƒ«ã§å‡¦ç†
                        continue

                    # ãã®ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ocr_resultsã«ä¿å­˜
                    output_prefix = f"{workspace_id}/{project_id}/ocr_results/"
                    gcs_path = upload_file_to_gcs(
                        str(result_file),
                        output_bucket,
                        output_prefix + result_file.name
                    )
                    output_files.append(gcs_path)
                    logger.info(f"âœ… Result file uploaded to: {gcs_path}")

        # ãƒ­ãƒ¼ã‚«ãƒ«ã®txtãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦Geminiã§æ§‹é€ åŒ–
        structured_json_path = None
        logger.info(f"ğŸ” Looking for local txt files to structure. Found {len(txt_files_to_delete)} txt files")
        for txt_file in txt_files_to_delete:
            # çµ±åˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆintegratedã‚’å«ã‚€ï¼‰ã‚’ç‰¹å®š
            if 'integrated' in txt_file.name:
                logger.info(f"ğŸ¯ Found integrated file for structuring: {txt_file.name}")
                try:
                    logger.info(f"ğŸ§  Starting Gemini structured output for local file: {txt_file.name}")
                    # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç›´æ¥ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿
                    with open(txt_file, 'r', encoding='utf-8') as f:
                        file_content = f.read()

                    # Geminiã®æ§‹é€ åŒ–å‡ºåŠ›ã‚’ä½¿ç”¨ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç‰ˆï¼‰
                    structured_result = convert_local_text_to_contract_schema(file_content, basename, workspace_id, project_id, output_bucket, workspace_id_int, selected_risk_ids)
                    if structured_result:
                        # æ§‹é€ åŒ–ã•ã‚ŒãŸJSONã‚’after_ocrã«ä¿å­˜
                        json_output_path = f"{workspace_id}/{project_id}/after_ocr/{basename}.json"
                        structured_json_path = upload_json_to_gcs(
                            structured_result,
                            output_bucket,
                            json_output_path
                        )
                        logger.info(f"âœ… Structured contract JSON saved to: {structured_json_path}")
                        break
                    else:
                        logger.warning(f"âš ï¸ Gemini structured output returned None for: {txt_file.name}")
                except Exception as e:
                    logger.error(f"âŒ Failed to structure contract data for {txt_file.name}: {str(e)}")
                    logger.error(f"âŒ Stack trace: {traceback.format_exc()}")
                    continue

        # æ§‹é€ åŒ–å‡¦ç†ãŒå®Œäº†ã—ãŸã‚‰ã€ãƒ­ãƒ¼ã‚«ãƒ«ã®txtãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        for txt_file in txt_files_to_delete:
            try:
                txt_file.unlink()
                logger.info(f"ğŸ—‘ï¸ Deleted local txt file: {txt_file.name}")
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to delete txt file {txt_file.name}: {e}")

        # GCSä¸Šã«txtãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ãªã„ãŸã‚ã€å‰Šé™¤å‡¦ç†ã¯ä¸è¦
        
        return {
            'success': True,
            'output_files': output_files,
            'structured_json': structured_json_path,
            'pipeline_result': pipeline_result
        }
        
    except Exception as e:
        logger.error(f"Error processing PDF: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            'success': False,
            'error': str(e)
        }

def split_contracts_by_termination(articles: list) -> list:
    """
    å¥‘ç´„æ›¸é…åˆ—ã‚’ã€Œå¥‘ç´„æ›¸çµ‚äº†ã€ã§åˆ†å‰²ã™ã‚‹

    Args:
        articles: æ¡æ–‡é…åˆ—

    Returns:
        å¥‘ç´„æ›¸ã”ã¨ã«åˆ†å‰²ã•ã‚ŒãŸé…åˆ—ã®ãƒªã‚¹ãƒˆ
    """
    contracts = []
    current_contract = []
    current_info = None

    for article in articles:
        # å¥‘ç´„æ›¸åŸºæœ¬æƒ…å ±ï¼ˆ2ã¤ç›®ä»¥é™ã®å¥‘ç´„æ›¸ï¼‰ã‚’æ¤œå‡º
        if "title" in article and "party" in article and "article_number" not in article:
            current_info = article
            continue

        # å¥‘ç´„æ›¸çµ‚äº†ã‚’æ¤œå‡º
        if article.get("title") == "å¥‘ç´„æ›¸çµ‚äº†" and article.get("content") == "----------":
            if current_contract:
                contracts.append({
                    "info": current_info,
                    "articles": current_contract
                })
                current_contract = []
                current_info = None
        else:
            current_contract.append(article)

    # æœ€å¾Œã®å¥‘ç´„æ›¸ã‚’è¿½åŠ 
    if current_contract:
        contracts.append({
            "info": current_info,
            "articles": current_contract
        })

    return contracts


def classify_contract_risks(articles: list, target_company: str, workspace_id: Optional[int] = None, selected_risk_ids: Optional[List[int]] = None, bucket_name: Optional[str] = None) -> list:
    """
    å¥‘ç´„æ›¸æ¡æ–‡ã‹ã‚‰ãƒªã‚¹ã‚¯ã‚’åˆ†é¡ã™ã‚‹ï¼ˆVertex AIä½¿ç”¨ï¼‰

    Args:
        articles: æ¡æ–‡é…åˆ—
        target_company: å¯¾è±¡ä¼šç¤¾å
        workspace_id: ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹IDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        selected_risk_ids: é¸æŠã•ã‚ŒãŸãƒªã‚¹ã‚¯IDã®ãƒªã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

    Returns:
        ãƒªã‚¹ã‚¯åˆ†é¡çµæœã®é…åˆ—
    """
    try:
        import vertexai
        from vertexai.generative_models import GenerativeModel, GenerationConfig, FunctionDeclaration, Tool

        project_id_env = os.getenv('GCP_PROJECT_ID')
        location = os.getenv('GCP_LOCATION', 'us-central1')
        if not project_id_env:
            logger.error("GCP_PROJECT_IDç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return []

        vertexai.init(project=project_id_env, location=location)

        # DBã‹ã‚‰ãƒªã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã‚’å–å¾—
        risks = get_risks_from_db(workspace_id=workspace_id, selected_risk_ids=selected_risk_ids, bucket_name=bucket_name)
        if not risks:
            logger.error("âŒ No risks found in database")
            return []

        logger.info(f"ğŸ“Š Fetched {len(risks)} risk types from database")

        # ãƒªã‚¹ã‚¯IDã®ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆï¼ˆæ–‡å­—åˆ—ã¨ã—ã¦ï¼‰
        risk_ids = [str(risk['id']) for risk in risks]

        # Function Declarationï¼ˆãƒªã‚¹ã‚¯åˆ†é¡çµæœã‚’å—ã‘å–ã‚‹é–¢æ•°å®šç¾©ï¼‰
        set_classifications_func = FunctionDeclaration(
            name="setClassifications",
            description="å¥‘ç´„æ›¸ã®ãƒªã‚¹ã‚¯åˆ†é¡çµæœã‚’è¨­å®šã™ã‚‹",
            parameters={
                "type": "object",
                "properties": {
                    "classifications": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "text": {"type": "string", "description": "ãƒªã‚¹ã‚¯æ¡æ–‡ã®åŸæ–‡"},
                                "type": {"type": "string", "enum": risk_ids, "description": f"ãƒªã‚¹ã‚¯ã‚¿ã‚¤ãƒ—IDï¼ˆ{', '.join(risk_ids)}ï¼‰"},
                                "reason": {"type": "string", "description": "ãƒªã‚¹ã‚¯ã®ç†ç”±"},
                                "pageNumber": {"type": "integer", "description": "ãƒšãƒ¼ã‚¸ç•ªå·ï¼ˆä¸æ˜ã¯-1ï¼‰"},
                                "articleInfo": {"type": "string", "description": "æ¡æ–‡ç•ªå·ï¼ˆä¾‹: ç¬¬10æ¡ï¼‰"},
                                "articleTitle": {"type": "string", "description": "æ¡æ–‡ã‚¿ã‚¤ãƒˆãƒ«"},
                                "articleOverview": {"type": "string", "description": "æŸ±æ›¸"},
                                "specificClause": {"type": "string", "description": "å…·ä½“çš„ãªå·"}
                            },
                            "required": ["text", "type", "reason", "pageNumber"]
                        }
                    }
                },
                "required": ["classifications"]
            }
        )

        # ãƒ„ãƒ¼ãƒ«è¨­å®š
        risk_classification_tool = Tool(
            function_declarations=[set_classifications_func]
        )

        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        model = GenerativeModel(
            'gemini-2.5-flash',
            tools=[risk_classification_tool]
        )

        # æ¡æ–‡ä¸€è¦§ã‚’æ§‹ç¯‰
        articles_text = "\n".join([
            f"### {article.get('article_number', '')} {article.get('title', '')}\n{article.get('content', '')}"
            for article in articles
        ])

        # ãƒªã‚¹ã‚¯ã‚¿ã‚¤ãƒ—èª¬æ˜ã‚’æ§‹ç¯‰ï¼ˆDBã‹ã‚‰å–å¾—ã—ãŸãƒªã‚¹ã‚¯ã‚’ä½¿ç”¨ï¼‰
        risk_types_text = "\n\n".join([
            f"{risk['id']}. {risk['title']}: {risk['description']}"
            for risk in risks
        ])

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆç°¡ç•¥ç‰ˆ - å®Ÿéš›ã¯TypeScriptã®é•·ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç§»æ¤ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼‰
        prompt = f"""ã‚ãªãŸã¯M&Aæ³•å‹™DDã«é•·ã‘ãŸå¼è­·å£«ã§ã™ã€‚å¯¾è±¡ä¼šç¤¾ã€Œ{target_company}ã€ã®è¦–ç‚¹ã§ãƒªã‚¹ã‚¯æ¡é …ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

## æ¡æ–‡ä¸€è¦§
{articles_text}

## åˆ©ç”¨å¯èƒ½ãªãƒªã‚¹ã‚¯ã‚¿ã‚¤ãƒ—
{risk_types_text}

ä¸ç¢ºå®Ÿãªã‚‰å‡ºåŠ›ã—ãªã„ï¼ˆã‚¢ãƒ–ã‚¹ãƒ†ã‚¤ãƒ³ï¼‰ã€‚ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ä¸ååˆ†ãƒ»ä¸»ä½“ä¸ç‰¹å®šãƒ»åˆ¤å®šãŒæ›–æ˜§ãªã‚‰ç„¡å‡ºåŠ›ã€‚

**setClassificationsé–¢æ•°ã‚’ä½¿ã£ã¦ã€ãƒªã‚¹ã‚¯åˆ†é¡çµæœã‚’è¿”ã—ã¦ãã ã•ã„ã€‚**
"""

        # Vertex AIå‘¼ã³å‡ºã—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–ï¼‰
        import asyncio
        from vertexai.generative_models import GenerationConfig

        async def generate_with_timeout():
            # Function Callingã‚’å¼·åˆ¶ã™ã‚‹ãŸã‚ã®è¨­å®š
            generation_config = GenerationConfig(
                temperature=0.1,
            )
            response = await model.generate_content_async(
                prompt,
                generation_config=generation_config,
            )
            return response

        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            response = loop.run_until_complete(
                asyncio.wait_for(generate_with_timeout(), timeout=3600)
            )
        except asyncio.TimeoutError:
            logger.error(f"â±ï¸ Timeout after 3600 seconds while classifying risks")
            return []
        finally:
            loop.close()

        # Function Callã®çµæœã‚’å–å¾—
        logger.info(f"ğŸ“Š Response from Gemini: {response}")
        logger.info(f"ğŸ“Š Candidates: {response.candidates if hasattr(response, 'candidates') else 'No candidates'}")

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®æ¤œè¨¼
        if not response.candidates or len(response.candidates) == 0:
            logger.error("âŒ No candidates in response")
            return []

        candidate = response.candidates[0]
        logger.info(f"ğŸ“Š Candidate content: {candidate.content if hasattr(candidate, 'content') else 'No content'}")

        if not hasattr(candidate, 'content') or not candidate.content.parts:
            logger.error("âŒ No content or parts in candidate")
            return []

        part = candidate.content.parts[0]
        if not hasattr(part, 'function_call'):
            logger.error(f"âŒ No function_call in part. Part type: {type(part)}, Part: {part}")
            return []

        function_call = part.function_call

        if function_call and function_call.name == "setClassifications":
            classifications = function_call.args.get("classifications", [])
            logger.info(f"âœ… Successfully classified {len(classifications)} risks")

            # IDã‚’ç”Ÿæˆã—ã¦è¿”ã™
            import time
            import random
            return [
                {
                    "id": f"{int(time.time() * 1000)}-{random.randint(100000, 999999)}",
                    "text": c.get("text", ""),
                    "type": c.get("type", ""),
                    "reason": c.get("reason", ""),
                    "pageNumber": c.get("pageNumber", -1),
                    "articleInfo": c.get("articleInfo", ""),
                    "articleTitle": c.get("articleTitle", ""),
                    "articleOverview": c.get("articleOverview", ""),
                    "specificClause": c.get("specificClause", "")
                }
                for c in classifications
            ]

        return []

    except Exception as e:
        logger.error(f"Error classifying contract risks: {str(e)}")
        logger.error(traceback.format_exc())
        return []


def add_risks_to_contract_data(structured_data: Dict[str, Any], workspace_id: Optional[int] = None, selected_risk_ids: Optional[List[int]] = None, bucket_name: Optional[str] = None) -> Dict[str, Any]:
    """
    æ§‹é€ åŒ–ã•ã‚ŒãŸå¥‘ç´„æ›¸ãƒ‡ãƒ¼ã‚¿ã«ãƒªã‚¹ã‚¯åˆ†é¡ã‚’è¿½åŠ ã™ã‚‹

    Args:
        structured_data: æ§‹é€ åŒ–ã•ã‚ŒãŸå¥‘ç´„æ›¸ãƒ‡ãƒ¼ã‚¿
        workspace_id: ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹IDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        selected_risk_ids: é¸æŠã•ã‚ŒãŸãƒªã‚¹ã‚¯IDã®ãƒªã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

    Returns:
        ãƒªã‚¹ã‚¯åˆ†é¡ãŒè¿½åŠ ã•ã‚ŒãŸå¥‘ç´„æ›¸ãƒ‡ãƒ¼ã‚¿
    """
    try:
        # TODO: å¯¾è±¡ä¼šç¤¾åã¯å¾Œã§DBã‹ã‚‰å–å¾—ã™ã‚‹
        # ç¾åœ¨ã¯info.partyã‹ã‚‰å–å¾—ï¼ˆæœ€åˆã®å½“äº‹è€…ã‚’å¯¾è±¡ä¼šç¤¾ã¨ã¿ãªã™ï¼‰
        main_party = structured_data.get("info", {}).get("party", "")
        target_company = main_party.split(",")[0].strip() if main_party else "å¯¾è±¡ä¼šç¤¾"

        logger.info(f"ğŸ¯ Starting risk classification for target company: {target_company}")
        if workspace_id:
            logger.info(f"ğŸ“¦ Workspace ID: {workspace_id}")
        if selected_risk_ids:
            logger.info(f"ğŸ¯ Selected Risk IDs: {selected_risk_ids}")

        # å¥‘ç´„æ›¸ã‚’åˆ†å‰²
        articles = structured_data.get("result", {}).get("articles", [])
        contracts = split_contracts_by_termination(articles)

        logger.info(f"ğŸ“„ Found {len(contracts)} contract(s) in document")

        # å„å¥‘ç´„æ›¸ã”ã¨ã«ãƒªã‚¹ã‚¯åˆ†é¡
        contract_risks = []
        for i, contract in enumerate(contracts):
            contract_info = contract.get("info")
            contract_articles = contract.get("articles", [])

            # å¯¾è±¡ä¼šç¤¾åã‚’ç‰¹å®šï¼ˆ2ã¤ç›®ä»¥é™ã®å¥‘ç´„æ›¸ã¯å€‹åˆ¥ã®partyã‹ã‚‰å–å¾—ï¼‰
            if contract_info and "party" in contract_info:
                contract_party = contract_info.get("party", "")
                contract_target = contract_party.split(",")[0].strip() if contract_party else target_company
            else:
                contract_target = target_company

            logger.info(f"ğŸ” Classifying risks for contract {i+1}/{len(contracts)} (target: {contract_target})")
            logger.info(f"ğŸ“Š Contract {i+1} has {len(contract_articles)} articles")

            # ãƒªã‚¹ã‚¯åˆ†é¡å®Ÿè¡Œï¼ˆworkspace_idã¨selected_risk_idsã‚’æ¸¡ã™ï¼‰
            risks = classify_contract_risks(contract_articles, contract_target, workspace_id=workspace_id, selected_risk_ids=selected_risk_ids, bucket_name=bucket_name)

            logger.info(f"âœ… Contract {i+1} classification returned {len(risks)} risks")
            for risk_idx, risk in enumerate(risks):
                logger.info(f"   Risk {risk_idx+1}: {risk.get('articleInfo', 'N/A')} - {risk.get('type', 'N/A')}")

            # å¥‘ç´„æ›¸ã”ã¨ã®æƒ…å ±ã‚’æ§‹ç¯‰
            contract_risks.append({
                "contractIndex": i,
                "targetCompany": contract_target,
                "articleCount": len(contract_articles),
                "risks": risks
            })

            logger.info(f"âœ… Contract {i+1} completed with {len(risks)} risks")

        # å…ƒã®ãƒ‡ãƒ¼ã‚¿ã«risksã‚­ãƒ¼ã‚’è¿½åŠ ï¼ˆå¥‘ç´„æ›¸ã”ã¨ã«åˆ†å‰²ï¼‰
        total_risks = sum(len(c["risks"]) for c in contract_risks)
        structured_data["risks"] = {
            "contracts": contract_risks
        }
        logger.info(f"âœ… Total {total_risks} risks added to structured data ({len(contract_risks)} contract(s))")

        return structured_data

    except Exception as e:
        logger.error(f"Error adding risks to contract data: {str(e)}")
        logger.error(traceback.format_exc())
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚risksã‚­ãƒ¼ã¯è¿½åŠ ï¼ˆç©ºã®å¥‘ç´„æ›¸é…åˆ—ï¼‰
        structured_data["risks"] = {
            "contracts": []
        }
        return structured_data


def convert_local_text_to_contract_schema(file_content: str, basename: str, workspace_id: str, project_id: str, bucket_name: str, workspace_id_int: Optional[int] = None, selected_risk_ids: Optional[List[int]] = None) -> Optional[Dict[str, Any]]:
    """
    ãƒ­ãƒ¼ã‚«ãƒ«ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’Vertex AIã®æ§‹é€ åŒ–å‡ºåŠ›ã‚’ä½¿ã£ã¦å¥‘ç´„æ›¸ã‚¹ã‚­ãƒ¼ãƒã«å¤‰æ›

    Args:
        file_content: ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹
        basename: ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹å
        workspace_id: ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹IDï¼ˆæ–‡å­—åˆ—ï¼‰
        project_id: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
        bucket_name: GCSãƒã‚±ãƒƒãƒˆå
        workspace_id_int: ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹IDï¼ˆæ•´æ•°ã€ãƒªã‚¹ã‚¯å–å¾—ç”¨ï¼‰
        selected_risk_ids: é¸æŠã•ã‚ŒãŸãƒªã‚¹ã‚¯IDã®ãƒªã‚¹ãƒˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

    Returns:
        æ§‹é€ åŒ–ã•ã‚ŒãŸå¥‘ç´„æ›¸ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯None
    """
    try:
        # Vertex AIè¨­å®š
        import vertexai
        from vertexai.generative_models import GenerativeModel, GenerationConfig

        project_id_env = os.getenv('GCP_PROJECT_ID')
        location = os.getenv('GCP_LOCATION', 'us-central1')
        if not project_id_env:
            logger.error("GCP_PROJECT_IDç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None

        vertexai.init(project=project_id_env, location=location)

        # å¥‘ç´„æ›¸ã‚¹ã‚­ãƒ¼ãƒã®å®šç¾©
        contract_schema = {
            "type": "object",
            "properties": {
                "success": {"type": "boolean"},
                "info": {
                    "type": "object",
                    "properties": {
                        "title": {"type": "string"},
                        "party": {"type": "string"},  # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®å½“äº‹è€…å
                        "start_date": {"type": "string"},  # ç©ºæ–‡å­—åˆ—ã§å¯¾å¿œ
                        "end_date": {"type": "string"},  # ç©ºæ–‡å­—åˆ—ã§å¯¾å¿œ
                        "conclusion_date": {"type": "string"}  # ç©ºæ–‡å­—åˆ—ã§å¯¾å¿œ
                    },
                    "required": ["title", "party"]
                },
                "result": {
                    "type": "object",
                    "properties": {
                        "articles": {
                            "type": "array",
                            "items": {
                                "anyOf": [
                                    {
                                        "type": "object",
                                        "properties": {
                                            "article_number": {"type": "string"},
                                            "title": {"type": "string"},
                                            "content": {"type": "string"},
                                            "table_number": {"type": "string"}
                                        },
                                        "required": ["content", "title"]
                                    },
                                    {
                                        "type": "object",
                                        "properties": {
                                            "title": {"type": "string"},
                                            "party": {"type": "string"},
                                            "start_date": {"type": "string"},
                                            "end_date": {"type": "string"},
                                            "conclusion_date": {"type": "string"}
                                        },
                                        "required": ["title", "party"]
                                    }
                                ]
                            }
                        }
                    },
                    "required": ["articles"]
                }
            },
            "required": ["success", "info", "result"]
        }

        if not file_content:
            logger.warning(f"Empty content provided")
            return None

        # Vertex AIãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ï¼ˆæ§‹é€ åŒ–å‡ºåŠ›å¯¾å¿œï¼‰
        model = GenerativeModel('gemini-2.5-flash')
        generation_config = GenerationConfig(
            response_mime_type="application/json",
            response_schema=contract_schema
        )

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆ
        prompt = f"""
ä»¥ä¸‹ã®OCRå‡¦ç†æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æã—ã€å¥‘ç´„æ›¸ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ãƒ•ã‚¡ã‚¤ãƒ«å: {basename}

ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹:
{file_content}

æŠ½å‡ºæŒ‡ç¤º:
1. success: å¸¸ã«true
2. infoéƒ¨åˆ†ï¼ˆ1ã¤ç›®ã®å¥‘ç´„æ›¸ã®æƒ…å ±ã®ã¿ï¼‰:
   - title: å¥‘ç´„æ›¸ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½¿ç”¨ï¼‰
   - party: å¥‘ç´„å½“äº‹è€…ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¨˜è¼‰ï¼ˆä¾‹: "æ ªå¼ä¼šç¤¾A,æ ªå¼ä¼šç¤¾B"ï¼‰
   - start_date: å¥‘ç´„é–‹å§‹æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰
   - end_date: å¥‘ç´„çµ‚äº†æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰
   - conclusion_date: å¥‘ç´„ç· çµæ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰

3. resultéƒ¨åˆ†:
   - articles: å¥‘ç´„æ¡é …ã®é…åˆ—ï¼ˆå…¨ã¦ã®æ¡é …ã‚’æ¼ã‚ŒãªãæŠ½å‡ºï¼‰
     - article_number: æ¡é …ç•ªå·ï¼ˆä¾‹: "ç¬¬1æ¡"ã€"ç¬¬2æ¡"ã€ç•ªå·ãŒãªã„å ´åˆã¯"ç½²åæ¬„"ç­‰ï¼‰
     - title: æ¡é …ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè¦‹å‡ºã—ãŒãªã„å ´åˆã¯å†…å®¹ã‹ã‚‰è¦ç´„ï¼‰
     - content: æ¡é …ã®å®Œå…¨ãªå†…å®¹ï¼ˆçœç•¥ç¦æ­¢ï¼‰
     - table_number: è¡¨ãŒã‚ã‚‹å ´åˆã®ã¿è¡¨ç•ªå·

é‡è¦ãªæ³¨æ„äº‹é …:
- ãƒ†ã‚­ã‚¹ãƒˆå†…ã®å…¨ã¦ã®æ¡é …ã‚’å¿…ãšæŠ½å‡ºã—ã¦ãã ã•ã„ï¼ˆç¬¬1æ¡ã‹ã‚‰æœ€å¾Œã¾ã§ï¼‰
- å„æ¡é …ã®contentã¯å®Œå…¨ã«ã‚³ãƒ”ãƒ¼ã—ã€çœç•¥ã‚„è¦ç´„ã¯è¡Œã‚ãªã„ã§ãã ã•ã„
- æ¡é …ç•ªå·ãŒæ˜è¨˜ã•ã‚Œã¦ã„ãªã„éƒ¨åˆ†ï¼ˆå‰æ–‡ã€ç½²åæ¬„ã€ä»˜è¨˜ç­‰ï¼‰ã‚‚ç‹¬ç«‹ã—ãŸæ¡é …ã¨ã—ã¦æ‰±ã£ã¦ãã ã•ã„
- æ—¥ä»˜ã¯å¯èƒ½ãªé™ã‚ŠYYYY-MM-DDå½¢å¼ã«å¤‰æ›ã—ã¦ãã ã•ã„
- è¡¨ã‚„å›³ãŒã‚ã‚‹å ´åˆã¯HTMLå½¢å¼ã§contentã«å«ã‚ã¦ãã ã•ã„
- ç½²åæ¬„ã‚‚å¿…ãš1ã¤ã®æ¡é …ã¨ã—ã¦æ‰±ã£ã¦ãã ã•ã„
- å‡ºåŠ›ã¯å¿…ãšå®Œå…¨ãªJSONå½¢å¼ã§ã€é€”ä¸­ã§åˆ‡ã‚Œã‚‹ã“ã¨ãªãæœ€å¾Œã¾ã§å‡ºåŠ›ã—ã¦ãã ã•ã„

ã€è¤‡æ•°å¥‘ç´„æ›¸ãŒã‚ã‚‹å ´åˆã®ç‰¹åˆ¥ãƒ«ãƒ¼ãƒ«ã€‘:
1. å¥‘ç´„æ›¸å†…éƒ¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨å¥‘ç´„æ›¸ã®åŒºåˆ‡ã‚Šã‚’æ­£ç¢ºã«åˆ¤åˆ¥:
   - ã€Œé ­æ›¸ã€ã€Œè¦é …ã€ã€Œå¥‘ç´„æ›¸æœ¬æ–‡ã€ã€Œç”¨ç´™ã€ã€Œæ¡ä»¶è¡¨ã€ã€Œæ¦‚è¦ã€ã€Œç‰¹ç´„ã€ã€Œç´°å‰‡ã€ã€Œåˆ¥ç´™ã€ã€Œä»•æ§˜æ›¸ã€ã€Œåˆ¥æ·»ã€ã€Œå›³é¢ã€ã€Œç´„æ¬¾ã€ã€Œæ´¾é£å€‹åˆ¥å¥‘ç´„ç¥¨å¥‘ç´„åŸºæœ¬æƒ…å ±ã€ã€Œå®šç¾©ä¸€è¦§è¡¨ã€ãªã©ã¯å¥‘ç´„æ›¸ã®å†…éƒ¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã‚ã‚Šã€åŒºåˆ‡ã‚Šã§ã¯ã‚ã‚Šã¾ã›ã‚“
   - ã“ã‚Œã‚‰ã¯1ã¤ã®å¥‘ç´„æ›¸ã‚’æ§‹æˆã™ã‚‹è¦ç´ ã¨ã—ã¦ã€åŒã˜articlesé…åˆ—å†…ã«å«ã‚ã¦ãã ã•ã„
   - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã®ã‚¿ã‚¤ãƒˆãƒ«ã€å¥‘ç´„å½“äº‹è€…ã€å¥‘ç´„æ¡é …ã€ç½²åãªã©ã®é‡è¦ãªæƒ…å ±ãŒåˆ†å‰²å¾Œã‚‚å®Œå…¨ã«ä¿æŒã•ã‚Œã‚‹ã‚ˆã†ã«èª¿æŸ»ã—ã¦ãã ã•ã„
   - åˆ†å‰²ã«ã‚ˆã‚Šæƒ…å ±ã®æ¬ å¦‚ã‚„æ¼ã‚ŒãŒç™ºç”Ÿã—ãªã„ã‚ˆã†ã€æ…é‡ã«åˆ†æã—ã¦ãã ã•ã„

2. å¥‘ç´„æ›¸ã®çµ‚äº†ã‚’ç¤ºã™ç®‡æ‰€ï¼ˆã€Œä»¥ä¸Šã€ç­‰ï¼‰ã¯ã€ä»¥ä¸‹ã®å½¢å¼ã§çµ±ä¸€:
   {{
     "article_number": "",
     "title": "å¥‘ç´„æ›¸çµ‚äº†",
     "content": "----------",
     "table_number": ""
   }}

3. 2ã¤ç›®ä»¥é™ã®å¥‘ç´„æ›¸ãŒå§‹ã¾ã‚‹å ´åˆã€å¥‘ç´„æ›¸çµ‚äº†ã®ç›´å¾Œã«å¥‘ç´„æ›¸åŸºæœ¬æƒ…å ±ã‚’ãã®ã¾ã¾æŒ¿å…¥:
   {{
     "title": "[å¥‘ç´„æ›¸ã‚¿ã‚¤ãƒˆãƒ«]",
     "party": "[å½“äº‹è€…ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š]",
     "start_date": "[YYYY-MM-DD ã¾ãŸã¯ç©ºæ–‡å­—åˆ—]",
     "end_date": "[YYYY-MM-DD ã¾ãŸã¯ç©ºæ–‡å­—åˆ—]",
     "conclusion_date": "[YYYY-MM-DD ã¾ãŸã¯ç©ºæ–‡å­—åˆ—]"
   }}

4. ãã®å¾Œã€2ã¤ç›®ã®å¥‘ç´„æ›¸ã®æ¡é …ã‚’ç¶šã‘ã¦è¨˜è¼‰
"""

        # Vertex AIã«é€ä¿¡ã—ã¦æ§‹é€ åŒ–å‡ºåŠ›ã‚’å–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–ï¼‰
        # ãƒšãƒ¼ã‚¸æ•°ãŒå¤šã„å ´åˆã€60ç§’ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§ã¯ä¸ååˆ†ãªãŸã‚éåŒæœŸç‰ˆã‚’ä½¿ç”¨
        # asyncio.run()ã§ã¯ãªãnew_event_loop()ã‚’ä½¿ç”¨ã—ã¦Flaskã¨ã®ç«¶åˆã‚’å›é¿
        import asyncio

        async def generate_with_timeout():
            response = await model.generate_content_async(
                prompt,
                generation_config=generation_config
            )
            return response

        # æ–°ã—ã„ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆï¼ˆFlaskã®æ—¢å­˜ãƒ«ãƒ¼ãƒ—ã¨ç«¶åˆã—ãªã„ï¼‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            # æœ€å¤§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ3600ç§’ = 1æ™‚é–“ï¼‰ã‚’è¨­å®š
            # æ³¨æ„: Cloud Runã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚‚3600ç§’ã«è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
            response = loop.run_until_complete(
                asyncio.wait_for(generate_with_timeout(), timeout=3600)
            )
        except asyncio.TimeoutError:
            logger.error(f"â±ï¸ Timeout after 3600 seconds while generating structured output")
            return None
        finally:
            loop.close()

        # JSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹
        try:
            structured_data = json.loads(response.text)
            logger.info(f"Successfully structured contract data with {len(structured_data.get('result', {}).get('articles', []))} articles")

            # æ§‹é€ åŒ–JSONç”Ÿæˆå¾Œã€è‡ªå‹•çš„ã«ãƒªã‚¹ã‚¯åˆ†é¡ã‚’è¿½åŠ 
            structured_data = add_risks_to_contract_data(structured_data, workspace_id=workspace_id_int, selected_risk_ids=selected_risk_ids, bucket_name=bucket_name)

            return structured_data
        except json.JSONDecodeError as json_error:
            logger.error(f"Error in Vertex AI structured output: {str(json_error)}")

            # ã‚¨ãƒ©ãƒ¼æ™‚ã«ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’GCSã«ä¿å­˜
            from datetime import datetime
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            error_path = f"{workspace_id}/{project_id}/err/{basename}_error_{timestamp}.txt"

            try:
                upload_json_to_gcs(
                    {"error": str(json_error), "response": response.text, "response_length": len(response.text)},
                    bucket_name,
                    error_path.replace('.txt', '.json')
                )
                logger.info(f"ğŸ“ Error response saved to: gs://{bucket_name}/{error_path.replace('.txt', '.json')}")
            except Exception as upload_error:
                logger.error(f"Failed to save error response: {upload_error}")

            return None

    except Exception as e:
        logger.error(f"Error in Vertex AI structured output: {str(e)}")
        return None


def convert_to_contract_schema(gcs_file_path: str, basename: str) -> Optional[Dict[str, Any]]:
    """
    GCSã«ä¿å­˜ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’Vertex AIã®æ§‹é€ åŒ–å‡ºåŠ›ã‚’ä½¿ã£ã¦å¥‘ç´„æ›¸ã‚¹ã‚­ãƒ¼ãƒã«å¤‰æ›

    Args:
        gcs_file_path: GCSã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        basename: ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ™ãƒ¼ã‚¹å

    Returns:
        æ§‹é€ åŒ–ã•ã‚ŒãŸå¥‘ç´„æ›¸ãƒ‡ãƒ¼ã‚¿ã¾ãŸã¯None
    """
    try:
        # Vertex AIè¨­å®š
        import vertexai
        from vertexai.generative_models import GenerativeModel, GenerationConfig

        project_id_env = os.getenv('GCP_PROJECT_ID')
        location = os.getenv('GCP_LOCATION', 'us-central1')
        if not project_id_env:
            logger.error("GCP_PROJECT_IDç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None

        vertexai.init(project=project_id_env, location=location)
        
        # å¥‘ç´„æ›¸ã‚¹ã‚­ãƒ¼ãƒã®å®šç¾©
        contract_schema = {
            "type": "object",
            "properties": {
                "success": {"type": "boolean"},
                "info": {
                    "type": "object",
                    "properties": {
                        "title": {"type": "string"},
                        "party": {"type": "string"},  # ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®å½“äº‹è€…å
                        "start_date": {"type": "string"},  # ç©ºæ–‡å­—åˆ—ã§å¯¾å¿œ
                        "end_date": {"type": "string"},  # ç©ºæ–‡å­—åˆ—ã§å¯¾å¿œ
                        "conclusion_date": {"type": "string"}  # ç©ºæ–‡å­—åˆ—ã§å¯¾å¿œ
                    },
                    "required": ["title", "party"]
                },
                "result": {
                    "type": "object",
                    "properties": {
                        "articles": {
                            "type": "array",
                            "items": {
                                "anyOf": [
                                    {
                                        "type": "object",
                                        "properties": {
                                            "article_number": {"type": "string"},
                                            "title": {"type": "string"},
                                            "content": {"type": "string"},
                                            "table_number": {"type": "string"}
                                        },
                                        "required": ["content", "title"]
                                    },
                                    {
                                        "type": "object",
                                        "properties": {
                                            "title": {"type": "string"},
                                            "party": {"type": "string"},
                                            "start_date": {"type": "string"},
                                            "end_date": {"type": "string"},
                                            "conclusion_date": {"type": "string"}
                                        },
                                        "required": ["title", "party"]
                                    }
                                ]
                            }
                        }
                    },
                    "required": ["articles"]
                }
            },
            "required": ["success", "info", "result"]
        }
        
        # GCSã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿å–ã‚Š
        file_content = download_text_from_gcs(gcs_file_path)
        if not file_content:
            logger.warning(f"Could not read content from: {gcs_file_path}")
            return None
        
        # Vertex AIãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ï¼ˆæ§‹é€ åŒ–å‡ºåŠ›å¯¾å¿œï¼‰
        model = GenerativeModel('gemini-2.5-flash')
        generation_config = GenerationConfig(
            response_mime_type="application/json",
            response_schema=contract_schema
        )

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆ
        prompt = f"""
ä»¥ä¸‹ã®OCRå‡¦ç†æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æã—ã€å¥‘ç´„æ›¸ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ãƒ•ã‚¡ã‚¤ãƒ«å: {basename}

ãƒ†ã‚­ã‚¹ãƒˆå†…å®¹:
{file_content}

æŠ½å‡ºæŒ‡ç¤º:
1. success: å¸¸ã«true
2. infoéƒ¨åˆ†ï¼ˆ1ã¤ç›®ã®å¥‘ç´„æ›¸ã®æƒ…å ±ã®ã¿ï¼‰:
   - title: å¥‘ç´„æ›¸ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½¿ç”¨ï¼‰
   - party: å¥‘ç´„å½“äº‹è€…ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¨˜è¼‰ï¼ˆä¾‹: "æ ªå¼ä¼šç¤¾A,æ ªå¼ä¼šç¤¾B"ï¼‰
   - start_date: å¥‘ç´„é–‹å§‹æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰
   - end_date: å¥‘ç´„çµ‚äº†æ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰
   - conclusion_date: å¥‘ç´„ç· çµæ—¥ï¼ˆYYYY-MM-DDå½¢å¼ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç©ºæ–‡å­—åˆ—ï¼‰

3. resultéƒ¨åˆ†:
   - articles: å¥‘ç´„æ¡é …ã®é…åˆ—ï¼ˆå…¨ã¦ã®æ¡é …ã‚’æ¼ã‚ŒãªãæŠ½å‡ºï¼‰
     - article_number: æ¡é …ç•ªå·ï¼ˆä¾‹: "ç¬¬1æ¡"ã€"ç¬¬2æ¡"ã€ç•ªå·ãŒãªã„å ´åˆã¯"ç½²åæ¬„"ç­‰ï¼‰
     - title: æ¡é …ã®ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè¦‹å‡ºã—ãŒãªã„å ´åˆã¯å†…å®¹ã‹ã‚‰è¦ç´„ï¼‰
     - content: æ¡é …ã®å®Œå…¨ãªå†…å®¹ï¼ˆçœç•¥ç¦æ­¢ï¼‰
     - table_number: è¡¨ãŒã‚ã‚‹å ´åˆã®ã¿è¡¨ç•ªå·

é‡è¦ãªæ³¨æ„äº‹é …:
- ãƒ†ã‚­ã‚¹ãƒˆå†…ã®å…¨ã¦ã®æ¡é …ã‚’å¿…ãšæŠ½å‡ºã—ã¦ãã ã•ã„ï¼ˆç¬¬1æ¡ã‹ã‚‰æœ€å¾Œã¾ã§ï¼‰
- å„æ¡é …ã®contentã¯å®Œå…¨ã«ã‚³ãƒ”ãƒ¼ã—ã€çœç•¥ã‚„è¦ç´„ã¯è¡Œã‚ãªã„ã§ãã ã•ã„
- æ¡é …ç•ªå·ãŒæ˜è¨˜ã•ã‚Œã¦ã„ãªã„éƒ¨åˆ†ï¼ˆå‰æ–‡ã€ç½²åæ¬„ã€ä»˜è¨˜ç­‰ï¼‰ã‚‚ç‹¬ç«‹ã—ãŸæ¡é …ã¨ã—ã¦æ‰±ã£ã¦ãã ã•ã„
- æ—¥ä»˜ã¯å¯èƒ½ãªé™ã‚ŠYYYY-MM-DDå½¢å¼ã«å¤‰æ›ã—ã¦ãã ã•ã„
- è¡¨ã‚„å›³ãŒã‚ã‚‹å ´åˆã¯HTMLå½¢å¼ã§contentã«å«ã‚ã¦ãã ã•ã„
- ç½²åæ¬„ã‚‚å¿…ãš1ã¤ã®æ¡é …ã¨ã—ã¦æ‰±ã£ã¦ãã ã•ã„
- å‡ºåŠ›ã¯å¿…ãšå®Œå…¨ãªJSONå½¢å¼ã§ã€é€”ä¸­ã§åˆ‡ã‚Œã‚‹ã“ã¨ãªãæœ€å¾Œã¾ã§å‡ºåŠ›ã—ã¦ãã ã•ã„

ã€è¤‡æ•°å¥‘ç´„æ›¸ãŒã‚ã‚‹å ´åˆã®ç‰¹åˆ¥ãƒ«ãƒ¼ãƒ«ã€‘:
1. å¥‘ç´„æ›¸å†…éƒ¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨å¥‘ç´„æ›¸ã®åŒºåˆ‡ã‚Šã‚’æ­£ç¢ºã«åˆ¤åˆ¥:
   - ã€Œé ­æ›¸ã€ã€Œè¦é …ã€ã€Œå¥‘ç´„æ›¸æœ¬æ–‡ã€ã€Œç”¨ç´™ã€ã€Œæ¡ä»¶è¡¨ã€ã€Œæ¦‚è¦ã€ã€Œç‰¹ç´„ã€ã€Œç´°å‰‡ã€ã€Œåˆ¥ç´™ã€ã€Œä»•æ§˜æ›¸ã€ã€Œåˆ¥æ·»ã€ã€Œå›³é¢ã€ã€Œç´„æ¬¾ã€ã€Œæ´¾é£å€‹åˆ¥å¥‘ç´„ç¥¨å¥‘ç´„åŸºæœ¬æƒ…å ±ã€ã€Œå®šç¾©ä¸€è¦§è¡¨ã€ãªã©ã¯å¥‘ç´„æ›¸ã®å†…éƒ¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã‚ã‚Šã€åŒºåˆ‡ã‚Šã§ã¯ã‚ã‚Šã¾ã›ã‚“
   - ã“ã‚Œã‚‰ã¯1ã¤ã®å¥‘ç´„æ›¸ã‚’æ§‹æˆã™ã‚‹è¦ç´ ã¨ã—ã¦ã€åŒã˜articlesé…åˆ—å†…ã«å«ã‚ã¦ãã ã•ã„
   - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã”ã¨ã®ã‚¿ã‚¤ãƒˆãƒ«ã€å¥‘ç´„å½“äº‹è€…ã€å¥‘ç´„æ¡é …ã€ç½²åãªã©ã®é‡è¦ãªæƒ…å ±ãŒåˆ†å‰²å¾Œã‚‚å®Œå…¨ã«ä¿æŒã•ã‚Œã‚‹ã‚ˆã†ã«èª¿æŸ»ã—ã¦ãã ã•ã„
   - åˆ†å‰²ã«ã‚ˆã‚Šæƒ…å ±ã®æ¬ å¦‚ã‚„æ¼ã‚ŒãŒç™ºç”Ÿã—ãªã„ã‚ˆã†ã€æ…é‡ã«åˆ†æã—ã¦ãã ã•ã„

2. å¥‘ç´„æ›¸ã®çµ‚äº†ã‚’ç¤ºã™ç®‡æ‰€ï¼ˆã€Œä»¥ä¸Šã€ç­‰ï¼‰ã¯ã€ä»¥ä¸‹ã®å½¢å¼ã§çµ±ä¸€:
   {{
     "article_number": "",
     "title": "å¥‘ç´„æ›¸çµ‚äº†",
     "content": "----------",
     "table_number": ""
   }}

3. 2ã¤ç›®ä»¥é™ã®å¥‘ç´„æ›¸ãŒå§‹ã¾ã‚‹å ´åˆã€å¥‘ç´„æ›¸çµ‚äº†ã®ç›´å¾Œã«å¥‘ç´„æ›¸åŸºæœ¬æƒ…å ±ã‚’ãã®ã¾ã¾æŒ¿å…¥:
   {{
     "title": "[å¥‘ç´„æ›¸ã‚¿ã‚¤ãƒˆãƒ«]",
     "party": "[å½“äº‹è€…ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š]",
     "start_date": "[YYYY-MM-DD ã¾ãŸã¯ç©ºæ–‡å­—åˆ—]",
     "end_date": "[YYYY-MM-DD ã¾ãŸã¯ç©ºæ–‡å­—åˆ—]",
     "conclusion_date": "[YYYY-MM-DD ã¾ãŸã¯ç©ºæ–‡å­—åˆ—]"
   }}

4. ãã®å¾Œã€2ã¤ç›®ã®å¥‘ç´„æ›¸ã®æ¡é …ã‚’ç¶šã‘ã¦è¨˜è¼‰
"""

        # Vertex AIã«é€ä¿¡ã—ã¦æ§‹é€ åŒ–å‡ºåŠ›ã‚’å–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¯¾ç­–ï¼‰
        # ãƒšãƒ¼ã‚¸æ•°ãŒå¤šã„å ´åˆã€60ç§’ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§ã¯ä¸ååˆ†ãªãŸã‚éåŒæœŸç‰ˆã‚’ä½¿ç”¨
        # asyncio.run()ã§ã¯ãªãnew_event_loop()ã‚’ä½¿ç”¨ã—ã¦Flaskã¨ã®ç«¶åˆã‚’å›é¿
        import asyncio

        async def generate_with_timeout():
            response = await model.generate_content_async(
                prompt,
                generation_config=generation_config
            )
            return response

        # æ–°ã—ã„ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆï¼ˆFlaskã®æ—¢å­˜ãƒ«ãƒ¼ãƒ—ã¨ç«¶åˆã—ãªã„ï¼‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            # æœ€å¤§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ3600ç§’ = 1æ™‚é–“ï¼‰ã‚’è¨­å®š
            # æ³¨æ„: Cloud Runã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚‚3600ç§’ã«è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
            response = loop.run_until_complete(
                asyncio.wait_for(generate_with_timeout(), timeout=3600)
            )
        except asyncio.TimeoutError:
            logger.error(f"â±ï¸ Timeout after 3600 seconds while generating structured output")
            return None
        finally:
            loop.close()

        # JSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹
        structured_data = json.loads(response.text)

        logger.info(f"Successfully structured contract data with {len(structured_data.get('result', {}).get('articles', []))} articles")

        return structured_data

    except Exception as e:
        logger.error(f"Error in Vertex AI structured output: {str(e)}")
        return None


def download_text_from_gcs(gcs_path: str) -> Optional[str]:
    """
    GCSã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’èª­ã¿å–ã‚Š

    Args:
        gcs_path: GCSã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (gs://bucket/path/to/file.txt)

    Returns:
        ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã¾ãŸã¯None
    """
    try:
        
        # GCS URIã‚’ãƒ‘ãƒ¼ã‚¹
        if not gcs_path.startswith('gs://'):
            return None
            
        path_parts = gcs_path.replace('gs://', '').split('/', 1)
        bucket_name = path_parts[0]
        blob_name = path_parts[1]
        
        # GCSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿å–ã‚Š
        client = storage.Client()
        bucket = client.bucket(bucket_name)
        blob = bucket.blob(blob_name)
        
        # ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦èª­ã¿å–ã‚Š
        content = blob.download_as_text(encoding='utf-8')
        
        return content
        
    except Exception as e:
        logger.error(f"Error downloading text from GCS: {str(e)}")
        return None

def download_from_gcs(gcs_uri: str, local_path: str) -> str:
    """
    GCSã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    """
    if not gcs_uri.startswith('gs://'):
        raise ValueError(f"Invalid GCS URI: {gcs_uri}")
    
    path_parts = gcs_uri[5:].split('/', 1)
    if len(path_parts) != 2:
        raise ValueError(f"Invalid GCS URI format: {gcs_uri}")
    
    bucket_name, blob_path = path_parts
    
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    
    # Try to find the blob directly first
    try:
        blob = bucket.blob(blob_path)
        logger.info(f"Downloading {gcs_uri} to {local_path}")
        blob.download_to_filename(local_path)
        return local_path
    except Exception as e:
        logger.warning(f"Direct download failed: {e}")
        
        # If direct access fails, try to list and find matching blobs
        logger.info(f"Searching for blobs matching: {blob_path}")
        
        # Get the directory path and filename
        path_parts = blob_path.split('/')
        if len(path_parts) > 1:
            prefix = '/'.join(path_parts[:-1]) + '/'
            target_filename = path_parts[-1]
        else:
            prefix = ''
            target_filename = blob_path
            
        logger.info(f"Listing blobs with prefix: {prefix}")
        blobs = list(bucket.list_blobs(prefix=prefix))
        logger.info(f"Found {len(blobs)} blobs with prefix")
        
        # Look for exact or partial matches
        for blob in blobs:
            blob_filename = blob.name.split('/')[-1]
            logger.info(f"Checking blob: {blob.name} (filename: {blob_filename})")
            
            # Normalize Unicode for comparison
            import unicodedata
            normalized_blob_name = unicodedata.normalize('NFC', blob.name)
            normalized_blob_filename = unicodedata.normalize('NFC', blob_filename)
            normalized_target_filename = unicodedata.normalize('NFC', target_filename)
            normalized_blob_path = unicodedata.normalize('NFC', blob_path)
            
            # Try multiple matching strategies with normalized strings
            match_found = (
                normalized_blob_filename == normalized_target_filename or  # Exact match
                normalized_target_filename in normalized_blob_filename or  # Target is substring of blob
                normalized_blob_filename in normalized_target_filename or  # Blob is substring of target
                normalized_blob_name == normalized_blob_path or           # Full path exact match
                normalized_blob_name.endswith(normalized_target_filename) or  # Ends with target filename
                # Also try without normalization for backward compatibility
                blob_filename == target_filename or
                target_filename in blob_filename or
                blob_filename in target_filename or
                blob.name == blob_path or
                blob.name.endswith(target_filename)
            )
            
            if match_found:
                logger.info(f"Found matching blob: {blob.name}")
                try:
                    blob.download_to_filename(local_path)
                    return local_path
                except Exception as download_error:
                    logger.warning(f"Failed to download {blob.name}: {download_error}")
                    continue

        raise FileNotFoundError(f"Could not find or download blob matching: {blob_path}")

def upload_file_to_gcs(local_path: str, bucket_name: str, blob_name: str) -> str:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    """
    
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(blob_name)
    
    logger.info(f"Uploading {local_path} to gs://{bucket_name}/{blob_name}")
    blob.upload_from_filename(local_path)
    
    return f"gs://{bucket_name}/{blob_name}"

def upload_json_to_gcs(json_data: Dict[str, Any], bucket_name: str, blob_path: str) -> str:
    """
    JSONãƒ‡ãƒ¼ã‚¿ã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    """
    
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(blob_path)
    
    json_str = json.dumps(json_data, ensure_ascii=False, indent=2)
    
    logger.info(f"Uploading JSON to gs://{bucket_name}/{blob_path}")
    blob.upload_from_string(json_str, content_type='application/json')
    
    return f"gs://{bucket_name}/{blob_path}"

def upload_results_to_gcs(result: Dict[str, Any], bucket_name: str, prefix: str) -> str:
    """
    å‡¦ç†çµæœã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    """
    
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    
    result_json = json.dumps(result, ensure_ascii=False, indent=2)
    blob_name = f"{prefix}result.json"
    blob = bucket.blob(blob_name)
    
    logger.info(f"Uploading results to gs://{bucket_name}/{blob_name}")
    blob.upload_from_string(result_json, content_type='application/json')
    
    return f"gs://{bucket_name}/{blob_name}"


# ================================
# Test Endpoints for DB Connection
# ================================

@app.route('/test/db-connection', methods=['GET'])
def test_db_connection():
    """
    DBæ¥ç¶šãƒ†ã‚¹ãƒˆç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    """
    try:
        logger.info("ğŸ§ª Testing database connection...")
        conn = get_db_connection()
        cursor = conn.cursor()

        # Simple query to test connection
        cursor.execute("SELECT version();")
        db_version = cursor.fetchone()

        cursor.close()
        conn.close()

        logger.info(f"âœ… Database connection successful! Version: {db_version[0] if db_version else 'Unknown'}")

        return jsonify({
            "success": True,
            "message": "Database connection successful",
            "db_version": db_version[0] if db_version else None
        }), 200

    except Exception as e:
        logger.error(f"âŒ Database connection failed: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }), 500


@app.route('/test/risks', methods=['GET'])
def test_get_risks():
    """
    ãƒªã‚¹ã‚¯ã‚¿ã‚¤ãƒ—å–å¾—ãƒ†ã‚¹ãƒˆç”¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ

    Query Parameters:
        - workspace_id: ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹IDï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        - selected_risk_ids: ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã®ãƒªã‚¹ã‚¯IDï¼ˆä¾‹: 1,2,3ï¼‰
    """
    try:
        workspace_id_str = request.args.get('workspace_id')
        selected_risk_ids_str = request.args.get('selected_risk_ids')

        workspace_id = None
        selected_risk_ids = None

        if workspace_id_str:
            try:
                workspace_id = int(workspace_id_str)
            except Exception as e:
                return jsonify({"error": f"Invalid workspace_id: {e}"}), 400

        if selected_risk_ids_str:
            try:
                selected_risk_ids = [int(id.strip()) for id in selected_risk_ids_str.split(",") if id.strip()]
            except Exception as e:
                return jsonify({"error": f"Invalid selected_risk_ids: {e}"}), 400

        logger.info(f"ğŸ§ª Testing risk retrieval - workspace_id: {workspace_id}, selected_risk_ids: {selected_risk_ids}")

        risks = get_risks_from_db(workspace_id=workspace_id, selected_risk_ids=selected_risk_ids)

        logger.info(f"âœ… Retrieved {len(risks)} risks from database")

        return jsonify({
            "success": True,
            "count": len(risks),
            "workspace_id": workspace_id,
            "selected_risk_ids": selected_risk_ids,
            "risks": risks
        }), 200

    except Exception as e:
        logger.error(f"âŒ Failed to retrieve risks: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            "success": False,
            "error": str(e),
            "traceback": traceback.format_exc()
        }), 500


if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8080))
    app.run(host='0.0.0.0', port=port, debug=False)