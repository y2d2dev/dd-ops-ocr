import os
import json
import base64
import logging
import shutil
from flask import Flask, request, jsonify
from datetime import datetime
import traceback
from typing import Dict, Any, Optional

# UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç¢ºå®Ÿã«ã™ã‚‹
os.environ.setdefault('PYTHONIOENCODING', 'utf-8')
os.environ.setdefault('LANG', 'ja_JP.UTF-8')
os.environ.setdefault('LC_ALL', 'ja_JP.UTF-8')

import subprocess
import sys
from pathlib import Path
from src.api.model_downloader import ensure_models_available

app = Flask(__name__)

# UTF-8ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç¢ºå®Ÿã«ã™ã‚‹
app.config['JSON_AS_ASCII'] = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Cloud Runèµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
def initialize_models():
    """èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆ512MBæ§‹æˆç”¨ã«è»½é‡åŒ–ï¼‰"""
    logger.info("ğŸš€ Initializing models (lightweight mode)...")
    
    # ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ãŒã‚ã‚‹å ´åˆã¯ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—
    memory_limit = os.environ.get('CLOUD_RUN_MEMORY', '512Mi')
    if '512' in memory_limit:
        logger.info("ğŸ’¡ 512MBæ§‹æˆæ¤œå‡º - ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
        logger.info("ğŸ”§ ãƒ¢ãƒ‡ãƒ«ã¯å‡¦ç†æ™‚ã«å‹•çš„ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™")
        return
    
    try:
        success = ensure_models_available()
        if success:
            logger.info("âœ… Models initialized successfully")
        else:
            logger.warning("âš ï¸ Some models may be missing")
    except Exception as e:
        logger.error(f"âŒ Failed to initialize models: {e}")
        logger.warning("âš ï¸ Continuing without models - OCR features may be limited")

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
initialize_models()

def get_project_root():
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—"""
    # Cloud Runã§ã¯/appé…ä¸‹ã«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒé…ç½®ã•ã‚Œã‚‹
    if Path("/app").exists():
        return Path("/app")
    else:
        # ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç’°å¢ƒ
        return Path(__file__).parent.parent.parent

def run_main_pipeline(pdf_path: str) -> Dict[str, Any]:
    """
    main_pipeline.pyã‚’å®Ÿè¡Œã—ã¦OCRå‡¦ç†ã‚’è¡Œã†
    
    Args:
        pdf_path: å‡¦ç†å¯¾è±¡ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        
    Returns:
        Dict: å‡¦ç†çµæœ
    """
    try:
        project_root = get_project_root()
        main_pipeline_path = project_root / "src" / "main_pipeline.py"
        
        if not main_pipeline_path.exists():
            raise FileNotFoundError(f"main_pipeline.py not found: {main_pipeline_path}")
        
        # main_pipeline.pyã‚’å®Ÿè¡Œ
        cmd = [
            sys.executable,
            str(main_pipeline_path),
            "--input", pdf_path
        ]
        
        logger.info(f"ğŸš€ Running main_pipeline.py with command: {' '.join(cmd)}")
        
        result = subprocess.run(
            cmd,
            cwd=str(project_root),
            capture_output=True,
            text=True,
            encoding='utf-8',
            timeout=300  # 5åˆ†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        )
        
        if result.returncode == 0:
            logger.info("âœ… main_pipeline.py executed successfully")
            return {
                "success": True,
                "stdout": result.stdout,
                "stderr": result.stderr
            }
        else:
            logger.error(f"âŒ main_pipeline.py failed with return code: {result.returncode}")
            logger.error(f"âŒ STDOUT: {result.stdout}")
            logger.error(f"âŒ STDERR: {result.stderr}")
            return {
                "success": False,
                "error": f"Pipeline execution failed: {result.stderr}",
                "stdout": result.stdout,
                "stderr": result.stderr
            }
            
    except subprocess.TimeoutExpired:
        logger.error("âŒ main_pipeline.py execution timed out")
        return {
            "success": False,
            "error": "Pipeline execution timed out"
        }
    except Exception as e:
        logger.error(f"âŒ Error running main_pipeline.py: {e}")
        return {
            "success": False,
            "error": str(e)
        }

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({'status': 'healthy', 'timestamp': datetime.utcnow().isoformat()}), 200

@app.route('/debug-blobs', methods=['GET'])
def debug_blobs():
    """GCSå†…ã®blobã‚’ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒªã‚¹ãƒˆã™ã‚‹"""
    try:
        from google.cloud import storage
        
        prefix = request.args.get('prefix', '')
        bucket_name = 'app_contracts_staging'
        
        storage_client = storage.Client()
        bucket = storage_client.bucket(bucket_name)
        
        blobs = list(bucket.list_blobs(prefix=prefix))
        
        blob_info = []
        for blob in blobs:
            blob_info.append({
                'name': blob.name,
                'size': blob.size,
                'created': blob.time_created.isoformat() if blob.time_created else None,
                'content_type': blob.content_type
            })
        
        return jsonify({
            'bucket': bucket_name,
            'prefix': prefix,
            'total_blobs': len(blob_info),
            'blobs': blob_info
        }), 200
        
    except Exception as e:
        logger.error(f"Debug blobs error: {str(e)}")
        return jsonify({
            'error': str(e)
        }), 500

@app.route('/', methods=['GET'])
def root():
    """
    ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ - APIæƒ…å ±ã‚’è¿”ã™
    """
    return jsonify({
        'service': 'DD-OPS OCR API',
        'version': '1.0.0',
        'endpoints': {
            '/health': 'Health check',
            '/ocr [GET]': 'Test OCR with local PDF files',
            '/ocr [POST]': 'OCR with GCS PDF URL',
            '/pubsub/push [POST]': 'PubSub webhook for automatic processing'
        },
        'usage': {
            'GET /ocr': 'Process PDF from /pdf/ directory',
            'GET /ocr?file=test.pdf': 'Process specific PDF file',
            'POST /ocr': 'Send {"pdf_url": "gs://bucket/file.pdf", "workspace_id": "ws", "project_id": "proj"}'
        },
        'timestamp': datetime.utcnow().isoformat()
    }), 200

@app.route('/ocr', methods=['GET'])
def ocr_test():
    """
    GETãƒ¡ã‚½ãƒƒãƒ‰ã§OCRå‡¦ç†ã‚’ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    
    ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:
        file: PDFãƒ•ã‚¡ã‚¤ãƒ«å (ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯è‡ªå‹•æ¤œå‡º)
        
    ä¾‹: /ocr?file=test.pdf
    """
    try:
        logger.info("ğŸš€ GET /ocr endpoint called")
        
        # ã‚¯ã‚¨ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—
        pdf_filename = request.args.get('file')
        
        # ãƒ†ã‚¹ãƒˆç”¨ã®OCRå‡¦ç†ã‚’å®Ÿè¡Œ
        result = process_test_pdf(pdf_filename)
        
        response = {
            "status": "completed",
            "method": "GET",
            "timestamp": datetime.now().isoformat(),
            "result": result
        }
        
        return jsonify(response), 200
        
    except Exception as e:
        logger.error(f"OCR test endpoint error: {str(e)}")
        return jsonify({
            "status": "error",
            "method": "GET",
            "timestamp": datetime.now().isoformat(),
            "error": str(e)
        }), 500

@app.route('/ocr', methods=['POST'])
def ocr_upload():
    """
    POSTãƒ¡ã‚½ãƒƒãƒ‰ã§OCRå‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    
    JSON body:
        {
            "pdf_url": "gs://bucket/path/to/file.pdf",
            "workspace_id": "workspace123",
            "project_id": "project456"
        }
    """
    try:
        logger.info("ğŸš€ POST /ocr endpoint called")
        
        data = request.get_json()
        if not data:
            return jsonify({
                "status": "error",
                "error": "JSON body required"
            }), 400
        
        pdf_url = data.get('pdf_url')
        workspace_id = data.get('workspace_id', 'test_workspace')
        project_id = data.get('project_id', 'test_project')
        
        if not pdf_url:
            return jsonify({
                "status": "error",
                "error": "pdf_url is required"
            }), 400
        
        # GCS URIã®å ´åˆ
        if pdf_url.startswith('gs://'):
            bucket_name = pdf_url.replace('gs://', '').split('/')[0]
            object_name = '/'.join(pdf_url.replace('gs://', '').split('/')[1:])
            
            result = process_single_pdf(bucket_name, object_name, workspace_id, project_id)
        else:
            return jsonify({
                "status": "error",
                "error": "Only gs:// URLs are supported"
            }), 400
        
        response = {
            "status": "completed",
            "method": "POST",
            "timestamp": datetime.now().isoformat(),
            "workspace_id": workspace_id,
            "project_id": project_id,
            "result": result
        }
        
        return jsonify(response), 200
        
    except Exception as e:
        logger.error(f"OCR upload endpoint error: {str(e)}")
        return jsonify({
            "status": "error",
            "method": "POST",
            "timestamp": datetime.now().isoformat(),
            "error": str(e)
        }), 500

@app.route('/pubsub/push', methods=['POST'])
def pubsub_push():
    """
    Cloud PubSub Pushé€šçŸ¥ã‚’å—ã‘å–ã‚‹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    GCS Storage Object notificationã‚’å‡¦ç†
    """
    try:
        logger.info("="*80)
        logger.info("ğŸ”µ PUBSUB PUSH REQUEST RECEIVED")
        logger.info("="*80)
        
        logger.info(f"ğŸ“Œ Request Method: {request.method}")
        logger.info(f"ğŸ“Œ Request URL: {request.url}")
        logger.info(f"ğŸ“Œ Request Path: {request.path}")
        
        logger.info("ğŸ“‹ REQUEST HEADERS:")
        for key, value in request.headers:
            logger.info(f"  - {key}: {value}")
        
        logger.info(f"ğŸ“ Content-Type: {request.content_type}")
        logger.info(f"ğŸ“ Content-Length: {request.content_length}")
        
        logger.info(f"ğŸ“¦ Raw Request Data: {request.data}")
        logger.info(f"ğŸ“¦ Request Data Type: {type(request.data)}")
        logger.info(f"ğŸ“¦ Request Data Length: {len(request.data) if request.data else 0}")
        
        logger.info("ğŸ” Attempting to parse JSON...")
        envelope = None
        try:
            envelope = request.get_json()
            logger.info(f"âœ… JSON parsed successfully")
            logger.info(f"ğŸ“Š Envelope type: {type(envelope)}")
            logger.info(f"ğŸ“Š Envelope keys: {list(envelope.keys()) if envelope else 'None'}")
            logger.info(f"ğŸ“Š Full envelope content: {json.dumps(envelope, indent=2) if envelope else 'None'}")
        except Exception as json_error:
            logger.error(f"âŒ JSON parsing failed: {str(json_error)}")
            logger.error(f"âŒ Error type: {type(json_error).__name__}")
        
        if not envelope:
            logger.error("âŒ No PubSub message received (envelope is None or empty)")
            return jsonify({"error": "Bad Request: no PubSub message received"}), 400
            
        if not isinstance(envelope, dict) or "message" not in envelope:
            logger.error(f"âŒ Invalid PubSub message format - envelope type: {type(envelope)}, has 'message' key: {'message' in envelope if isinstance(envelope, dict) else 'N/A'}")
            return jsonify({"error": "Bad Request: invalid PubSub message format"}), 400
            
        pubsub_message = envelope["message"]
        logger.info("ğŸ“¨ PUBSUB MESSAGE:")
        logger.info(f"  - Message type: {type(pubsub_message)}")
        logger.info(f"  - Message keys: {list(pubsub_message.keys()) if isinstance(pubsub_message, dict) else 'Not a dict'}")
        logger.info(f"  - Full message: {json.dumps(pubsub_message, indent=2)}")
        
        if isinstance(pubsub_message.get("data"), str):
            try:
                logger.info("ğŸ”“ Attempting Base64 decode...")
                logger.info(f"  - Data length (encoded): {len(pubsub_message['data'])}")
                logger.info(f"  - First 100 chars: {pubsub_message['data'][:100]}...")
                
                message_data = base64.b64decode(pubsub_message["data"]).decode("utf-8")
                logger.info(f"âœ… Base64 decode successful")
                logger.info(f"  - Decoded length: {len(message_data)}")
                logger.info(f"  - First 200 chars: {message_data[:200] if message_data else 'Empty'}...")
                
                if not message_data or not message_data.strip():
                    logger.warning(f"âš ï¸ Decoded message is empty or whitespace only")
                    return jsonify({"status": "ignored", "reason": "Empty message"}), 200
                
                storage_object = json.loads(message_data)
                logger.info(f"âœ… JSON parse of decoded data successful")
                logger.info(f"ğŸ“„ Storage Object keys: {list(storage_object.keys())}")
                logger.info(f"ğŸ“„ Full Storage Object: {json.dumps(storage_object, indent=2)}")
            except Exception as e:
                logger.error(f"âŒ Failed to decode PubSub message: {str(e)}")
                logger.error(f"âŒ Error type: {type(e).__name__}")
                logger.error(f"âŒ Stack trace: {traceback.format_exc()}")
                return jsonify({"error": "Bad Request: invalid message data"}), 400
        else:
            logger.error(f"âŒ PubSub message data is not a string, type: {type(pubsub_message.get('data'))}")
            return jsonify({"error": "Bad Request: message data must be base64 encoded"}), 400
            
        if not isinstance(storage_object, dict) or "id" not in storage_object:
            logger.error(f"âŒ Invalid Storage Object format - type: {type(storage_object)}, has 'id': {'id' in storage_object if isinstance(storage_object, dict) else 'N/A'}")
            return jsonify({"error": "Bad Request: invalid Storage Object"}), 400
            
        object_id = storage_object.get("id", "")
        object_name = storage_object.get("name", "")
        object_bucket = storage_object.get("bucket", "")
        
        logger.info("ğŸ—‚ï¸ STORAGE OBJECT INFO:")
        logger.info(f"  - Object ID: {object_id}")
        logger.info(f"  - Object Name: {object_name}")
        logger.info(f"  - Bucket: {object_bucket}")
        logger.info(f"  - Content Type: {storage_object.get('contentType', 'N/A')}")
        logger.info(f"  - Size: {storage_object.get('size', 'N/A')} bytes")
        
        name_parts = object_name.split("/")
        logger.info(f"ğŸ“‚ Name parts: {name_parts}")
        logger.info(f"ğŸ“‚ Number of parts: {len(name_parts)}")
        
        if len(name_parts) < 3:
            logger.error(f"âŒ Invalid object name format: {object_name} - expected at least 3 parts")
            return jsonify({"error": "Invalid object name format"}), 400
            
        workspace_id = name_parts[0]
        project_id = name_parts[1]
        filename = "/".join(name_parts[2:])
        
        logger.info("âœ¨ EXTRACTED INFO:")
        logger.info(f"  - Workspace ID: {workspace_id}")
        logger.info(f"  - Project ID: {project_id}")
        logger.info(f"  - Filename: {filename}")
        
        if not filename.lower().endswith('.pdf'):
            logger.info(f"âš ï¸ Ignoring non-PDF file: {filename}")
            return jsonify({"message": "File ignored (not a PDF)"}), 200
            
        logger.info(f"ğŸš€ Starting PDF processing - workspace: {workspace_id}, project: {project_id}, file: {filename}")
        
        result = process_single_pdf(object_bucket, object_name, workspace_id, project_id)
        
        response = {
            "workspace_id": workspace_id,
            "project_id": project_id,
            "file": filename,
            "success": result["success"],
            "timestamp": datetime.now().isoformat()
        }
        
        if result["success"]:
            response["contract_json"] = result.get("contract_json", "")
            response["output_files"] = result.get("output_files", [])
            logger.info(f"Successfully processed PDF: {filename}")
        else:
            response["error"] = result.get("error", "Processing failed")
            logger.error(f"Failed to process PDF: {filename} - {result.get('error')}")
            
        return jsonify(response), 200
        
    except Exception as e:
        logger.error(f"Unexpected error in PubSub handler: {str(e)}")
        logger.error(traceback.format_exc())
        return jsonify({
            "error": "Internal server error",
            "message": str(e)
        }), 200

def process_test_pdf(pdf_filename: Optional[str] = None) -> Dict[str, Any]:
    """
    ãƒ†ã‚¹ãƒˆç”¨ã®PDFå‡¦ç†ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã®pdf/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ï¼‰
    
    Args:
        pdf_filename: å‡¦ç†ã™ã‚‹PDFãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
    Returns:
        å‡¦ç†çµæœã‚’å«ã‚€è¾æ›¸
    """
    try:
        project_root = get_project_root()
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ã®pdf/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰PDFã‚’æ¤œç´¢
        local_pdf_dir = project_root / "pdf"
        
        if not local_pdf_dir.exists():
            return {
                'success': False,
                'error': 'PDF directory not found. Please place PDF files in /pdf/ directory.'
            }
        
        # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®æ±ºå®š
        if pdf_filename:
            pdf_path = local_pdf_dir / pdf_filename
            if not pdf_path.exists():
                return {
                    'success': False,
                    'error': f'PDF file not found: {pdf_filename}'
                }
        else:
            # è‡ªå‹•æ¤œå‡ºï¼šæœ€åˆã«è¦‹ã¤ã‹ã£ãŸPDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
            pdf_files = list(local_pdf_dir.glob("*.pdf"))
            if not pdf_files:
                return {
                    'success': False,
                    'error': 'No PDF files found in /pdf/ directory'
                }
            pdf_path = pdf_files[0]
            pdf_filename = pdf_path.name
        
        logger.info(f"Processing local PDF: {pdf_path}")
        
        # main_pipeline.pyã‚’å®Ÿè¡Œ
        pipeline_result = run_main_pipeline(str(pdf_path))
        
        if not pipeline_result["success"]:
            logger.error(f"Pipeline execution failed: {pipeline_result.get('error')}")
            return {
                'success': False,
                'error': pipeline_result.get('error', 'Pipeline execution failed'),
                'pipeline_output': pipeline_result
            }
        
        # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’åé›†
        basename = os.path.splitext(pdf_filename)[0]
        result_files = []
        contract_data = None
        
        # resultãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒ£ãƒ³
        result_dir = project_root / "result"
        if not result_dir.exists():
            result_dir = Path("/tmp/result")
        
        if result_dir.exists():
            for result_file in result_dir.glob("*"):
                if result_file.is_file():
                    file_info = {
                        "filename": result_file.name,
                        "size": result_file.stat().st_size,
                        "path": str(result_file)
                    }
                    
                    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯å†…å®¹ã‚‚èª­ã¿å–ã‚Š
                    if result_file.suffix == '.json':
                        try:
                            with open(result_file, 'r', encoding='utf-8') as f:
                                json_content = json.load(f)
                            file_info["content"] = json_content
                            
                            # integration_metadataã®å ´åˆã¯å¥‘ç´„ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æ‰±ã†
                            if 'integration_metadata' in result_file.name:
                                contract_data = json_content
                        except Exception as e:
                            logger.warning(f"Failed to read JSON file {result_file.name}: {e}")
                    
                    result_files.append(file_info)
        
        return {
            'success': True,
            'pdf_file': pdf_filename,
            'pdf_path': str(pdf_path),
            'result_files': result_files,
            'contract_data': contract_data,
            'pipeline_result': pipeline_result,
            'processing_time': datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error processing test PDF: {str(e)}")
        return {
            'success': False,
            'error': str(e)
        }

def process_single_pdf(bucket_name: str, object_name: str, workspace_id: str, project_id: str) -> Dict[str, Any]:
    """
    å˜ä¸€ã®PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    
    Args:
        bucket_name: GCSãƒã‚±ãƒƒãƒˆå
        object_name: ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ (workspace_id/project_id/filename.pdf)
        workspace_id: ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ID
        project_id: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID
    
    Returns:
        å‡¦ç†çµæœã‚’å«ã‚€è¾æ›¸
    """
    try:
        gcs_uri = f"gs://{bucket_name}/{object_name}"
        logger.info(f"Processing PDF from: {gcs_uri}")
        
        # Cloud Runå¯¾å¿œï¼šæ›¸ãè¾¼ã¿å¯èƒ½ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨
        pdf_dir = Path("/tmp/pdf")
        pdf_dir.mkdir(exist_ok=True)
        
        # GCSã‹ã‚‰PDFã‚’ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        filename = os.path.basename(object_name)
        local_file_path = pdf_dir / filename
        download_from_gcs(gcs_uri, str(local_file_path))
        
        logger.info(f"Starting OCR pipeline for: {local_file_path}")
        
        # main_pipeline.pyã‚’å®Ÿè¡Œ
        pipeline_result = run_main_pipeline(str(local_file_path))
        
        if not pipeline_result["success"]:
            logger.error(f"Pipeline execution failed: {pipeline_result.get('error')}")
            return {
                'success': False,
                'error': pipeline_result.get('error', 'Pipeline execution failed')
            }
        
        # å‡¦ç†å¾Œã«PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        if local_file_path.exists():
            local_file_path.unlink()
            logger.info(f"Cleaned up local PDF file: {local_file_path}")
        
        output_bucket = os.environ.get('GCS_BUCKET_NAME', bucket_name)
        
        # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        basename = os.path.splitext(filename)[0]
        
        # resultãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªãƒ»ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        # Cloud Runã§ã¯çµæœã‚‚tmpã«å‡ºåŠ›ã•ã‚Œã‚‹å¯èƒ½æ€§ã‚’è€ƒæ…®
        project_root = get_project_root()
        result_dir = project_root / "result"
        if not result_dir.exists():
            result_dir = Path("/tmp/result")
        output_files = []
        contract_json_path = None
        
        if result_dir.exists():
            # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«ï¼‰
            for result_file in result_dir.glob("*"):
                if result_file.is_file():
                    # GCSã®ãƒ‘ã‚¹ã‚’ç”Ÿæˆ
                    if result_file.suffix == '.json' and 'integration_metadata' in result_file.name:
                        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿JSONã‚’after_ocrã«ä¿å­˜
                        json_output_path = f"{workspace_id}/{project_id}/after_ocr/{basename}.json"
                        json_gcs_path = upload_file_to_gcs(
                            str(result_file),
                            output_bucket,
                            json_output_path
                        )
                        contract_json_path = json_gcs_path
                        logger.info(f"âœ… Contract JSON saved to: {json_gcs_path}")
                    else:
                        # ãã®ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ocr_resultsã«ä¿å­˜
                        output_prefix = f"{workspace_id}/{project_id}/ocr_results/"
                        gcs_path = upload_file_to_gcs(
                            str(result_file),
                            output_bucket,
                            output_prefix + result_file.name
                        )
                        output_files.append(gcs_path)
                        logger.info(f"âœ… Result file uploaded to: {gcs_path}")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¥‘ç´„æ›¸JSONãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
        if not contract_json_path:
            logger.warning(f"âš ï¸ å¥‘ç´„æ›¸JSONãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆJSONã‚’ä½œæˆã—ã¾ã™ã€‚")
            default_json = {
                "success": True,
                "info": {
                    "title": basename,
                    "party": "",
                    "start_date": "",
                    "end_date": "",
                    "conclusion_date": ""
                },
                "result": {
                    "articles": []
                },
                "note": "main_pipeline.pyå®Ÿè¡Œå®Œäº†ã€‚è©³ç´°ãªçµæœã¯ä»–ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"
            }
            
            json_output_path = f"{workspace_id}/{project_id}/after_ocr/{basename}.json"
            contract_json_path = upload_json_to_gcs(
                default_json,
                output_bucket,
                json_output_path
            )
            logger.info(f"âœ… Default contract JSON saved to: {contract_json_path}")
        
        return {
            'success': True,
            'contract_json': contract_json_path,
            'output_files': output_files,
            'pipeline_result': pipeline_result
        }
        
    except Exception as e:
        logger.error(f"Error processing PDF: {str(e)}")
        logger.error(traceback.format_exc())
        return {
            'success': False,
            'error': str(e)
        }

def download_from_gcs(gcs_uri: str, local_path: str) -> str:
    """
    GCSã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    """
    from google.cloud import storage
    import urllib.parse
    
    if not gcs_uri.startswith('gs://'):
        raise ValueError(f"Invalid GCS URI: {gcs_uri}")
    
    path_parts = gcs_uri[5:].split('/', 1)
    if len(path_parts) != 2:
        raise ValueError(f"Invalid GCS URI format: {gcs_uri}")
    
    bucket_name, blob_path = path_parts
    
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    
    # Try to find the blob directly first
    try:
        blob = bucket.blob(blob_path)
        logger.info(f"Downloading {gcs_uri} to {local_path}")
        blob.download_to_filename(local_path)
        return local_path
    except Exception as e:
        logger.warning(f"Direct download failed: {e}")
        
        # If direct access fails, try to list and find matching blobs
        logger.info(f"Searching for blobs matching: {blob_path}")
        
        # Get the directory path and filename
        path_parts = blob_path.split('/')
        if len(path_parts) > 1:
            prefix = '/'.join(path_parts[:-1]) + '/'
            target_filename = path_parts[-1]
        else:
            prefix = ''
            target_filename = blob_path
            
        logger.info(f"Listing blobs with prefix: {prefix}")
        blobs = list(bucket.list_blobs(prefix=prefix))
        logger.info(f"Found {len(blobs)} blobs with prefix")
        
        # Look for exact or partial matches
        for blob in blobs:
            blob_filename = blob.name.split('/')[-1]
            logger.info(f"Checking blob: {blob.name} (filename: {blob_filename})")
            
            # Normalize Unicode for comparison
            import unicodedata
            normalized_blob_name = unicodedata.normalize('NFC', blob.name)
            normalized_blob_filename = unicodedata.normalize('NFC', blob_filename)
            normalized_target_filename = unicodedata.normalize('NFC', target_filename)
            normalized_blob_path = unicodedata.normalize('NFC', blob_path)
            
            # Try multiple matching strategies with normalized strings
            match_found = (
                normalized_blob_filename == normalized_target_filename or  # Exact match
                normalized_target_filename in normalized_blob_filename or  # Target is substring of blob
                normalized_blob_filename in normalized_target_filename or  # Blob is substring of target
                normalized_blob_name == normalized_blob_path or           # Full path exact match
                normalized_blob_name.endswith(normalized_target_filename) or  # Ends with target filename
                # Also try without normalization for backward compatibility
                blob_filename == target_filename or
                target_filename in blob_filename or
                blob_filename in target_filename or
                blob.name == blob_path or
                blob.name.endswith(target_filename)
            )
            
            if match_found:
                logger.info(f"Found matching blob: {blob.name}")
                try:
                    blob.download_to_filename(local_path)
                    return local_path
                except Exception as download_error:
                    logger.warning(f"Failed to download {blob.name}: {download_error}")
                    continue
                    
        raise FileNotFoundError(f"Could not find or download blob matching: {blob_path}")
    
    return local_path

def upload_file_to_gcs(local_path: str, bucket_name: str, blob_name: str) -> str:
    """
    ãƒ•ã‚¡ã‚¤ãƒ«ã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    """
    from google.cloud import storage
    
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(blob_name)
    
    logger.info(f"Uploading {local_path} to gs://{bucket_name}/{blob_name}")
    blob.upload_from_filename(local_path)
    
    return f"gs://{bucket_name}/{blob_name}"

def upload_json_to_gcs(json_data: Dict[str, Any], bucket_name: str, blob_path: str) -> str:
    """
    JSONãƒ‡ãƒ¼ã‚¿ã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    """
    from google.cloud import storage
    
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(blob_path)
    
    json_str = json.dumps(json_data, ensure_ascii=False, indent=2)
    
    logger.info(f"Uploading JSON to gs://{bucket_name}/{blob_path}")
    blob.upload_from_string(json_str, content_type='application/json')
    
    return f"gs://{bucket_name}/{blob_path}"

def upload_results_to_gcs(result: Dict[str, Any], bucket_name: str, prefix: str) -> str:
    """
    å‡¦ç†çµæœã‚’GCSã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    """
    from google.cloud import storage
    
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    
    result_json = json.dumps(result, ensure_ascii=False, indent=2)
    blob_name = f"{prefix}result.json"
    blob = bucket.blob(blob_name)
    
    logger.info(f"Uploading results to gs://{bucket_name}/{blob_name}")
    blob.upload_from_string(result_json, content_type='application/json')
    
    return f"gs://{bucket_name}/{blob_name}"


if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8080))
    app.run(host='0.0.0.0', port=port, debug=False)